I1006 15:42:06.514442 22737 caffe.cpp:184] Using GPUs 0
I1006 15:42:07.088081 22737 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part2.prototxt"
I1006 15:42:07.088114 22737 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part2.prototxt
I1006 15:42:07.088279 22737 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:42:07.088321 22737 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part2.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part2.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:42:07.088363 22737 layer_factory.hpp:76] Creating layer data_layer
I1006 15:42:07.114884 22737 net.cpp:110] Creating Layer data_layer
I1006 15:42:07.114914 22737 net.cpp:433] data_layer -> data_blob
I1006 15:42:07.114936 22737 net.cpp:433] data_layer -> label_blob
I1006 15:42:07.115499 22741 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part2.train
I1006 15:42:07.801570 22737 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:42:07.812170 22737 net.cpp:155] Setting up data_layer
I1006 15:42:07.812224 22737 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:42:07.812230 22737 net.cpp:163] Top shape: 40000 (40000)
I1006 15:42:07.812248 22737 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:42:07.812271 22737 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:42:07.812278 22737 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:42:07.812312 22737 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:42:07.812722 22737 net.cpp:155] Setting up hidden_sum_layer
I1006 15:42:07.812741 22737 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:42:07.812752 22737 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:42:07.812772 22737 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:42:07.812777 22737 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:42:07.812790 22737 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:42:11.016165 22737 net.cpp:155] Setting up hidden_act_layer
I1006 15:42:11.016197 22737 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:42:11.016201 22737 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:42:11.016211 22737 net.cpp:110] Creating Layer output_sum_layer
I1006 15:42:11.016214 22737 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:42:11.016219 22737 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:42:11.016332 22737 net.cpp:155] Setting up output_sum_layer
I1006 15:42:11.016341 22737 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:42:11.016361 22737 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:42:11.016367 22737 net.cpp:110] Creating Layer output_act_layer
I1006 15:42:11.016369 22737 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:42:11.016372 22737 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:42:11.016453 22737 net.cpp:155] Setting up output_act_layer
I1006 15:42:11.016470 22737 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:42:11.016484 22737 layer_factory.hpp:76] Creating layer error_layer
I1006 15:42:11.016500 22737 net.cpp:110] Creating Layer error_layer
I1006 15:42:11.016504 22737 net.cpp:477] error_layer <- output_act_blob
I1006 15:42:11.016506 22737 net.cpp:477] error_layer <- label_blob
I1006 15:42:11.016510 22737 net.cpp:433] error_layer -> error_blob
I1006 15:42:11.016556 22737 net.cpp:155] Setting up error_layer
I1006 15:42:11.016564 22737 net.cpp:163] Top shape: (1)
I1006 15:42:11.016567 22737 net.cpp:168]     with loss weight 1
I1006 15:42:11.016597 22737 net.cpp:236] error_layer needs backward computation.
I1006 15:42:11.016603 22737 net.cpp:236] output_act_layer needs backward computation.
I1006 15:42:11.016607 22737 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:42:11.016610 22737 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:42:11.016616 22737 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:42:11.016620 22737 net.cpp:240] data_layer does not need backward computation.
I1006 15:42:11.016623 22737 net.cpp:283] This network produces output error_blob
I1006 15:42:11.016628 22737 net.cpp:297] Network initialization done.
I1006 15:42:11.016629 22737 net.cpp:298] Memory required for data: 13440004
I1006 15:42:11.016773 22737 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part2.prototxt
I1006 15:42:11.016789 22737 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:42:11.016836 22737 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part2.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part2.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:42:11.016876 22737 layer_factory.hpp:76] Creating layer data_layer
I1006 15:42:11.019403 22737 net.cpp:110] Creating Layer data_layer
I1006 15:42:11.019420 22737 net.cpp:433] data_layer -> data_blob
I1006 15:42:11.019428 22737 net.cpp:433] data_layer -> label_blob
I1006 15:42:11.019991 22743 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part2.test
I1006 15:42:11.020059 22737 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:42:11.021993 22737 net.cpp:155] Setting up data_layer
I1006 15:42:11.022011 22737 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:42:11.022016 22737 net.cpp:163] Top shape: 4000 (4000)
I1006 15:42:11.022020 22737 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:42:11.022032 22737 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:42:11.022054 22737 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:42:11.022074 22737 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:42:11.022171 22737 net.cpp:155] Setting up hidden_sum_layer
I1006 15:42:11.022177 22737 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:42:11.022189 22737 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:42:11.022199 22737 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:42:11.022213 22737 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:42:11.022222 22737 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:42:11.022378 22737 net.cpp:155] Setting up hidden_act_layer
I1006 15:42:11.022387 22737 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:42:11.022392 22737 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:42:11.022397 22737 net.cpp:110] Creating Layer output_sum_layer
I1006 15:42:11.022403 22737 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:42:11.022409 22737 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:42:11.022480 22737 net.cpp:155] Setting up output_sum_layer
I1006 15:42:11.022488 22737 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:42:11.022497 22737 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:42:11.022505 22737 net.cpp:110] Creating Layer output_act_layer
I1006 15:42:11.022510 22737 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:42:11.022516 22737 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:42:11.022575 22737 net.cpp:155] Setting up output_act_layer
I1006 15:42:11.022583 22737 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:42:11.022586 22737 layer_factory.hpp:76] Creating layer error_layer
I1006 15:42:11.022593 22737 net.cpp:110] Creating Layer error_layer
I1006 15:42:11.022598 22737 net.cpp:477] error_layer <- output_act_blob
I1006 15:42:11.022603 22737 net.cpp:477] error_layer <- label_blob
I1006 15:42:11.022608 22737 net.cpp:433] error_layer -> error_blob
I1006 15:42:11.022637 22737 net.cpp:155] Setting up error_layer
I1006 15:42:11.022644 22737 net.cpp:163] Top shape: (1)
I1006 15:42:11.022647 22737 net.cpp:168]     with loss weight 1
I1006 15:42:11.022660 22737 net.cpp:236] error_layer needs backward computation.
I1006 15:42:11.022668 22737 net.cpp:236] output_act_layer needs backward computation.
I1006 15:42:11.022675 22737 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:42:11.022677 22737 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:42:11.022680 22737 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:42:11.022685 22737 net.cpp:240] data_layer does not need backward computation.
I1006 15:42:11.022687 22737 net.cpp:283] This network produces output error_blob
I1006 15:42:11.022696 22737 net.cpp:297] Network initialization done.
I1006 15:42:11.022698 22737 net.cpp:298] Memory required for data: 1344004
I1006 15:42:11.022725 22737 solver.cpp:66] Solver scaffolding done.
I1006 15:42:11.022897 22737 caffe.cpp:212] Starting Optimization
I1006 15:42:11.022907 22737 solver.cpp:294] Solving full_batch_sce/model2_part2.prototxt
I1006 15:42:11.022909 22737 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:42:11.023066 22737 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:42:11.023125 22737 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:42:11.034221 22737 solver.cpp:415]     Test net output #0: error_blob = 0.731047 (* 1 = 0.731047 loss)
I1006 15:42:11.038275 22737 solver.cpp:243] Iteration 0, loss = 0.726789
I1006 15:42:11.038298 22737 solver.cpp:259]     Train net output #0: error_blob = 0.726789 (* 1 = 0.726789 loss)
I1006 15:42:11.038311 22737 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:42:15.862429 22737 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:42:15.863872 22737 solver.cpp:415]     Test net output #0: error_blob = 0.700615 (* 1 = 0.700615 loss)
I1006 15:42:15.916781 22737 solver.cpp:243] Iteration 100, loss = 0.69988
I1006 15:42:15.916815 22737 solver.cpp:259]     Train net output #0: error_blob = 0.69988 (* 1 = 0.69988 loss)
I1006 15:42:15.916822 22737 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:42:20.953179 22737 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:42:20.954615 22737 solver.cpp:415]     Test net output #0: error_blob = 0.689671 (* 1 = 0.689671 loss)
I1006 15:42:21.005964 22737 solver.cpp:243] Iteration 200, loss = 0.688911
I1006 15:42:21.006011 22737 solver.cpp:259]     Train net output #0: error_blob = 0.688911 (* 1 = 0.688911 loss)
I1006 15:42:21.006048 22737 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:42:25.985677 22737 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:42:25.987125 22737 solver.cpp:415]     Test net output #0: error_blob = 0.689551 (* 1 = 0.689551 loss)
I1006 15:42:26.036411 22737 solver.cpp:243] Iteration 300, loss = 0.682419
I1006 15:42:26.036442 22737 solver.cpp:259]     Train net output #0: error_blob = 0.682419 (* 1 = 0.682419 loss)
I1006 15:42:26.036448 22737 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:42:30.885617 22737 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:42:30.887073 22737 solver.cpp:415]     Test net output #0: error_blob = 0.686084 (* 1 = 0.686084 loss)
I1006 15:42:30.938645 22737 solver.cpp:243] Iteration 400, loss = 0.679629
I1006 15:42:30.938680 22737 solver.cpp:259]     Train net output #0: error_blob = 0.679629 (* 1 = 0.679629 loss)
I1006 15:42:30.938688 22737 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:42:35.909325 22737 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:42:35.910809 22737 solver.cpp:415]     Test net output #0: error_blob = 0.683303 (* 1 = 0.683303 loss)
I1006 15:42:35.959964 22737 solver.cpp:243] Iteration 500, loss = 0.676639
I1006 15:42:35.959993 22737 solver.cpp:259]     Train net output #0: error_blob = 0.676639 (* 1 = 0.676639 loss)
I1006 15:42:35.959998 22737 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:42:40.818166 22737 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:42:40.819670 22737 solver.cpp:415]     Test net output #0: error_blob = 0.679097 (* 1 = 0.679097 loss)
I1006 15:42:40.872421 22737 solver.cpp:243] Iteration 600, loss = 0.674484
I1006 15:42:40.872453 22737 solver.cpp:259]     Train net output #0: error_blob = 0.674484 (* 1 = 0.674484 loss)
I1006 15:42:40.872460 22737 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:42:45.728591 22737 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:42:45.730022 22737 solver.cpp:415]     Test net output #0: error_blob = 0.674499 (* 1 = 0.674499 loss)
I1006 15:42:45.779733 22737 solver.cpp:243] Iteration 700, loss = 0.672868
I1006 15:42:45.779767 22737 solver.cpp:259]     Train net output #0: error_blob = 0.672868 (* 1 = 0.672868 loss)
I1006 15:42:45.779774 22737 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:42:50.613656 22737 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:42:50.615114 22737 solver.cpp:415]     Test net output #0: error_blob = 0.673462 (* 1 = 0.673462 loss)
I1006 15:42:50.664322 22737 solver.cpp:243] Iteration 800, loss = 0.670957
I1006 15:42:50.664358 22737 solver.cpp:259]     Train net output #0: error_blob = 0.670957 (* 1 = 0.670957 loss)
I1006 15:42:50.664367 22737 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:42:55.493298 22737 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:42:55.494734 22737 solver.cpp:415]     Test net output #0: error_blob = 0.666273 (* 1 = 0.666273 loss)
I1006 15:42:55.546486 22737 solver.cpp:243] Iteration 900, loss = 0.672047
I1006 15:42:55.546516 22737 solver.cpp:259]     Train net output #0: error_blob = 0.672047 (* 1 = 0.672047 loss)
I1006 15:42:55.546524 22737 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:43:00.454841 22737 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:43:00.456169 22737 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:43:00.505662 22737 solver.cpp:327] Iteration 1000, loss = 0.666313
I1006 15:43:00.505702 22737 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:43:00.506208 22737 solver.cpp:415]     Test net output #0: error_blob = 0.664515 (* 1 = 0.664515 loss)
I1006 15:43:00.506218 22737 solver.cpp:332] Optimization Done.
I1006 15:43:00.506222 22737 caffe.cpp:215] Optimization Done.
I1006 15:43:00.639024 22758 caffe.cpp:184] Using GPUs 0
I1006 15:43:01.199084 22758 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part5.prototxt"
I1006 15:43:01.199115 22758 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part5.prototxt
I1006 15:43:01.199283 22758 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:43:01.199331 22758 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part5.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part5.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:43:01.199368 22758 layer_factory.hpp:76] Creating layer data_layer
I1006 15:43:01.225643 22758 net.cpp:110] Creating Layer data_layer
I1006 15:43:01.225680 22758 net.cpp:433] data_layer -> data_blob
I1006 15:43:01.225704 22758 net.cpp:433] data_layer -> label_blob
I1006 15:43:01.226307 22762 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part5.train
I1006 15:43:01.910632 22758 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:43:01.921155 22758 net.cpp:155] Setting up data_layer
I1006 15:43:01.921197 22758 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:43:01.921202 22758 net.cpp:163] Top shape: 40000 (40000)
I1006 15:43:01.921210 22758 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:43:01.921223 22758 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:43:01.921239 22758 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:43:01.921253 22758 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:43:01.921641 22758 net.cpp:155] Setting up hidden_sum_layer
I1006 15:43:01.921649 22758 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:43:01.921661 22758 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:43:01.921669 22758 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:43:01.921674 22758 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:43:01.921679 22758 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:43:05.161419 22758 net.cpp:155] Setting up hidden_act_layer
I1006 15:43:05.161442 22758 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:43:05.161448 22758 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:43:05.161458 22758 net.cpp:110] Creating Layer output_sum_layer
I1006 15:43:05.161461 22758 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:43:05.161468 22758 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:43:05.161571 22758 net.cpp:155] Setting up output_sum_layer
I1006 15:43:05.161578 22758 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:43:05.161586 22758 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:43:05.161591 22758 net.cpp:110] Creating Layer output_act_layer
I1006 15:43:05.161592 22758 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:43:05.161595 22758 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:43:05.161659 22758 net.cpp:155] Setting up output_act_layer
I1006 15:43:05.161681 22758 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:43:05.161684 22758 layer_factory.hpp:76] Creating layer error_layer
I1006 15:43:05.161689 22758 net.cpp:110] Creating Layer error_layer
I1006 15:43:05.161691 22758 net.cpp:477] error_layer <- output_act_blob
I1006 15:43:05.161695 22758 net.cpp:477] error_layer <- label_blob
I1006 15:43:05.161697 22758 net.cpp:433] error_layer -> error_blob
I1006 15:43:05.161732 22758 net.cpp:155] Setting up error_layer
I1006 15:43:05.161738 22758 net.cpp:163] Top shape: (1)
I1006 15:43:05.161741 22758 net.cpp:168]     with loss weight 1
I1006 15:43:05.161756 22758 net.cpp:236] error_layer needs backward computation.
I1006 15:43:05.161759 22758 net.cpp:236] output_act_layer needs backward computation.
I1006 15:43:05.161763 22758 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:43:05.161767 22758 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:43:05.161769 22758 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:43:05.161772 22758 net.cpp:240] data_layer does not need backward computation.
I1006 15:43:05.161777 22758 net.cpp:283] This network produces output error_blob
I1006 15:43:05.161783 22758 net.cpp:297] Network initialization done.
I1006 15:43:05.161788 22758 net.cpp:298] Memory required for data: 13440004
I1006 15:43:05.161909 22758 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part5.prototxt
I1006 15:43:05.161939 22758 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:43:05.161983 22758 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part5.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part5.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:43:05.162005 22758 layer_factory.hpp:76] Creating layer data_layer
I1006 15:43:05.164257 22758 net.cpp:110] Creating Layer data_layer
I1006 15:43:05.164263 22758 net.cpp:433] data_layer -> data_blob
I1006 15:43:05.164284 22758 net.cpp:433] data_layer -> label_blob
I1006 15:43:05.164870 22764 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part5.test
I1006 15:43:05.164953 22758 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:43:05.166754 22758 net.cpp:155] Setting up data_layer
I1006 15:43:05.166774 22758 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:43:05.166776 22758 net.cpp:163] Top shape: 4000 (4000)
I1006 15:43:05.166780 22758 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:43:05.166787 22758 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:43:05.166790 22758 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:43:05.166797 22758 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:43:05.166918 22758 net.cpp:155] Setting up hidden_sum_layer
I1006 15:43:05.166924 22758 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:43:05.166931 22758 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:43:05.166937 22758 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:43:05.166965 22758 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:43:05.166970 22758 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:43:05.167177 22758 net.cpp:155] Setting up hidden_act_layer
I1006 15:43:05.167183 22758 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:43:05.167186 22758 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:43:05.167191 22758 net.cpp:110] Creating Layer output_sum_layer
I1006 15:43:05.167193 22758 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:43:05.167198 22758 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:43:05.167265 22758 net.cpp:155] Setting up output_sum_layer
I1006 15:43:05.167271 22758 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:43:05.167278 22758 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:43:05.167282 22758 net.cpp:110] Creating Layer output_act_layer
I1006 15:43:05.167285 22758 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:43:05.167290 22758 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:43:05.167343 22758 net.cpp:155] Setting up output_act_layer
I1006 15:43:05.167348 22758 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:43:05.167351 22758 layer_factory.hpp:76] Creating layer error_layer
I1006 15:43:05.167356 22758 net.cpp:110] Creating Layer error_layer
I1006 15:43:05.167361 22758 net.cpp:477] error_layer <- output_act_blob
I1006 15:43:05.167364 22758 net.cpp:477] error_layer <- label_blob
I1006 15:43:05.167369 22758 net.cpp:433] error_layer -> error_blob
I1006 15:43:05.167394 22758 net.cpp:155] Setting up error_layer
I1006 15:43:05.167399 22758 net.cpp:163] Top shape: (1)
I1006 15:43:05.167402 22758 net.cpp:168]     with loss weight 1
I1006 15:43:05.167413 22758 net.cpp:236] error_layer needs backward computation.
I1006 15:43:05.167418 22758 net.cpp:236] output_act_layer needs backward computation.
I1006 15:43:05.167420 22758 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:43:05.167424 22758 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:43:05.167428 22758 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:43:05.167431 22758 net.cpp:240] data_layer does not need backward computation.
I1006 15:43:05.167434 22758 net.cpp:283] This network produces output error_blob
I1006 15:43:05.167440 22758 net.cpp:297] Network initialization done.
I1006 15:43:05.167443 22758 net.cpp:298] Memory required for data: 1344004
I1006 15:43:05.167466 22758 solver.cpp:66] Solver scaffolding done.
I1006 15:43:05.167557 22758 caffe.cpp:212] Starting Optimization
I1006 15:43:05.167562 22758 solver.cpp:294] Solving full_batch_sce/model2_part5.prototxt
I1006 15:43:05.167565 22758 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:43:05.167734 22758 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:43:05.167871 22758 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:43:05.173131 22758 solver.cpp:415]     Test net output #0: error_blob = 0.722864 (* 1 = 0.722864 loss)
I1006 15:43:05.177134 22758 solver.cpp:243] Iteration 0, loss = 0.731706
I1006 15:43:05.177151 22758 solver.cpp:259]     Train net output #0: error_blob = 0.731706 (* 1 = 0.731706 loss)
I1006 15:43:05.177160 22758 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:43:10.037504 22758 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:43:10.038959 22758 solver.cpp:415]     Test net output #0: error_blob = 0.688421 (* 1 = 0.688421 loss)
I1006 15:43:10.092211 22758 solver.cpp:243] Iteration 100, loss = 0.69312
I1006 15:43:10.092247 22758 solver.cpp:259]     Train net output #0: error_blob = 0.69312 (* 1 = 0.69312 loss)
I1006 15:43:10.092253 22758 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:43:15.030853 22758 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:43:15.032311 22758 solver.cpp:415]     Test net output #0: error_blob = 0.677301 (* 1 = 0.677301 loss)
I1006 15:43:15.086856 22758 solver.cpp:243] Iteration 200, loss = 0.683812
I1006 15:43:15.086896 22758 solver.cpp:259]     Train net output #0: error_blob = 0.683812 (* 1 = 0.683812 loss)
I1006 15:43:15.086921 22758 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:43:19.990293 22758 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:43:19.991763 22758 solver.cpp:415]     Test net output #0: error_blob = 0.673817 (* 1 = 0.673817 loss)
I1006 15:43:20.042804 22758 solver.cpp:243] Iteration 300, loss = 0.677162
I1006 15:43:20.042832 22758 solver.cpp:259]     Train net output #0: error_blob = 0.677162 (* 1 = 0.677162 loss)
I1006 15:43:20.042837 22758 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:43:25.036660 22758 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:43:25.038135 22758 solver.cpp:415]     Test net output #0: error_blob = 0.67056 (* 1 = 0.67056 loss)
I1006 15:43:25.094476 22758 solver.cpp:243] Iteration 400, loss = 0.67521
I1006 15:43:25.094506 22758 solver.cpp:259]     Train net output #0: error_blob = 0.67521 (* 1 = 0.67521 loss)
I1006 15:43:25.094511 22758 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:43:30.001220 22758 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:43:30.002710 22758 solver.cpp:415]     Test net output #0: error_blob = 0.667522 (* 1 = 0.667522 loss)
I1006 15:43:30.056610 22758 solver.cpp:243] Iteration 500, loss = 0.673404
I1006 15:43:30.056650 22758 solver.cpp:259]     Train net output #0: error_blob = 0.673404 (* 1 = 0.673404 loss)
I1006 15:43:30.056656 22758 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:43:34.967655 22758 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:43:34.969137 22758 solver.cpp:415]     Test net output #0: error_blob = 0.665203 (* 1 = 0.665203 loss)
I1006 15:43:35.020799 22758 solver.cpp:243] Iteration 600, loss = 0.671313
I1006 15:43:35.020828 22758 solver.cpp:259]     Train net output #0: error_blob = 0.671313 (* 1 = 0.671313 loss)
I1006 15:43:35.020833 22758 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:43:39.918927 22758 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:43:39.920413 22758 solver.cpp:415]     Test net output #0: error_blob = 0.661057 (* 1 = 0.661057 loss)
I1006 15:43:39.973096 22758 solver.cpp:243] Iteration 700, loss = 0.668952
I1006 15:43:39.973124 22758 solver.cpp:259]     Train net output #0: error_blob = 0.668952 (* 1 = 0.668952 loss)
I1006 15:43:39.973129 22758 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:43:44.851229 22758 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:43:44.852656 22758 solver.cpp:415]     Test net output #0: error_blob = 0.659176 (* 1 = 0.659176 loss)
I1006 15:43:44.903017 22758 solver.cpp:243] Iteration 800, loss = 0.667465
I1006 15:43:44.903048 22758 solver.cpp:259]     Train net output #0: error_blob = 0.667465 (* 1 = 0.667465 loss)
I1006 15:43:44.903055 22758 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:43:49.845373 22758 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:43:49.846791 22758 solver.cpp:415]     Test net output #0: error_blob = 0.657652 (* 1 = 0.657652 loss)
I1006 15:43:49.900521 22758 solver.cpp:243] Iteration 900, loss = 0.66609
I1006 15:43:49.900568 22758 solver.cpp:259]     Train net output #0: error_blob = 0.66609 (* 1 = 0.66609 loss)
I1006 15:43:49.900574 22758 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:43:54.844729 22758 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:43:54.846052 22758 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:43:54.893757 22758 solver.cpp:327] Iteration 1000, loss = 0.663111
I1006 15:43:54.893786 22758 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:43:54.894281 22758 solver.cpp:415]     Test net output #0: error_blob = 0.657154 (* 1 = 0.657154 loss)
I1006 15:43:54.894292 22758 solver.cpp:332] Optimization Done.
I1006 15:43:54.894296 22758 caffe.cpp:215] Optimization Done.
I1006 15:43:55.045305 22777 caffe.cpp:184] Using GPUs 0
I1006 15:43:55.607226 22777 solver.cpp:54] Initializing solver from parameters: 
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_full.prototxt"
I1006 15:43:55.607254 22777 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_full.prototxt
I1006 15:43:55.607429 22777 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_full.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.full"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:43:55.607478 22777 layer_factory.hpp:76] Creating layer data_layer
I1006 15:43:55.633786 22777 net.cpp:110] Creating Layer data_layer
I1006 15:43:55.633823 22777 net.cpp:433] data_layer -> data_blob
I1006 15:43:55.633855 22777 net.cpp:433] data_layer -> label_blob
I1006 15:43:55.634459 22781 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.full
I1006 15:43:56.317961 22777 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:43:56.328400 22777 net.cpp:155] Setting up data_layer
I1006 15:43:56.328441 22777 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:43:56.328445 22777 net.cpp:163] Top shape: 40000 (40000)
I1006 15:43:56.328461 22777 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:43:56.328474 22777 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:43:56.328477 22777 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:43:56.328491 22777 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:43:56.328898 22777 net.cpp:155] Setting up hidden_sum_layer
I1006 15:43:56.328907 22777 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:43:56.328928 22777 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:43:56.328945 22777 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:43:56.328948 22777 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:43:56.328951 22777 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:43:59.560966 22777 net.cpp:155] Setting up hidden_act_layer
I1006 15:43:59.560997 22777 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:43:59.561002 22777 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:43:59.561012 22777 net.cpp:110] Creating Layer output_sum_layer
I1006 15:43:59.561015 22777 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:43:59.561020 22777 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:43:59.561121 22777 net.cpp:155] Setting up output_sum_layer
I1006 15:43:59.561127 22777 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:43:59.561143 22777 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:43:59.561148 22777 net.cpp:110] Creating Layer output_act_layer
I1006 15:43:59.561151 22777 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:43:59.561153 22777 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:43:59.561220 22777 net.cpp:155] Setting up output_act_layer
I1006 15:43:59.561224 22777 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:43:59.561226 22777 layer_factory.hpp:76] Creating layer error_layer
I1006 15:43:59.561241 22777 net.cpp:110] Creating Layer error_layer
I1006 15:43:59.561259 22777 net.cpp:477] error_layer <- output_act_blob
I1006 15:43:59.561262 22777 net.cpp:477] error_layer <- label_blob
I1006 15:43:59.561265 22777 net.cpp:433] error_layer -> error_blob
I1006 15:43:59.561291 22777 net.cpp:155] Setting up error_layer
I1006 15:43:59.561305 22777 net.cpp:163] Top shape: (1)
I1006 15:43:59.561307 22777 net.cpp:168]     with loss weight 1
I1006 15:43:59.561331 22777 net.cpp:236] error_layer needs backward computation.
I1006 15:43:59.561333 22777 net.cpp:236] output_act_layer needs backward computation.
I1006 15:43:59.561336 22777 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:43:59.561337 22777 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:43:59.561341 22777 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:43:59.561342 22777 net.cpp:240] data_layer does not need backward computation.
I1006 15:43:59.561344 22777 net.cpp:283] This network produces output error_blob
I1006 15:43:59.561348 22777 net.cpp:297] Network initialization done.
I1006 15:43:59.561350 22777 net.cpp:298] Memory required for data: 13440004
I1006 15:43:59.561372 22777 solver.cpp:66] Solver scaffolding done.
I1006 15:43:59.561463 22777 caffe.cpp:212] Starting Optimization
I1006 15:43:59.561468 22777 solver.cpp:294] Solving full_batch_sce/model2_full.prototxt
I1006 15:43:59.561480 22777 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:43:59.565500 22777 solver.cpp:243] Iteration 0, loss = 0.720367
I1006 15:43:59.565523 22777 solver.cpp:259]     Train net output #0: error_blob = 0.720367 (* 1 = 0.720367 loss)
I1006 15:43:59.565531 22777 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:43:59.569288 22777 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:44:04.336037 22777 solver.cpp:243] Iteration 100, loss = 0.695304
I1006 15:44:04.336066 22777 solver.cpp:259]     Train net output #0: error_blob = 0.695304 (* 1 = 0.695304 loss)
I1006 15:44:04.336071 22777 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:44:09.163184 22777 solver.cpp:243] Iteration 200, loss = 0.685184
I1006 15:44:09.163213 22777 solver.cpp:259]     Train net output #0: error_blob = 0.685184 (* 1 = 0.685184 loss)
I1006 15:44:09.163218 22777 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:44:14.006789 22777 solver.cpp:243] Iteration 300, loss = 0.681685
I1006 15:44:14.006819 22777 solver.cpp:259]     Train net output #0: error_blob = 0.681685 (* 1 = 0.681685 loss)
I1006 15:44:14.006824 22777 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:44:18.809762 22777 solver.cpp:243] Iteration 400, loss = 0.676594
I1006 15:44:18.809790 22777 solver.cpp:259]     Train net output #0: error_blob = 0.676594 (* 1 = 0.676594 loss)
I1006 15:44:18.809795 22777 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:44:23.639015 22777 solver.cpp:243] Iteration 500, loss = 0.674276
I1006 15:44:23.639045 22777 solver.cpp:259]     Train net output #0: error_blob = 0.674276 (* 1 = 0.674276 loss)
I1006 15:44:23.639050 22777 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:44:28.460690 22777 solver.cpp:243] Iteration 600, loss = 0.673458
I1006 15:44:28.460749 22777 solver.cpp:259]     Train net output #0: error_blob = 0.673458 (* 1 = 0.673458 loss)
I1006 15:44:28.460754 22777 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:44:33.275898 22777 solver.cpp:243] Iteration 700, loss = 0.670822
I1006 15:44:33.275938 22777 solver.cpp:259]     Train net output #0: error_blob = 0.670822 (* 1 = 0.670822 loss)
I1006 15:44:33.275943 22777 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:44:38.134800 22777 solver.cpp:243] Iteration 800, loss = 0.669894
I1006 15:44:38.134829 22777 solver.cpp:259]     Train net output #0: error_blob = 0.669894 (* 1 = 0.669894 loss)
I1006 15:44:38.134834 22777 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:44:42.987197 22777 solver.cpp:243] Iteration 900, loss = 0.667367
I1006 15:44:42.987226 22777 solver.cpp:259]     Train net output #0: error_blob = 0.667367 (* 1 = 0.667367 loss)
I1006 15:44:42.987231 22777 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:44:47.728135 22777 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:44:47.730335 22777 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:44:47.775862 22777 solver.cpp:327] Iteration 1000, loss = 0.666218
I1006 15:44:47.775887 22777 solver.cpp:332] Optimization Done.
I1006 15:44:47.775890 22777 caffe.cpp:215] Optimization Done.
I1006 15:44:47.916250 22794 caffe.cpp:184] Using GPUs 0
I1006 15:44:48.476366 22794 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part9.prototxt"
I1006 15:44:48.476397 22794 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part9.prototxt
I1006 15:44:48.476604 22794 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:44:48.476654 22794 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part9.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part9.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:44:48.476732 22794 layer_factory.hpp:76] Creating layer data_layer
I1006 15:44:48.503031 22794 net.cpp:110] Creating Layer data_layer
I1006 15:44:48.503060 22794 net.cpp:433] data_layer -> data_blob
I1006 15:44:48.503093 22794 net.cpp:433] data_layer -> label_blob
I1006 15:44:48.503676 22799 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part9.train
I1006 15:44:49.190779 22794 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:44:49.201172 22794 net.cpp:155] Setting up data_layer
I1006 15:44:49.201236 22794 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:44:49.201241 22794 net.cpp:163] Top shape: 40000 (40000)
I1006 15:44:49.201248 22794 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:44:49.201262 22794 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:44:49.201267 22794 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:44:49.201277 22794 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:44:49.201665 22794 net.cpp:155] Setting up hidden_sum_layer
I1006 15:44:49.201673 22794 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:44:49.201695 22794 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:44:49.201714 22794 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:44:49.201719 22794 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:44:49.201722 22794 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:44:52.412186 22794 net.cpp:155] Setting up hidden_act_layer
I1006 15:44:52.412209 22794 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:44:52.412215 22794 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:44:52.412225 22794 net.cpp:110] Creating Layer output_sum_layer
I1006 15:44:52.412228 22794 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:44:52.412245 22794 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:44:52.412343 22794 net.cpp:155] Setting up output_sum_layer
I1006 15:44:52.412351 22794 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:44:52.412360 22794 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:44:52.412365 22794 net.cpp:110] Creating Layer output_act_layer
I1006 15:44:52.412367 22794 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:44:52.412380 22794 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:44:52.412446 22794 net.cpp:155] Setting up output_act_layer
I1006 15:44:52.412462 22794 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:44:52.412466 22794 layer_factory.hpp:76] Creating layer error_layer
I1006 15:44:52.412470 22794 net.cpp:110] Creating Layer error_layer
I1006 15:44:52.412472 22794 net.cpp:477] error_layer <- output_act_blob
I1006 15:44:52.412474 22794 net.cpp:477] error_layer <- label_blob
I1006 15:44:52.412478 22794 net.cpp:433] error_layer -> error_blob
I1006 15:44:52.412525 22794 net.cpp:155] Setting up error_layer
I1006 15:44:52.412530 22794 net.cpp:163] Top shape: (1)
I1006 15:44:52.412544 22794 net.cpp:168]     with loss weight 1
I1006 15:44:52.412561 22794 net.cpp:236] error_layer needs backward computation.
I1006 15:44:52.412564 22794 net.cpp:236] output_act_layer needs backward computation.
I1006 15:44:52.412576 22794 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:44:52.412577 22794 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:44:52.412580 22794 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:44:52.412581 22794 net.cpp:240] data_layer does not need backward computation.
I1006 15:44:52.412583 22794 net.cpp:283] This network produces output error_blob
I1006 15:44:52.412588 22794 net.cpp:297] Network initialization done.
I1006 15:44:52.412590 22794 net.cpp:298] Memory required for data: 13440004
I1006 15:44:52.412714 22794 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part9.prototxt
I1006 15:44:52.412725 22794 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:44:52.412765 22794 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part9.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part9.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:44:52.412814 22794 layer_factory.hpp:76] Creating layer data_layer
I1006 15:44:52.415113 22794 net.cpp:110] Creating Layer data_layer
I1006 15:44:52.415119 22794 net.cpp:433] data_layer -> data_blob
I1006 15:44:52.415141 22794 net.cpp:433] data_layer -> label_blob
I1006 15:44:52.415755 22801 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part9.test
I1006 15:44:52.415827 22794 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:44:52.417709 22794 net.cpp:155] Setting up data_layer
I1006 15:44:52.417743 22794 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:44:52.417747 22794 net.cpp:163] Top shape: 4000 (4000)
I1006 15:44:52.417750 22794 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:44:52.417757 22794 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:44:52.417759 22794 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:44:52.417763 22794 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:44:52.417892 22794 net.cpp:155] Setting up hidden_sum_layer
I1006 15:44:52.417897 22794 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:44:52.417914 22794 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:44:52.417919 22794 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:44:52.417933 22794 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:44:52.417937 22794 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:44:52.418105 22794 net.cpp:155] Setting up hidden_act_layer
I1006 15:44:52.418112 22794 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:44:52.418125 22794 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:44:52.418129 22794 net.cpp:110] Creating Layer output_sum_layer
I1006 15:44:52.418131 22794 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:44:52.418135 22794 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:44:52.418202 22794 net.cpp:155] Setting up output_sum_layer
I1006 15:44:52.418207 22794 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:44:52.418223 22794 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:44:52.418227 22794 net.cpp:110] Creating Layer output_act_layer
I1006 15:44:52.418229 22794 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:44:52.418232 22794 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:44:52.418289 22794 net.cpp:155] Setting up output_act_layer
I1006 15:44:52.418293 22794 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:44:52.418305 22794 layer_factory.hpp:76] Creating layer error_layer
I1006 15:44:52.418310 22794 net.cpp:110] Creating Layer error_layer
I1006 15:44:52.418313 22794 net.cpp:477] error_layer <- output_act_blob
I1006 15:44:52.418314 22794 net.cpp:477] error_layer <- label_blob
I1006 15:44:52.418318 22794 net.cpp:433] error_layer -> error_blob
I1006 15:44:52.418336 22794 net.cpp:155] Setting up error_layer
I1006 15:44:52.418340 22794 net.cpp:163] Top shape: (1)
I1006 15:44:52.418342 22794 net.cpp:168]     with loss weight 1
I1006 15:44:52.418350 22794 net.cpp:236] error_layer needs backward computation.
I1006 15:44:52.418352 22794 net.cpp:236] output_act_layer needs backward computation.
I1006 15:44:52.418354 22794 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:44:52.418356 22794 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:44:52.418359 22794 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:44:52.418360 22794 net.cpp:240] data_layer does not need backward computation.
I1006 15:44:52.418362 22794 net.cpp:283] This network produces output error_blob
I1006 15:44:52.418366 22794 net.cpp:297] Network initialization done.
I1006 15:44:52.418368 22794 net.cpp:298] Memory required for data: 1344004
I1006 15:44:52.418388 22794 solver.cpp:66] Solver scaffolding done.
I1006 15:44:52.418506 22794 caffe.cpp:212] Starting Optimization
I1006 15:44:52.418526 22794 solver.cpp:294] Solving full_batch_sce/model2_part9.prototxt
I1006 15:44:52.418529 22794 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:44:52.418684 22794 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:44:52.418761 22794 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:44:52.423877 22794 solver.cpp:415]     Test net output #0: error_blob = 0.719903 (* 1 = 0.719903 loss)
I1006 15:44:52.427647 22794 solver.cpp:243] Iteration 0, loss = 0.716557
I1006 15:44:52.427670 22794 solver.cpp:259]     Train net output #0: error_blob = 0.716557 (* 1 = 0.716557 loss)
I1006 15:44:52.427678 22794 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:44:57.276893 22794 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:44:57.278311 22794 solver.cpp:415]     Test net output #0: error_blob = 0.695566 (* 1 = 0.695566 loss)
I1006 15:44:57.330943 22794 solver.cpp:243] Iteration 100, loss = 0.694427
I1006 15:44:57.330981 22794 solver.cpp:259]     Train net output #0: error_blob = 0.694427 (* 1 = 0.694427 loss)
I1006 15:44:57.330988 22794 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:45:02.194296 22794 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:45:02.195732 22794 solver.cpp:415]     Test net output #0: error_blob = 0.683435 (* 1 = 0.683435 loss)
I1006 15:45:02.244412 22794 solver.cpp:243] Iteration 200, loss = 0.684531
I1006 15:45:02.244444 22794 solver.cpp:259]     Train net output #0: error_blob = 0.684531 (* 1 = 0.684531 loss)
I1006 15:45:02.244473 22794 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:45:07.167645 22794 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:45:07.169061 22794 solver.cpp:415]     Test net output #0: error_blob = 0.679431 (* 1 = 0.679431 loss)
I1006 15:45:07.217969 22794 solver.cpp:243] Iteration 300, loss = 0.681736
I1006 15:45:07.218001 22794 solver.cpp:259]     Train net output #0: error_blob = 0.681736 (* 1 = 0.681736 loss)
I1006 15:45:07.218008 22794 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:45:12.122566 22794 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:45:12.124003 22794 solver.cpp:415]     Test net output #0: error_blob = 0.677207 (* 1 = 0.677207 loss)
I1006 15:45:12.177785 22794 solver.cpp:243] Iteration 400, loss = 0.676319
I1006 15:45:12.177829 22794 solver.cpp:259]     Train net output #0: error_blob = 0.676319 (* 1 = 0.676319 loss)
I1006 15:45:12.177835 22794 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:45:17.128149 22794 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:45:17.129657 22794 solver.cpp:415]     Test net output #0: error_blob = 0.679391 (* 1 = 0.679391 loss)
I1006 15:45:17.180181 22794 solver.cpp:243] Iteration 500, loss = 0.67318
I1006 15:45:17.180218 22794 solver.cpp:259]     Train net output #0: error_blob = 0.67318 (* 1 = 0.67318 loss)
I1006 15:45:17.180227 22794 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:45:22.020306 22794 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:45:22.021733 22794 solver.cpp:415]     Test net output #0: error_blob = 0.677533 (* 1 = 0.677533 loss)
I1006 15:45:22.070886 22794 solver.cpp:243] Iteration 600, loss = 0.670163
I1006 15:45:22.070915 22794 solver.cpp:259]     Train net output #0: error_blob = 0.670163 (* 1 = 0.670163 loss)
I1006 15:45:22.070920 22794 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:45:27.043205 22794 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:45:27.044675 22794 solver.cpp:415]     Test net output #0: error_blob = 0.675106 (* 1 = 0.675106 loss)
I1006 15:45:27.096860 22794 solver.cpp:243] Iteration 700, loss = 0.667532
I1006 15:45:27.096892 22794 solver.cpp:259]     Train net output #0: error_blob = 0.667532 (* 1 = 0.667532 loss)
I1006 15:45:27.096899 22794 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:45:32.051239 22794 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:45:32.052670 22794 solver.cpp:415]     Test net output #0: error_blob = 0.669332 (* 1 = 0.669332 loss)
I1006 15:45:32.104426 22794 solver.cpp:243] Iteration 800, loss = 0.666373
I1006 15:45:32.104460 22794 solver.cpp:259]     Train net output #0: error_blob = 0.666373 (* 1 = 0.666373 loss)
I1006 15:45:32.104465 22794 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:45:37.121161 22794 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:45:37.122619 22794 solver.cpp:415]     Test net output #0: error_blob = 0.673556 (* 1 = 0.673556 loss)
I1006 15:45:37.174165 22794 solver.cpp:243] Iteration 900, loss = 0.666753
I1006 15:45:37.174198 22794 solver.cpp:259]     Train net output #0: error_blob = 0.666753 (* 1 = 0.666753 loss)
I1006 15:45:37.174206 22794 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:45:42.204392 22794 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:45:42.205742 22794 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:45:42.254088 22794 solver.cpp:327] Iteration 1000, loss = 0.661653
I1006 15:45:42.254115 22794 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:45:42.254555 22794 solver.cpp:415]     Test net output #0: error_blob = 0.669104 (* 1 = 0.669104 loss)
I1006 15:45:42.254564 22794 solver.cpp:332] Optimization Done.
I1006 15:45:42.254567 22794 caffe.cpp:215] Optimization Done.
I1006 15:45:42.403450 22814 caffe.cpp:184] Using GPUs 0
I1006 15:45:42.968183 22814 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part0.prototxt"
I1006 15:45:42.968214 22814 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part0.prototxt
I1006 15:45:42.968384 22814 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:45:42.968430 22814 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part0.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part0.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:45:42.968508 22814 layer_factory.hpp:76] Creating layer data_layer
I1006 15:45:42.994951 22814 net.cpp:110] Creating Layer data_layer
I1006 15:45:42.994981 22814 net.cpp:433] data_layer -> data_blob
I1006 15:45:42.995007 22814 net.cpp:433] data_layer -> label_blob
I1006 15:45:42.995580 22818 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part0.train
I1006 15:45:43.680903 22814 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:45:43.691318 22814 net.cpp:155] Setting up data_layer
I1006 15:45:43.691359 22814 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:45:43.691365 22814 net.cpp:163] Top shape: 40000 (40000)
I1006 15:45:43.691371 22814 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:45:43.691385 22814 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:45:43.691393 22814 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:45:43.691406 22814 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:45:43.691802 22814 net.cpp:155] Setting up hidden_sum_layer
I1006 15:45:43.691810 22814 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:45:43.691831 22814 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:45:43.691841 22814 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:45:43.691845 22814 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:45:43.691854 22814 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:45:46.917718 22814 net.cpp:155] Setting up hidden_act_layer
I1006 15:45:46.917742 22814 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:45:46.917747 22814 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:45:46.917757 22814 net.cpp:110] Creating Layer output_sum_layer
I1006 15:45:46.917759 22814 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:45:46.917765 22814 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:45:46.917872 22814 net.cpp:155] Setting up output_sum_layer
I1006 15:45:46.917879 22814 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:45:46.917886 22814 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:45:46.917891 22814 net.cpp:110] Creating Layer output_act_layer
I1006 15:45:46.917893 22814 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:45:46.917896 22814 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:45:46.917960 22814 net.cpp:155] Setting up output_act_layer
I1006 15:45:46.917980 22814 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:45:46.917982 22814 layer_factory.hpp:76] Creating layer error_layer
I1006 15:45:46.917989 22814 net.cpp:110] Creating Layer error_layer
I1006 15:45:46.917990 22814 net.cpp:477] error_layer <- output_act_blob
I1006 15:45:46.917992 22814 net.cpp:477] error_layer <- label_blob
I1006 15:45:46.917996 22814 net.cpp:433] error_layer -> error_blob
I1006 15:45:46.918030 22814 net.cpp:155] Setting up error_layer
I1006 15:45:46.918037 22814 net.cpp:163] Top shape: (1)
I1006 15:45:46.918040 22814 net.cpp:168]     with loss weight 1
I1006 15:45:46.918054 22814 net.cpp:236] error_layer needs backward computation.
I1006 15:45:46.918056 22814 net.cpp:236] output_act_layer needs backward computation.
I1006 15:45:46.918058 22814 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:45:46.918061 22814 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:45:46.918064 22814 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:45:46.918067 22814 net.cpp:240] data_layer does not need backward computation.
I1006 15:45:46.918071 22814 net.cpp:283] This network produces output error_blob
I1006 15:45:46.918077 22814 net.cpp:297] Network initialization done.
I1006 15:45:46.918081 22814 net.cpp:298] Memory required for data: 13440004
I1006 15:45:46.918215 22814 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part0.prototxt
I1006 15:45:46.918231 22814 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:45:46.918277 22814 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part0.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part0.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:45:46.918303 22814 layer_factory.hpp:76] Creating layer data_layer
I1006 15:45:46.920545 22814 net.cpp:110] Creating Layer data_layer
I1006 15:45:46.920562 22814 net.cpp:433] data_layer -> data_blob
I1006 15:45:46.920567 22814 net.cpp:433] data_layer -> label_blob
I1006 15:45:46.921120 22820 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part0.test
I1006 15:45:46.921191 22814 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:45:46.923022 22814 net.cpp:155] Setting up data_layer
I1006 15:45:46.923043 22814 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:45:46.923046 22814 net.cpp:163] Top shape: 4000 (4000)
I1006 15:45:46.923049 22814 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:45:46.923058 22814 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:45:46.923061 22814 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:45:46.923066 22814 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:45:46.923197 22814 net.cpp:155] Setting up hidden_sum_layer
I1006 15:45:46.923202 22814 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:45:46.923209 22814 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:45:46.923215 22814 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:45:46.923233 22814 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:45:46.923238 22814 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:45:46.923404 22814 net.cpp:155] Setting up hidden_act_layer
I1006 15:45:46.923411 22814 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:45:46.923414 22814 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:45:46.923419 22814 net.cpp:110] Creating Layer output_sum_layer
I1006 15:45:46.923424 22814 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:45:46.923429 22814 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:45:46.923494 22814 net.cpp:155] Setting up output_sum_layer
I1006 15:45:46.923499 22814 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:45:46.923506 22814 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:45:46.923511 22814 net.cpp:110] Creating Layer output_act_layer
I1006 15:45:46.923516 22814 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:45:46.923521 22814 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:45:46.923573 22814 net.cpp:155] Setting up output_act_layer
I1006 15:45:46.923578 22814 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:45:46.923580 22814 layer_factory.hpp:76] Creating layer error_layer
I1006 15:45:46.923586 22814 net.cpp:110] Creating Layer error_layer
I1006 15:45:46.923589 22814 net.cpp:477] error_layer <- output_act_blob
I1006 15:45:46.923593 22814 net.cpp:477] error_layer <- label_blob
I1006 15:45:46.923599 22814 net.cpp:433] error_layer -> error_blob
I1006 15:45:46.923624 22814 net.cpp:155] Setting up error_layer
I1006 15:45:46.923629 22814 net.cpp:163] Top shape: (1)
I1006 15:45:46.923630 22814 net.cpp:168]     with loss weight 1
I1006 15:45:46.923640 22814 net.cpp:236] error_layer needs backward computation.
I1006 15:45:46.923645 22814 net.cpp:236] output_act_layer needs backward computation.
I1006 15:45:46.923647 22814 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:45:46.923651 22814 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:45:46.923655 22814 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:45:46.923657 22814 net.cpp:240] data_layer does not need backward computation.
I1006 15:45:46.923660 22814 net.cpp:283] This network produces output error_blob
I1006 15:45:46.923667 22814 net.cpp:297] Network initialization done.
I1006 15:45:46.923671 22814 net.cpp:298] Memory required for data: 1344004
I1006 15:45:46.923692 22814 solver.cpp:66] Solver scaffolding done.
I1006 15:45:46.923784 22814 caffe.cpp:212] Starting Optimization
I1006 15:45:46.923789 22814 solver.cpp:294] Solving full_batch_sce/model2_part0.prototxt
I1006 15:45:46.923791 22814 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:45:46.923956 22814 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:45:46.924094 22814 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:45:46.928920 22814 solver.cpp:415]     Test net output #0: error_blob = 0.752739 (* 1 = 0.752739 loss)
I1006 15:45:46.932664 22814 solver.cpp:243] Iteration 0, loss = 0.7323
I1006 15:45:46.932682 22814 solver.cpp:259]     Train net output #0: error_blob = 0.7323 (* 1 = 0.7323 loss)
I1006 15:45:46.932693 22814 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:45:51.748237 22814 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:45:51.749670 22814 solver.cpp:415]     Test net output #0: error_blob = 0.711878 (* 1 = 0.711878 loss)
I1006 15:45:51.804246 22814 solver.cpp:243] Iteration 100, loss = 0.701059
I1006 15:45:51.804280 22814 solver.cpp:259]     Train net output #0: error_blob = 0.701059 (* 1 = 0.701059 loss)
I1006 15:45:51.804287 22814 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:45:56.742156 22814 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:45:56.743577 22814 solver.cpp:415]     Test net output #0: error_blob = 0.69439 (* 1 = 0.69439 loss)
I1006 15:45:56.797899 22814 solver.cpp:243] Iteration 200, loss = 0.688925
I1006 15:45:56.797931 22814 solver.cpp:259]     Train net output #0: error_blob = 0.688925 (* 1 = 0.688925 loss)
I1006 15:45:56.797967 22814 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:46:01.727490 22814 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:46:01.728929 22814 solver.cpp:415]     Test net output #0: error_blob = 0.700348 (* 1 = 0.700348 loss)
I1006 15:46:01.780019 22814 solver.cpp:243] Iteration 300, loss = 0.683596
I1006 15:46:01.780053 22814 solver.cpp:259]     Train net output #0: error_blob = 0.683596 (* 1 = 0.683596 loss)
I1006 15:46:01.780061 22814 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:46:06.791738 22814 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:46:06.793191 22814 solver.cpp:415]     Test net output #0: error_blob = 0.684379 (* 1 = 0.684379 loss)
I1006 15:46:06.848732 22814 solver.cpp:243] Iteration 400, loss = 0.682166
I1006 15:46:06.848768 22814 solver.cpp:259]     Train net output #0: error_blob = 0.682166 (* 1 = 0.682166 loss)
I1006 15:46:06.848776 22814 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:46:11.821683 22814 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:46:11.823128 22814 solver.cpp:415]     Test net output #0: error_blob = 0.683022 (* 1 = 0.683022 loss)
I1006 15:46:11.876047 22814 solver.cpp:243] Iteration 500, loss = 0.679031
I1006 15:46:11.876082 22814 solver.cpp:259]     Train net output #0: error_blob = 0.679031 (* 1 = 0.679031 loss)
I1006 15:46:11.876090 22814 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:46:16.912848 22814 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:46:16.914302 22814 solver.cpp:415]     Test net output #0: error_blob = 0.695678 (* 1 = 0.695678 loss)
I1006 15:46:16.966788 22814 solver.cpp:243] Iteration 600, loss = 0.676708
I1006 15:46:16.966821 22814 solver.cpp:259]     Train net output #0: error_blob = 0.676708 (* 1 = 0.676708 loss)
I1006 15:46:16.966828 22814 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:46:21.986369 22814 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:46:21.987823 22814 solver.cpp:415]     Test net output #0: error_blob = 0.688793 (* 1 = 0.688793 loss)
I1006 15:46:22.043367 22814 solver.cpp:243] Iteration 700, loss = 0.674239
I1006 15:46:22.043406 22814 solver.cpp:259]     Train net output #0: error_blob = 0.674239 (* 1 = 0.674239 loss)
I1006 15:46:22.043411 22814 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:46:26.934123 22814 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:46:26.935570 22814 solver.cpp:415]     Test net output #0: error_blob = 0.685509 (* 1 = 0.685509 loss)
I1006 15:46:26.990725 22814 solver.cpp:243] Iteration 800, loss = 0.674055
I1006 15:46:26.990756 22814 solver.cpp:259]     Train net output #0: error_blob = 0.674055 (* 1 = 0.674055 loss)
I1006 15:46:26.990764 22814 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:46:31.901453 22814 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:46:31.902907 22814 solver.cpp:415]     Test net output #0: error_blob = 0.684346 (* 1 = 0.684346 loss)
I1006 15:46:31.956050 22814 solver.cpp:243] Iteration 900, loss = 0.671395
I1006 15:46:31.956079 22814 solver.cpp:259]     Train net output #0: error_blob = 0.671395 (* 1 = 0.671395 loss)
I1006 15:46:31.956085 22814 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:46:36.822178 22814 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:46:36.824440 22814 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:46:36.871984 22814 solver.cpp:327] Iteration 1000, loss = 0.669266
I1006 15:46:36.872009 22814 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:46:36.872457 22814 solver.cpp:415]     Test net output #0: error_blob = 0.683435 (* 1 = 0.683435 loss)
I1006 15:46:36.872467 22814 solver.cpp:332] Optimization Done.
I1006 15:46:36.872473 22814 caffe.cpp:215] Optimization Done.
I1006 15:46:37.017191 22833 caffe.cpp:184] Using GPUs 0
I1006 15:46:37.578193 22833 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part1.prototxt"
I1006 15:46:37.578224 22833 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part1.prototxt
I1006 15:46:37.578397 22833 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:46:37.578444 22833 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part1.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part1.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:46:37.578480 22833 layer_factory.hpp:76] Creating layer data_layer
I1006 15:46:37.604755 22833 net.cpp:110] Creating Layer data_layer
I1006 15:46:37.604786 22833 net.cpp:433] data_layer -> data_blob
I1006 15:46:37.604818 22833 net.cpp:433] data_layer -> label_blob
I1006 15:46:37.605423 22837 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part1.train
I1006 15:46:38.293398 22833 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:46:38.303864 22833 net.cpp:155] Setting up data_layer
I1006 15:46:38.303930 22833 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:46:38.303935 22833 net.cpp:163] Top shape: 40000 (40000)
I1006 15:46:38.303941 22833 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:46:38.303951 22833 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:46:38.303956 22833 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:46:38.303966 22833 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:46:38.304368 22833 net.cpp:155] Setting up hidden_sum_layer
I1006 15:46:38.304376 22833 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:46:38.304397 22833 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:46:38.304414 22833 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:46:38.304416 22833 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:46:38.304419 22833 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:46:41.523427 22833 net.cpp:155] Setting up hidden_act_layer
I1006 15:46:41.523452 22833 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:46:41.523457 22833 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:46:41.523465 22833 net.cpp:110] Creating Layer output_sum_layer
I1006 15:46:41.523468 22833 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:46:41.523474 22833 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:46:41.523564 22833 net.cpp:155] Setting up output_sum_layer
I1006 15:46:41.523571 22833 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:46:41.523578 22833 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:46:41.523583 22833 net.cpp:110] Creating Layer output_act_layer
I1006 15:46:41.523584 22833 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:46:41.523587 22833 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:46:41.523643 22833 net.cpp:155] Setting up output_act_layer
I1006 15:46:41.523664 22833 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:46:41.523666 22833 layer_factory.hpp:76] Creating layer error_layer
I1006 15:46:41.523672 22833 net.cpp:110] Creating Layer error_layer
I1006 15:46:41.523674 22833 net.cpp:477] error_layer <- output_act_blob
I1006 15:46:41.523676 22833 net.cpp:477] error_layer <- label_blob
I1006 15:46:41.523680 22833 net.cpp:433] error_layer -> error_blob
I1006 15:46:41.523705 22833 net.cpp:155] Setting up error_layer
I1006 15:46:41.523707 22833 net.cpp:163] Top shape: (1)
I1006 15:46:41.523710 22833 net.cpp:168]     with loss weight 1
I1006 15:46:41.523723 22833 net.cpp:236] error_layer needs backward computation.
I1006 15:46:41.523726 22833 net.cpp:236] output_act_layer needs backward computation.
I1006 15:46:41.523727 22833 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:46:41.523730 22833 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:46:41.523731 22833 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:46:41.523733 22833 net.cpp:240] data_layer does not need backward computation.
I1006 15:46:41.523736 22833 net.cpp:283] This network produces output error_blob
I1006 15:46:41.523740 22833 net.cpp:297] Network initialization done.
I1006 15:46:41.523741 22833 net.cpp:298] Memory required for data: 13440004
I1006 15:46:41.523864 22833 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part1.prototxt
I1006 15:46:41.523875 22833 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:46:41.523908 22833 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part1.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part1.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:46:41.523928 22833 layer_factory.hpp:76] Creating layer data_layer
I1006 15:46:41.526170 22833 net.cpp:110] Creating Layer data_layer
I1006 15:46:41.526176 22833 net.cpp:433] data_layer -> data_blob
I1006 15:46:41.526196 22833 net.cpp:433] data_layer -> label_blob
I1006 15:46:41.526778 22839 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part1.test
I1006 15:46:41.526881 22833 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:46:41.528836 22833 net.cpp:155] Setting up data_layer
I1006 15:46:41.528847 22833 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:46:41.528851 22833 net.cpp:163] Top shape: 4000 (4000)
I1006 15:46:41.528853 22833 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:46:41.528872 22833 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:46:41.528874 22833 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:46:41.528879 22833 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:46:41.529005 22833 net.cpp:155] Setting up hidden_sum_layer
I1006 15:46:41.529009 22833 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:46:41.529016 22833 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:46:41.529031 22833 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:46:41.529047 22833 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:46:41.529052 22833 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:46:41.529233 22833 net.cpp:155] Setting up hidden_act_layer
I1006 15:46:41.529240 22833 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:46:41.529242 22833 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:46:41.529258 22833 net.cpp:110] Creating Layer output_sum_layer
I1006 15:46:41.529259 22833 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:46:41.529263 22833 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:46:41.529330 22833 net.cpp:155] Setting up output_sum_layer
I1006 15:46:41.529335 22833 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:46:41.529338 22833 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:46:41.529352 22833 net.cpp:110] Creating Layer output_act_layer
I1006 15:46:41.529355 22833 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:46:41.529357 22833 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:46:41.529414 22833 net.cpp:155] Setting up output_act_layer
I1006 15:46:41.529418 22833 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:46:41.529420 22833 layer_factory.hpp:76] Creating layer error_layer
I1006 15:46:41.529424 22833 net.cpp:110] Creating Layer error_layer
I1006 15:46:41.529436 22833 net.cpp:477] error_layer <- output_act_blob
I1006 15:46:41.529439 22833 net.cpp:477] error_layer <- label_blob
I1006 15:46:41.529443 22833 net.cpp:433] error_layer -> error_blob
I1006 15:46:41.529463 22833 net.cpp:155] Setting up error_layer
I1006 15:46:41.529465 22833 net.cpp:163] Top shape: (1)
I1006 15:46:41.529467 22833 net.cpp:168]     with loss weight 1
I1006 15:46:41.529484 22833 net.cpp:236] error_layer needs backward computation.
I1006 15:46:41.529485 22833 net.cpp:236] output_act_layer needs backward computation.
I1006 15:46:41.529487 22833 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:46:41.529489 22833 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:46:41.529501 22833 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:46:41.529505 22833 net.cpp:240] data_layer does not need backward computation.
I1006 15:46:41.529505 22833 net.cpp:283] This network produces output error_blob
I1006 15:46:41.529510 22833 net.cpp:297] Network initialization done.
I1006 15:46:41.529511 22833 net.cpp:298] Memory required for data: 1344004
I1006 15:46:41.529531 22833 solver.cpp:66] Solver scaffolding done.
I1006 15:46:41.529628 22833 caffe.cpp:212] Starting Optimization
I1006 15:46:41.529634 22833 solver.cpp:294] Solving full_batch_sce/model2_part1.prototxt
I1006 15:46:41.529635 22833 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:46:41.529844 22833 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:46:41.529995 22833 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:46:41.534987 22833 solver.cpp:415]     Test net output #0: error_blob = 0.718106 (* 1 = 0.718106 loss)
I1006 15:46:41.539016 22833 solver.cpp:243] Iteration 0, loss = 0.728441
I1006 15:46:41.539038 22833 solver.cpp:259]     Train net output #0: error_blob = 0.728441 (* 1 = 0.728441 loss)
I1006 15:46:41.539048 22833 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:46:46.332425 22833 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:46:46.333863 22833 solver.cpp:415]     Test net output #0: error_blob = 0.688055 (* 1 = 0.688055 loss)
I1006 15:46:46.384738 22833 solver.cpp:243] Iteration 100, loss = 0.695267
I1006 15:46:46.384766 22833 solver.cpp:259]     Train net output #0: error_blob = 0.695267 (* 1 = 0.695267 loss)
I1006 15:46:46.384773 22833 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:46:51.331429 22833 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:46:51.332895 22833 solver.cpp:415]     Test net output #0: error_blob = 0.686963 (* 1 = 0.686963 loss)
I1006 15:46:51.384992 22833 solver.cpp:243] Iteration 200, loss = 0.683107
I1006 15:46:51.385020 22833 solver.cpp:259]     Train net output #0: error_blob = 0.683107 (* 1 = 0.683107 loss)
I1006 15:46:51.385048 22833 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:46:56.409528 22833 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:46:56.410984 22833 solver.cpp:415]     Test net output #0: error_blob = 0.683859 (* 1 = 0.683859 loss)
I1006 15:46:56.463944 22833 solver.cpp:243] Iteration 300, loss = 0.678364
I1006 15:46:56.463980 22833 solver.cpp:259]     Train net output #0: error_blob = 0.678364 (* 1 = 0.678364 loss)
I1006 15:46:56.463987 22833 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:47:01.558135 22833 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:47:01.559578 22833 solver.cpp:415]     Test net output #0: error_blob = 0.674232 (* 1 = 0.674232 loss)
I1006 15:47:01.611906 22833 solver.cpp:243] Iteration 400, loss = 0.674275
I1006 15:47:01.611935 22833 solver.cpp:259]     Train net output #0: error_blob = 0.674275 (* 1 = 0.674275 loss)
I1006 15:47:01.611943 22833 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:47:06.624119 22833 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:47:06.625540 22833 solver.cpp:415]     Test net output #0: error_blob = 0.675291 (* 1 = 0.675291 loss)
I1006 15:47:06.675508 22833 solver.cpp:243] Iteration 500, loss = 0.672639
I1006 15:47:06.675539 22833 solver.cpp:259]     Train net output #0: error_blob = 0.672639 (* 1 = 0.672639 loss)
I1006 15:47:06.675546 22833 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:47:11.666842 22833 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:47:11.668311 22833 solver.cpp:415]     Test net output #0: error_blob = 0.664072 (* 1 = 0.664072 loss)
I1006 15:47:11.722273 22833 solver.cpp:243] Iteration 600, loss = 0.670009
I1006 15:47:11.722311 22833 solver.cpp:259]     Train net output #0: error_blob = 0.670009 (* 1 = 0.670009 loss)
I1006 15:47:11.722318 22833 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:47:16.653926 22833 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:47:16.655395 22833 solver.cpp:415]     Test net output #0: error_blob = 0.661159 (* 1 = 0.661159 loss)
I1006 15:47:16.708240 22833 solver.cpp:243] Iteration 700, loss = 0.664701
I1006 15:47:16.708274 22833 solver.cpp:259]     Train net output #0: error_blob = 0.664701 (* 1 = 0.664701 loss)
I1006 15:47:16.708282 22833 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:47:21.647783 22833 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:47:21.649209 22833 solver.cpp:415]     Test net output #0: error_blob = 0.669908 (* 1 = 0.669908 loss)
I1006 15:47:21.699630 22833 solver.cpp:243] Iteration 800, loss = 0.666607
I1006 15:47:21.699661 22833 solver.cpp:259]     Train net output #0: error_blob = 0.666607 (* 1 = 0.666607 loss)
I1006 15:47:21.699667 22833 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:47:26.671088 22833 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:47:26.672550 22833 solver.cpp:415]     Test net output #0: error_blob = 0.671937 (* 1 = 0.671937 loss)
I1006 15:47:26.724242 22833 solver.cpp:243] Iteration 900, loss = 0.665583
I1006 15:47:26.724287 22833 solver.cpp:259]     Train net output #0: error_blob = 0.665583 (* 1 = 0.665583 loss)
I1006 15:47:26.724292 22833 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:47:31.746428 22833 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:47:31.747735 22833 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:47:31.797009 22833 solver.cpp:327] Iteration 1000, loss = 0.667069
I1006 15:47:31.797039 22833 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:47:31.797547 22833 solver.cpp:415]     Test net output #0: error_blob = 0.662565 (* 1 = 0.662565 loss)
I1006 15:47:31.797559 22833 solver.cpp:332] Optimization Done.
I1006 15:47:31.797562 22833 caffe.cpp:215] Optimization Done.
I1006 15:47:31.922646 22852 caffe.cpp:184] Using GPUs 0
I1006 15:47:32.484653 22852 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part8.prototxt"
I1006 15:47:32.484684 22852 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part8.prototxt
I1006 15:47:32.484853 22852 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:47:32.484900 22852 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part8.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part8.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:47:32.484979 22852 layer_factory.hpp:76] Creating layer data_layer
I1006 15:47:32.511437 22852 net.cpp:110] Creating Layer data_layer
I1006 15:47:32.511474 22852 net.cpp:433] data_layer -> data_blob
I1006 15:47:32.511498 22852 net.cpp:433] data_layer -> label_blob
I1006 15:47:32.512096 22856 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part8.train
I1006 15:47:33.199125 22852 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:47:33.209671 22852 net.cpp:155] Setting up data_layer
I1006 15:47:33.209713 22852 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:47:33.209717 22852 net.cpp:163] Top shape: 40000 (40000)
I1006 15:47:33.209723 22852 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:47:33.209734 22852 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:47:33.209740 22852 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:47:33.209759 22852 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:47:33.210139 22852 net.cpp:155] Setting up hidden_sum_layer
I1006 15:47:33.210147 22852 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:47:33.210168 22852 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:47:33.210186 22852 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:47:33.210188 22852 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:47:33.210191 22852 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:47:36.420810 22852 net.cpp:155] Setting up hidden_act_layer
I1006 15:47:36.420846 22852 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:47:36.420851 22852 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:47:36.420864 22852 net.cpp:110] Creating Layer output_sum_layer
I1006 15:47:36.420868 22852 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:47:36.420876 22852 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:47:36.420985 22852 net.cpp:155] Setting up output_sum_layer
I1006 15:47:36.420990 22852 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:47:36.421008 22852 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:47:36.421013 22852 net.cpp:110] Creating Layer output_act_layer
I1006 15:47:36.421015 22852 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:47:36.421018 22852 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:47:36.421082 22852 net.cpp:155] Setting up output_act_layer
I1006 15:47:36.421102 22852 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:47:36.421114 22852 layer_factory.hpp:76] Creating layer error_layer
I1006 15:47:36.421120 22852 net.cpp:110] Creating Layer error_layer
I1006 15:47:36.421123 22852 net.cpp:477] error_layer <- output_act_blob
I1006 15:47:36.421125 22852 net.cpp:477] error_layer <- label_blob
I1006 15:47:36.421128 22852 net.cpp:433] error_layer -> error_blob
I1006 15:47:36.421152 22852 net.cpp:155] Setting up error_layer
I1006 15:47:36.421164 22852 net.cpp:163] Top shape: (1)
I1006 15:47:36.421166 22852 net.cpp:168]     with loss weight 1
I1006 15:47:36.421191 22852 net.cpp:236] error_layer needs backward computation.
I1006 15:47:36.421193 22852 net.cpp:236] output_act_layer needs backward computation.
I1006 15:47:36.421195 22852 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:47:36.421197 22852 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:47:36.421200 22852 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:47:36.421201 22852 net.cpp:240] data_layer does not need backward computation.
I1006 15:47:36.421203 22852 net.cpp:283] This network produces output error_blob
I1006 15:47:36.421207 22852 net.cpp:297] Network initialization done.
I1006 15:47:36.421210 22852 net.cpp:298] Memory required for data: 13440004
I1006 15:47:36.421349 22852 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part8.prototxt
I1006 15:47:36.421373 22852 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:47:36.421423 22852 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part8.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part8.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:47:36.421454 22852 layer_factory.hpp:76] Creating layer data_layer
I1006 15:47:36.423720 22852 net.cpp:110] Creating Layer data_layer
I1006 15:47:36.423725 22852 net.cpp:433] data_layer -> data_blob
I1006 15:47:36.423729 22852 net.cpp:433] data_layer -> label_blob
I1006 15:47:36.424335 22858 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part8.test
I1006 15:47:36.424404 22852 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:47:36.426256 22852 net.cpp:155] Setting up data_layer
I1006 15:47:36.426277 22852 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:47:36.426280 22852 net.cpp:163] Top shape: 4000 (4000)
I1006 15:47:36.426283 22852 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:47:36.426301 22852 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:47:36.426302 22852 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:47:36.426306 22852 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:47:36.426482 22852 net.cpp:155] Setting up hidden_sum_layer
I1006 15:47:36.426487 22852 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:47:36.426493 22852 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:47:36.426508 22852 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:47:36.426522 22852 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:47:36.426525 22852 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:47:36.426697 22852 net.cpp:155] Setting up hidden_act_layer
I1006 15:47:36.426703 22852 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:47:36.426705 22852 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:47:36.426709 22852 net.cpp:110] Creating Layer output_sum_layer
I1006 15:47:36.426723 22852 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:47:36.426725 22852 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:47:36.426784 22852 net.cpp:155] Setting up output_sum_layer
I1006 15:47:36.426787 22852 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:47:36.426792 22852 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:47:36.426796 22852 net.cpp:110] Creating Layer output_act_layer
I1006 15:47:36.426798 22852 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:47:36.426800 22852 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:47:36.426848 22852 net.cpp:155] Setting up output_act_layer
I1006 15:47:36.426852 22852 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:47:36.426854 22852 layer_factory.hpp:76] Creating layer error_layer
I1006 15:47:36.426858 22852 net.cpp:110] Creating Layer error_layer
I1006 15:47:36.426859 22852 net.cpp:477] error_layer <- output_act_blob
I1006 15:47:36.426862 22852 net.cpp:477] error_layer <- label_blob
I1006 15:47:36.426864 22852 net.cpp:433] error_layer -> error_blob
I1006 15:47:36.426884 22852 net.cpp:155] Setting up error_layer
I1006 15:47:36.426889 22852 net.cpp:163] Top shape: (1)
I1006 15:47:36.426890 22852 net.cpp:168]     with loss weight 1
I1006 15:47:36.426898 22852 net.cpp:236] error_layer needs backward computation.
I1006 15:47:36.426899 22852 net.cpp:236] output_act_layer needs backward computation.
I1006 15:47:36.426901 22852 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:47:36.426903 22852 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:47:36.426905 22852 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:47:36.426908 22852 net.cpp:240] data_layer does not need backward computation.
I1006 15:47:36.426909 22852 net.cpp:283] This network produces output error_blob
I1006 15:47:36.426913 22852 net.cpp:297] Network initialization done.
I1006 15:47:36.426915 22852 net.cpp:298] Memory required for data: 1344004
I1006 15:47:36.426933 22852 solver.cpp:66] Solver scaffolding done.
I1006 15:47:36.427021 22852 caffe.cpp:212] Starting Optimization
I1006 15:47:36.427026 22852 solver.cpp:294] Solving full_batch_sce/model2_part8.prototxt
I1006 15:47:36.427027 22852 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:47:36.427189 22852 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:47:36.427325 22852 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:47:36.432188 22852 solver.cpp:415]     Test net output #0: error_blob = 0.722163 (* 1 = 0.722163 loss)
I1006 15:47:36.435891 22852 solver.cpp:243] Iteration 0, loss = 0.716857
I1006 15:47:36.435920 22852 solver.cpp:259]     Train net output #0: error_blob = 0.716857 (* 1 = 0.716857 loss)
I1006 15:47:36.435927 22852 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:47:41.350571 22852 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:47:41.352008 22852 solver.cpp:415]     Test net output #0: error_blob = 0.699956 (* 1 = 0.699956 loss)
I1006 15:47:41.400988 22852 solver.cpp:243] Iteration 100, loss = 0.694885
I1006 15:47:41.401016 22852 solver.cpp:259]     Train net output #0: error_blob = 0.694885 (* 1 = 0.694885 loss)
I1006 15:47:41.401022 22852 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:47:46.459669 22852 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:47:46.461155 22852 solver.cpp:415]     Test net output #0: error_blob = 0.691711 (* 1 = 0.691711 loss)
I1006 15:47:46.513473 22852 solver.cpp:243] Iteration 200, loss = 0.686369
I1006 15:47:46.513499 22852 solver.cpp:259]     Train net output #0: error_blob = 0.686369 (* 1 = 0.686369 loss)
I1006 15:47:46.513528 22852 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:47:51.561907 22852 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:47:51.563391 22852 solver.cpp:415]     Test net output #0: error_blob = 0.690057 (* 1 = 0.690057 loss)
I1006 15:47:51.615607 22852 solver.cpp:243] Iteration 300, loss = 0.679236
I1006 15:47:51.615638 22852 solver.cpp:259]     Train net output #0: error_blob = 0.679236 (* 1 = 0.679236 loss)
I1006 15:47:51.615643 22852 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:47:56.645948 22852 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:47:56.647375 22852 solver.cpp:415]     Test net output #0: error_blob = 0.687617 (* 1 = 0.687617 loss)
I1006 15:47:56.697751 22852 solver.cpp:243] Iteration 400, loss = 0.677064
I1006 15:47:56.697779 22852 solver.cpp:259]     Train net output #0: error_blob = 0.677064 (* 1 = 0.677064 loss)
I1006 15:47:56.697784 22852 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:48:01.689641 22852 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:48:01.691149 22852 solver.cpp:415]     Test net output #0: error_blob = 0.684466 (* 1 = 0.684466 loss)
I1006 15:48:01.742398 22852 solver.cpp:243] Iteration 500, loss = 0.673951
I1006 15:48:01.742434 22852 solver.cpp:259]     Train net output #0: error_blob = 0.673951 (* 1 = 0.673951 loss)
I1006 15:48:01.742439 22852 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:48:06.877440 22852 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:48:06.878938 22852 solver.cpp:415]     Test net output #0: error_blob = 0.681944 (* 1 = 0.681944 loss)
I1006 15:48:06.930946 22852 solver.cpp:243] Iteration 600, loss = 0.671029
I1006 15:48:06.930984 22852 solver.cpp:259]     Train net output #0: error_blob = 0.671029 (* 1 = 0.671029 loss)
I1006 15:48:06.930990 22852 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:48:11.995648 22852 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:48:11.997135 22852 solver.cpp:415]     Test net output #0: error_blob = 0.680946 (* 1 = 0.680946 loss)
I1006 15:48:12.049181 22852 solver.cpp:243] Iteration 700, loss = 0.669626
I1006 15:48:12.049216 22852 solver.cpp:259]     Train net output #0: error_blob = 0.669626 (* 1 = 0.669626 loss)
I1006 15:48:12.049221 22852 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:48:17.032074 22852 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:48:17.033515 22852 solver.cpp:415]     Test net output #0: error_blob = 0.680857 (* 1 = 0.680857 loss)
I1006 15:48:17.087548 22852 solver.cpp:243] Iteration 800, loss = 0.666223
I1006 15:48:17.087587 22852 solver.cpp:259]     Train net output #0: error_blob = 0.666223 (* 1 = 0.666223 loss)
I1006 15:48:17.087592 22852 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:48:22.229404 22852 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:48:22.230900 22852 solver.cpp:415]     Test net output #0: error_blob = 0.679826 (* 1 = 0.679826 loss)
I1006 15:48:22.284641 22852 solver.cpp:243] Iteration 900, loss = 0.666278
I1006 15:48:22.284672 22852 solver.cpp:259]     Train net output #0: error_blob = 0.666278 (* 1 = 0.666278 loss)
I1006 15:48:22.284677 22852 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:48:27.382815 22852 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:48:27.384098 22852 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:48:27.433019 22852 solver.cpp:327] Iteration 1000, loss = 0.664737
I1006 15:48:27.433056 22852 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:48:27.433568 22852 solver.cpp:415]     Test net output #0: error_blob = 0.677992 (* 1 = 0.677992 loss)
I1006 15:48:27.433591 22852 solver.cpp:332] Optimization Done.
I1006 15:48:27.433595 22852 caffe.cpp:215] Optimization Done.
I1006 15:48:27.581110 22871 caffe.cpp:184] Using GPUs 0
I1006 15:48:28.143440 22871 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part4.prototxt"
I1006 15:48:28.143471 22871 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part4.prototxt
I1006 15:48:28.143640 22871 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:48:28.143683 22871 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part4.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part4.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:48:28.143741 22871 layer_factory.hpp:76] Creating layer data_layer
I1006 15:48:28.169874 22871 net.cpp:110] Creating Layer data_layer
I1006 15:48:28.169898 22871 net.cpp:433] data_layer -> data_blob
I1006 15:48:28.169931 22871 net.cpp:433] data_layer -> label_blob
I1006 15:48:28.170519 22875 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part4.train
I1006 15:48:28.855237 22871 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:48:28.865767 22871 net.cpp:155] Setting up data_layer
I1006 15:48:28.865810 22871 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:48:28.865814 22871 net.cpp:163] Top shape: 40000 (40000)
I1006 15:48:28.865830 22871 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:48:28.865842 22871 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:48:28.865846 22871 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:48:28.865855 22871 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:48:28.866235 22871 net.cpp:155] Setting up hidden_sum_layer
I1006 15:48:28.866242 22871 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:48:28.866263 22871 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:48:28.866281 22871 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:48:28.866283 22871 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:48:28.866286 22871 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:48:32.103581 22871 net.cpp:155] Setting up hidden_act_layer
I1006 15:48:32.103618 22871 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:48:32.103626 22871 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:48:32.103638 22871 net.cpp:110] Creating Layer output_sum_layer
I1006 15:48:32.103652 22871 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:48:32.103659 22871 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:48:32.103786 22871 net.cpp:155] Setting up output_sum_layer
I1006 15:48:32.103791 22871 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:48:32.103809 22871 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:48:32.103814 22871 net.cpp:110] Creating Layer output_act_layer
I1006 15:48:32.103816 22871 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:48:32.103829 22871 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:48:32.103904 22871 net.cpp:155] Setting up output_act_layer
I1006 15:48:32.103920 22871 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:48:32.103934 22871 layer_factory.hpp:76] Creating layer error_layer
I1006 15:48:32.103938 22871 net.cpp:110] Creating Layer error_layer
I1006 15:48:32.103940 22871 net.cpp:477] error_layer <- output_act_blob
I1006 15:48:32.103943 22871 net.cpp:477] error_layer <- label_blob
I1006 15:48:32.103957 22871 net.cpp:433] error_layer -> error_blob
I1006 15:48:32.103981 22871 net.cpp:155] Setting up error_layer
I1006 15:48:32.103986 22871 net.cpp:163] Top shape: (1)
I1006 15:48:32.103987 22871 net.cpp:168]     with loss weight 1
I1006 15:48:32.104023 22871 net.cpp:236] error_layer needs backward computation.
I1006 15:48:32.104025 22871 net.cpp:236] output_act_layer needs backward computation.
I1006 15:48:32.104037 22871 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:48:32.104038 22871 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:48:32.104040 22871 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:48:32.104043 22871 net.cpp:240] data_layer does not need backward computation.
I1006 15:48:32.104044 22871 net.cpp:283] This network produces output error_blob
I1006 15:48:32.104060 22871 net.cpp:297] Network initialization done.
I1006 15:48:32.104061 22871 net.cpp:298] Memory required for data: 13440004
I1006 15:48:32.104223 22871 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part4.prototxt
I1006 15:48:32.104244 22871 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:48:32.104295 22871 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part4.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part4.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:48:32.104336 22871 layer_factory.hpp:76] Creating layer data_layer
I1006 15:48:32.106649 22871 net.cpp:110] Creating Layer data_layer
I1006 15:48:32.106665 22871 net.cpp:433] data_layer -> data_blob
I1006 15:48:32.106670 22871 net.cpp:433] data_layer -> label_blob
I1006 15:48:32.107249 22877 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part4.test
I1006 15:48:32.107319 22871 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:48:32.109186 22871 net.cpp:155] Setting up data_layer
I1006 15:48:32.109207 22871 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:48:32.109211 22871 net.cpp:163] Top shape: 4000 (4000)
I1006 15:48:32.109215 22871 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:48:32.109222 22871 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:48:32.109225 22871 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:48:32.109228 22871 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:48:32.109344 22871 net.cpp:155] Setting up hidden_sum_layer
I1006 15:48:32.109349 22871 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:48:32.109356 22871 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:48:32.109361 22871 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:48:32.109374 22871 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:48:32.109377 22871 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:48:32.109583 22871 net.cpp:155] Setting up hidden_act_layer
I1006 15:48:32.109591 22871 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:48:32.109592 22871 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:48:32.109596 22871 net.cpp:110] Creating Layer output_sum_layer
I1006 15:48:32.109598 22871 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:48:32.109602 22871 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:48:32.109660 22871 net.cpp:155] Setting up output_sum_layer
I1006 15:48:32.109665 22871 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:48:32.109670 22871 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:48:32.109674 22871 net.cpp:110] Creating Layer output_act_layer
I1006 15:48:32.109676 22871 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:48:32.109679 22871 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:48:32.109726 22871 net.cpp:155] Setting up output_act_layer
I1006 15:48:32.109730 22871 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:48:32.109732 22871 layer_factory.hpp:76] Creating layer error_layer
I1006 15:48:32.109737 22871 net.cpp:110] Creating Layer error_layer
I1006 15:48:32.109738 22871 net.cpp:477] error_layer <- output_act_blob
I1006 15:48:32.109741 22871 net.cpp:477] error_layer <- label_blob
I1006 15:48:32.109745 22871 net.cpp:433] error_layer -> error_blob
I1006 15:48:32.109763 22871 net.cpp:155] Setting up error_layer
I1006 15:48:32.109767 22871 net.cpp:163] Top shape: (1)
I1006 15:48:32.109768 22871 net.cpp:168]     with loss weight 1
I1006 15:48:32.109776 22871 net.cpp:236] error_layer needs backward computation.
I1006 15:48:32.109778 22871 net.cpp:236] output_act_layer needs backward computation.
I1006 15:48:32.109781 22871 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:48:32.109782 22871 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:48:32.109784 22871 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:48:32.109786 22871 net.cpp:240] data_layer does not need backward computation.
I1006 15:48:32.109787 22871 net.cpp:283] This network produces output error_blob
I1006 15:48:32.109792 22871 net.cpp:297] Network initialization done.
I1006 15:48:32.109793 22871 net.cpp:298] Memory required for data: 1344004
I1006 15:48:32.109812 22871 solver.cpp:66] Solver scaffolding done.
I1006 15:48:32.109901 22871 caffe.cpp:212] Starting Optimization
I1006 15:48:32.109907 22871 solver.cpp:294] Solving full_batch_sce/model2_part4.prototxt
I1006 15:48:32.109910 22871 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:48:32.110074 22871 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:48:32.110213 22871 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:48:32.115232 22871 solver.cpp:415]     Test net output #0: error_blob = 0.717813 (* 1 = 0.717813 loss)
I1006 15:48:32.119156 22871 solver.cpp:243] Iteration 0, loss = 0.738824
I1006 15:48:32.119179 22871 solver.cpp:259]     Train net output #0: error_blob = 0.738824 (* 1 = 0.738824 loss)
I1006 15:48:32.119187 22871 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:48:36.909814 22871 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:48:36.911317 22871 solver.cpp:415]     Test net output #0: error_blob = 0.691849 (* 1 = 0.691849 loss)
I1006 15:48:36.961352 22871 solver.cpp:243] Iteration 100, loss = 0.706608
I1006 15:48:36.961380 22871 solver.cpp:259]     Train net output #0: error_blob = 0.706608 (* 1 = 0.706608 loss)
I1006 15:48:36.961385 22871 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:48:41.861395 22871 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:48:41.862871 22871 solver.cpp:415]     Test net output #0: error_blob = 0.682203 (* 1 = 0.682203 loss)
I1006 15:48:41.915805 22871 solver.cpp:243] Iteration 200, loss = 0.691732
I1006 15:48:41.915834 22871 solver.cpp:259]     Train net output #0: error_blob = 0.691732 (* 1 = 0.691732 loss)
I1006 15:48:41.915863 22871 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:48:46.792233 22871 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:48:46.793710 22871 solver.cpp:415]     Test net output #0: error_blob = 0.677117 (* 1 = 0.677117 loss)
I1006 15:48:46.846042 22871 solver.cpp:243] Iteration 300, loss = 0.686055
I1006 15:48:46.846078 22871 solver.cpp:259]     Train net output #0: error_blob = 0.686055 (* 1 = 0.686055 loss)
I1006 15:48:46.846083 22871 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:48:51.747441 22871 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:48:51.748865 22871 solver.cpp:415]     Test net output #0: error_blob = 0.6732 (* 1 = 0.6732 loss)
I1006 15:48:51.801575 22871 solver.cpp:243] Iteration 400, loss = 0.682154
I1006 15:48:51.801611 22871 solver.cpp:259]     Train net output #0: error_blob = 0.682154 (* 1 = 0.682154 loss)
I1006 15:48:51.801616 22871 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:48:56.627668 22871 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:48:56.629163 22871 solver.cpp:415]     Test net output #0: error_blob = 0.669716 (* 1 = 0.669716 loss)
I1006 15:48:56.680308 22871 solver.cpp:243] Iteration 500, loss = 0.679803
I1006 15:48:56.680336 22871 solver.cpp:259]     Train net output #0: error_blob = 0.679803 (* 1 = 0.679803 loss)
I1006 15:48:56.680341 22871 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:49:01.564534 22871 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:49:01.566025 22871 solver.cpp:415]     Test net output #0: error_blob = 0.666609 (* 1 = 0.666609 loss)
I1006 15:49:01.617944 22871 solver.cpp:243] Iteration 600, loss = 0.676755
I1006 15:49:01.617982 22871 solver.cpp:259]     Train net output #0: error_blob = 0.676755 (* 1 = 0.676755 loss)
I1006 15:49:01.617988 22871 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:49:06.487671 22871 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:49:06.489099 22871 solver.cpp:415]     Test net output #0: error_blob = 0.663962 (* 1 = 0.663962 loss)
I1006 15:49:06.537981 22871 solver.cpp:243] Iteration 700, loss = 0.673614
I1006 15:49:06.538008 22871 solver.cpp:259]     Train net output #0: error_blob = 0.673614 (* 1 = 0.673614 loss)
I1006 15:49:06.538013 22871 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:49:11.407582 22871 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:49:11.408992 22871 solver.cpp:415]     Test net output #0: error_blob = 0.661637 (* 1 = 0.661637 loss)
I1006 15:49:11.461309 22871 solver.cpp:243] Iteration 800, loss = 0.671529
I1006 15:49:11.461339 22871 solver.cpp:259]     Train net output #0: error_blob = 0.671529 (* 1 = 0.671529 loss)
I1006 15:49:11.461344 22871 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:49:16.331964 22871 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:49:16.333487 22871 solver.cpp:415]     Test net output #0: error_blob = 0.659709 (* 1 = 0.659709 loss)
I1006 15:49:16.384299 22871 solver.cpp:243] Iteration 900, loss = 0.671173
I1006 15:49:16.384327 22871 solver.cpp:259]     Train net output #0: error_blob = 0.671173 (* 1 = 0.671173 loss)
I1006 15:49:16.384331 22871 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:49:21.243300 22871 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:49:21.245800 22871 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:49:21.291903 22871 solver.cpp:327] Iteration 1000, loss = 0.668798
I1006 15:49:21.291939 22871 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:49:21.292428 22871 solver.cpp:415]     Test net output #0: error_blob = 0.657934 (* 1 = 0.657934 loss)
I1006 15:49:21.292436 22871 solver.cpp:332] Optimization Done.
I1006 15:49:21.292450 22871 caffe.cpp:215] Optimization Done.
I1006 15:49:21.420604 22890 caffe.cpp:184] Using GPUs 0
I1006 15:49:21.980731 22890 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part7.prototxt"
I1006 15:49:21.980762 22890 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part7.prototxt
I1006 15:49:21.980906 22890 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:49:21.980942 22890 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part7.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part7.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:49:21.980978 22890 layer_factory.hpp:76] Creating layer data_layer
I1006 15:49:22.007333 22890 net.cpp:110] Creating Layer data_layer
I1006 15:49:22.007364 22890 net.cpp:433] data_layer -> data_blob
I1006 15:49:22.007385 22890 net.cpp:433] data_layer -> label_blob
I1006 15:49:22.007954 22894 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part7.train
I1006 15:49:22.690412 22890 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:49:22.700845 22890 net.cpp:155] Setting up data_layer
I1006 15:49:22.700886 22890 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:49:22.700891 22890 net.cpp:163] Top shape: 40000 (40000)
I1006 15:49:22.700896 22890 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:49:22.700907 22890 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:49:22.700916 22890 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:49:22.700924 22890 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:49:22.701298 22890 net.cpp:155] Setting up hidden_sum_layer
I1006 15:49:22.701305 22890 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:49:22.701328 22890 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:49:22.701333 22890 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:49:22.701336 22890 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:49:22.701339 22890 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:49:25.913105 22890 net.cpp:155] Setting up hidden_act_layer
I1006 15:49:25.913137 22890 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:49:25.913142 22890 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:49:25.913151 22890 net.cpp:110] Creating Layer output_sum_layer
I1006 15:49:25.913154 22890 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:49:25.913161 22890 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:49:25.913257 22890 net.cpp:155] Setting up output_sum_layer
I1006 15:49:25.913264 22890 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:49:25.913280 22890 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:49:25.913286 22890 net.cpp:110] Creating Layer output_act_layer
I1006 15:49:25.913288 22890 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:49:25.913291 22890 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:49:25.913355 22890 net.cpp:155] Setting up output_act_layer
I1006 15:49:25.913373 22890 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:49:25.913386 22890 layer_factory.hpp:76] Creating layer error_layer
I1006 15:49:25.913391 22890 net.cpp:110] Creating Layer error_layer
I1006 15:49:25.913393 22890 net.cpp:477] error_layer <- output_act_blob
I1006 15:49:25.913396 22890 net.cpp:477] error_layer <- label_blob
I1006 15:49:25.913400 22890 net.cpp:433] error_layer -> error_blob
I1006 15:49:25.913424 22890 net.cpp:155] Setting up error_layer
I1006 15:49:25.913437 22890 net.cpp:163] Top shape: (1)
I1006 15:49:25.913439 22890 net.cpp:168]     with loss weight 1
I1006 15:49:25.913465 22890 net.cpp:236] error_layer needs backward computation.
I1006 15:49:25.913467 22890 net.cpp:236] output_act_layer needs backward computation.
I1006 15:49:25.913470 22890 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:49:25.913472 22890 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:49:25.913475 22890 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:49:25.913476 22890 net.cpp:240] data_layer does not need backward computation.
I1006 15:49:25.913478 22890 net.cpp:283] This network produces output error_blob
I1006 15:49:25.913482 22890 net.cpp:297] Network initialization done.
I1006 15:49:25.913485 22890 net.cpp:298] Memory required for data: 13440004
I1006 15:49:25.913626 22890 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part7.prototxt
I1006 15:49:25.913642 22890 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:49:25.913682 22890 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part7.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part7.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:49:25.913702 22890 layer_factory.hpp:76] Creating layer data_layer
I1006 15:49:25.916043 22890 net.cpp:110] Creating Layer data_layer
I1006 15:49:25.916060 22890 net.cpp:433] data_layer -> data_blob
I1006 15:49:25.916065 22890 net.cpp:433] data_layer -> label_blob
I1006 15:49:25.916652 22896 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part7.test
I1006 15:49:25.916733 22890 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:49:25.918684 22890 net.cpp:155] Setting up data_layer
I1006 15:49:25.918696 22890 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:49:25.918700 22890 net.cpp:163] Top shape: 4000 (4000)
I1006 15:49:25.918704 22890 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:49:25.918714 22890 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:49:25.918715 22890 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:49:25.918720 22890 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:49:25.918861 22890 net.cpp:155] Setting up hidden_sum_layer
I1006 15:49:25.918866 22890 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:49:25.918874 22890 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:49:25.918879 22890 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:49:25.918895 22890 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:49:25.918898 22890 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:49:25.919078 22890 net.cpp:155] Setting up hidden_act_layer
I1006 15:49:25.919087 22890 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:49:25.919090 22890 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:49:25.919095 22890 net.cpp:110] Creating Layer output_sum_layer
I1006 15:49:25.919097 22890 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:49:25.919101 22890 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:49:25.919164 22890 net.cpp:155] Setting up output_sum_layer
I1006 15:49:25.919169 22890 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:49:25.919175 22890 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:49:25.919179 22890 net.cpp:110] Creating Layer output_act_layer
I1006 15:49:25.919181 22890 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:49:25.919184 22890 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:49:25.919236 22890 net.cpp:155] Setting up output_act_layer
I1006 15:49:25.919240 22890 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:49:25.919244 22890 layer_factory.hpp:76] Creating layer error_layer
I1006 15:49:25.919248 22890 net.cpp:110] Creating Layer error_layer
I1006 15:49:25.919250 22890 net.cpp:477] error_layer <- output_act_blob
I1006 15:49:25.919252 22890 net.cpp:477] error_layer <- label_blob
I1006 15:49:25.919255 22890 net.cpp:433] error_layer -> error_blob
I1006 15:49:25.919277 22890 net.cpp:155] Setting up error_layer
I1006 15:49:25.919281 22890 net.cpp:163] Top shape: (1)
I1006 15:49:25.919283 22890 net.cpp:168]     with loss weight 1
I1006 15:49:25.919291 22890 net.cpp:236] error_layer needs backward computation.
I1006 15:49:25.919293 22890 net.cpp:236] output_act_layer needs backward computation.
I1006 15:49:25.919296 22890 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:49:25.919298 22890 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:49:25.919301 22890 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:49:25.919302 22890 net.cpp:240] data_layer does not need backward computation.
I1006 15:49:25.919306 22890 net.cpp:283] This network produces output error_blob
I1006 15:49:25.919309 22890 net.cpp:297] Network initialization done.
I1006 15:49:25.919312 22890 net.cpp:298] Memory required for data: 1344004
I1006 15:49:25.919333 22890 solver.cpp:66] Solver scaffolding done.
I1006 15:49:25.919430 22890 caffe.cpp:212] Starting Optimization
I1006 15:49:25.919436 22890 solver.cpp:294] Solving full_batch_sce/model2_part7.prototxt
I1006 15:49:25.919437 22890 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:49:25.919584 22890 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:49:25.919642 22890 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:49:25.930330 22890 solver.cpp:415]     Test net output #0: error_blob = 0.729663 (* 1 = 0.729663 loss)
I1006 15:49:25.934095 22890 solver.cpp:243] Iteration 0, loss = 0.735013
I1006 15:49:25.934118 22890 solver.cpp:259]     Train net output #0: error_blob = 0.735013 (* 1 = 0.735013 loss)
I1006 15:49:25.934126 22890 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:49:30.982607 22890 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:49:30.984069 22890 solver.cpp:415]     Test net output #0: error_blob = 0.694655 (* 1 = 0.694655 loss)
I1006 15:49:31.037542 22890 solver.cpp:243] Iteration 100, loss = 0.697649
I1006 15:49:31.037570 22890 solver.cpp:259]     Train net output #0: error_blob = 0.697649 (* 1 = 0.697649 loss)
I1006 15:49:31.037575 22890 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:49:36.311641 22890 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:49:36.313120 22890 solver.cpp:415]     Test net output #0: error_blob = 0.681904 (* 1 = 0.681904 loss)
I1006 15:49:36.363867 22890 solver.cpp:243] Iteration 200, loss = 0.682345
I1006 15:49:36.363904 22890 solver.cpp:259]     Train net output #0: error_blob = 0.682345 (* 1 = 0.682345 loss)
I1006 15:49:36.363936 22890 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:49:41.444227 22890 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:49:41.445710 22890 solver.cpp:415]     Test net output #0: error_blob = 0.676641 (* 1 = 0.676641 loss)
I1006 15:49:41.497789 22890 solver.cpp:243] Iteration 300, loss = 0.676484
I1006 15:49:41.497828 22890 solver.cpp:259]     Train net output #0: error_blob = 0.676484 (* 1 = 0.676484 loss)
I1006 15:49:41.497835 22890 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:49:46.578500 22890 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:49:46.579931 22890 solver.cpp:415]     Test net output #0: error_blob = 0.673548 (* 1 = 0.673548 loss)
I1006 15:49:46.633198 22890 solver.cpp:243] Iteration 400, loss = 0.672372
I1006 15:49:46.633236 22890 solver.cpp:259]     Train net output #0: error_blob = 0.672372 (* 1 = 0.672372 loss)
I1006 15:49:46.633241 22890 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:49:51.688081 22890 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:49:51.689508 22890 solver.cpp:415]     Test net output #0: error_blob = 0.671387 (* 1 = 0.671387 loss)
I1006 15:49:51.741858 22890 solver.cpp:243] Iteration 500, loss = 0.670186
I1006 15:49:51.741899 22890 solver.cpp:259]     Train net output #0: error_blob = 0.670186 (* 1 = 0.670186 loss)
I1006 15:49:51.741904 22890 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:49:56.890427 22890 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:49:56.891870 22890 solver.cpp:415]     Test net output #0: error_blob = 0.670385 (* 1 = 0.670385 loss)
I1006 15:49:56.946738 22890 solver.cpp:243] Iteration 600, loss = 0.668368
I1006 15:49:56.946779 22890 solver.cpp:259]     Train net output #0: error_blob = 0.668368 (* 1 = 0.668368 loss)
I1006 15:49:56.946784 22890 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:50:02.011811 22890 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:50:02.013239 22890 solver.cpp:415]     Test net output #0: error_blob = 0.670076 (* 1 = 0.670076 loss)
I1006 15:50:02.066974 22890 solver.cpp:243] Iteration 700, loss = 0.665918
I1006 15:50:02.067006 22890 solver.cpp:259]     Train net output #0: error_blob = 0.665918 (* 1 = 0.665918 loss)
I1006 15:50:02.067013 22890 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:50:07.211400 22890 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:50:07.212826 22890 solver.cpp:415]     Test net output #0: error_blob = 0.669202 (* 1 = 0.669202 loss)
I1006 15:50:07.269498 22890 solver.cpp:243] Iteration 800, loss = 0.66625
I1006 15:50:07.269547 22890 solver.cpp:259]     Train net output #0: error_blob = 0.66625 (* 1 = 0.66625 loss)
I1006 15:50:07.269557 22890 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:50:12.358240 22890 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:50:12.359657 22890 solver.cpp:415]     Test net output #0: error_blob = 0.668561 (* 1 = 0.668561 loss)
I1006 15:50:12.414795 22890 solver.cpp:243] Iteration 900, loss = 0.663019
I1006 15:50:12.414836 22890 solver.cpp:259]     Train net output #0: error_blob = 0.663019 (* 1 = 0.663019 loss)
I1006 15:50:12.414844 22890 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:50:17.478569 22890 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:50:17.479894 22890 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:50:17.532534 22890 solver.cpp:327] Iteration 1000, loss = 0.664428
I1006 15:50:17.532574 22890 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:50:17.533099 22890 solver.cpp:415]     Test net output #0: error_blob = 0.667841 (* 1 = 0.667841 loss)
I1006 15:50:17.533109 22890 solver.cpp:332] Optimization Done.
I1006 15:50:17.533123 22890 caffe.cpp:215] Optimization Done.
I1006 15:50:17.667350 22909 caffe.cpp:184] Using GPUs 0
I1006 15:50:18.230413 22909 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part3.prototxt"
I1006 15:50:18.230444 22909 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part3.prototxt
I1006 15:50:18.230592 22909 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:50:18.230629 22909 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part3.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part3.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:50:18.230665 22909 layer_factory.hpp:76] Creating layer data_layer
I1006 15:50:18.257022 22909 net.cpp:110] Creating Layer data_layer
I1006 15:50:18.257048 22909 net.cpp:433] data_layer -> data_blob
I1006 15:50:18.257069 22909 net.cpp:433] data_layer -> label_blob
I1006 15:50:18.257652 22915 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part3.train
I1006 15:50:18.943344 22909 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:50:18.953748 22909 net.cpp:155] Setting up data_layer
I1006 15:50:18.953788 22909 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:50:18.953791 22909 net.cpp:163] Top shape: 40000 (40000)
I1006 15:50:18.953799 22909 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:50:18.953809 22909 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:50:18.953812 22909 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:50:18.953820 22909 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:50:18.954186 22909 net.cpp:155] Setting up hidden_sum_layer
I1006 15:50:18.954193 22909 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:50:18.954205 22909 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:50:18.954212 22909 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:50:18.954215 22909 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:50:18.954217 22909 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:50:22.188113 22909 net.cpp:155] Setting up hidden_act_layer
I1006 15:50:22.188136 22909 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:50:22.188140 22909 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:50:22.188150 22909 net.cpp:110] Creating Layer output_sum_layer
I1006 15:50:22.188155 22909 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:50:22.188161 22909 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:50:22.188253 22909 net.cpp:155] Setting up output_sum_layer
I1006 15:50:22.188258 22909 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:50:22.188266 22909 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:50:22.188271 22909 net.cpp:110] Creating Layer output_act_layer
I1006 15:50:22.188274 22909 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:50:22.188277 22909 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:50:22.188338 22909 net.cpp:155] Setting up output_act_layer
I1006 15:50:22.188356 22909 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:50:22.188359 22909 layer_factory.hpp:76] Creating layer error_layer
I1006 15:50:22.188365 22909 net.cpp:110] Creating Layer error_layer
I1006 15:50:22.188367 22909 net.cpp:477] error_layer <- output_act_blob
I1006 15:50:22.188370 22909 net.cpp:477] error_layer <- label_blob
I1006 15:50:22.188374 22909 net.cpp:433] error_layer -> error_blob
I1006 15:50:22.188400 22909 net.cpp:155] Setting up error_layer
I1006 15:50:22.188405 22909 net.cpp:163] Top shape: (1)
I1006 15:50:22.188407 22909 net.cpp:168]     with loss weight 1
I1006 15:50:22.188422 22909 net.cpp:236] error_layer needs backward computation.
I1006 15:50:22.188426 22909 net.cpp:236] output_act_layer needs backward computation.
I1006 15:50:22.188427 22909 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:50:22.188429 22909 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:50:22.188431 22909 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:50:22.188434 22909 net.cpp:240] data_layer does not need backward computation.
I1006 15:50:22.188436 22909 net.cpp:283] This network produces output error_blob
I1006 15:50:22.188441 22909 net.cpp:297] Network initialization done.
I1006 15:50:22.188443 22909 net.cpp:298] Memory required for data: 13440004
I1006 15:50:22.188576 22909 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part3.prototxt
I1006 15:50:22.188591 22909 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:50:22.188623 22909 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part3.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part3.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:50:22.188644 22909 layer_factory.hpp:76] Creating layer data_layer
I1006 15:50:22.191097 22909 net.cpp:110] Creating Layer data_layer
I1006 15:50:22.191104 22909 net.cpp:433] data_layer -> data_blob
I1006 15:50:22.191109 22909 net.cpp:433] data_layer -> label_blob
I1006 15:50:22.191671 22917 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part3.test
I1006 15:50:22.191743 22909 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:50:22.193693 22909 net.cpp:155] Setting up data_layer
I1006 15:50:22.193708 22909 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:50:22.193713 22909 net.cpp:163] Top shape: 4000 (4000)
I1006 15:50:22.193717 22909 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:50:22.193725 22909 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:50:22.193728 22909 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:50:22.193732 22909 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:50:22.193853 22909 net.cpp:155] Setting up hidden_sum_layer
I1006 15:50:22.193858 22909 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:50:22.193866 22909 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:50:22.193871 22909 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:50:22.193886 22909 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:50:22.193891 22909 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:50:22.194074 22909 net.cpp:155] Setting up hidden_act_layer
I1006 15:50:22.194080 22909 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:50:22.194083 22909 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:50:22.194088 22909 net.cpp:110] Creating Layer output_sum_layer
I1006 15:50:22.194090 22909 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:50:22.194094 22909 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:50:22.194154 22909 net.cpp:155] Setting up output_sum_layer
I1006 15:50:22.194159 22909 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:50:22.194164 22909 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:50:22.194167 22909 net.cpp:110] Creating Layer output_act_layer
I1006 15:50:22.194169 22909 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:50:22.194172 22909 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:50:22.194221 22909 net.cpp:155] Setting up output_act_layer
I1006 15:50:22.194224 22909 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:50:22.194227 22909 layer_factory.hpp:76] Creating layer error_layer
I1006 15:50:22.194231 22909 net.cpp:110] Creating Layer error_layer
I1006 15:50:22.194233 22909 net.cpp:477] error_layer <- output_act_blob
I1006 15:50:22.194236 22909 net.cpp:477] error_layer <- label_blob
I1006 15:50:22.194238 22909 net.cpp:433] error_layer -> error_blob
I1006 15:50:22.194258 22909 net.cpp:155] Setting up error_layer
I1006 15:50:22.194262 22909 net.cpp:163] Top shape: (1)
I1006 15:50:22.194264 22909 net.cpp:168]     with loss weight 1
I1006 15:50:22.194272 22909 net.cpp:236] error_layer needs backward computation.
I1006 15:50:22.194274 22909 net.cpp:236] output_act_layer needs backward computation.
I1006 15:50:22.194277 22909 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:50:22.194278 22909 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:50:22.194280 22909 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:50:22.194283 22909 net.cpp:240] data_layer does not need backward computation.
I1006 15:50:22.194284 22909 net.cpp:283] This network produces output error_blob
I1006 15:50:22.194288 22909 net.cpp:297] Network initialization done.
I1006 15:50:22.194290 22909 net.cpp:298] Memory required for data: 1344004
I1006 15:50:22.194310 22909 solver.cpp:66] Solver scaffolding done.
I1006 15:50:22.194401 22909 caffe.cpp:212] Starting Optimization
I1006 15:50:22.194408 22909 solver.cpp:294] Solving full_batch_sce/model2_part3.prototxt
I1006 15:50:22.194409 22909 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:50:22.194548 22909 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:50:22.194623 22909 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:50:22.204355 22909 solver.cpp:415]     Test net output #0: error_blob = 0.720439 (* 1 = 0.720439 loss)
I1006 15:50:22.208101 22909 solver.cpp:243] Iteration 0, loss = 0.727485
I1006 15:50:22.208127 22909 solver.cpp:259]     Train net output #0: error_blob = 0.727485 (* 1 = 0.727485 loss)
I1006 15:50:22.208137 22909 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:50:27.207372 22909 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:50:27.208875 22909 solver.cpp:415]     Test net output #0: error_blob = 0.691442 (* 1 = 0.691442 loss)
I1006 15:50:27.258815 22909 solver.cpp:243] Iteration 100, loss = 0.69941
I1006 15:50:27.258852 22909 solver.cpp:259]     Train net output #0: error_blob = 0.69941 (* 1 = 0.69941 loss)
I1006 15:50:27.258857 22909 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:50:32.213974 22909 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:50:32.215478 22909 solver.cpp:415]     Test net output #0: error_blob = 0.680635 (* 1 = 0.680635 loss)
I1006 15:50:32.264199 22909 solver.cpp:243] Iteration 200, loss = 0.689368
I1006 15:50:32.264237 22909 solver.cpp:259]     Train net output #0: error_blob = 0.689368 (* 1 = 0.689368 loss)
I1006 15:50:32.264266 22909 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:50:37.262717 22909 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:50:37.264125 22909 solver.cpp:415]     Test net output #0: error_blob = 0.674851 (* 1 = 0.674851 loss)
I1006 15:50:37.317698 22909 solver.cpp:243] Iteration 300, loss = 0.685224
I1006 15:50:37.317739 22909 solver.cpp:259]     Train net output #0: error_blob = 0.685224 (* 1 = 0.685224 loss)
I1006 15:50:37.317745 22909 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:50:42.249723 22909 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:50:42.251209 22909 solver.cpp:415]     Test net output #0: error_blob = 0.670342 (* 1 = 0.670342 loss)
I1006 15:50:42.299928 22909 solver.cpp:243] Iteration 400, loss = 0.681241
I1006 15:50:42.299969 22909 solver.cpp:259]     Train net output #0: error_blob = 0.681241 (* 1 = 0.681241 loss)
I1006 15:50:42.299975 22909 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:50:47.269361 22909 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:50:47.270848 22909 solver.cpp:415]     Test net output #0: error_blob = 0.666365 (* 1 = 0.666365 loss)
I1006 15:50:47.320989 22909 solver.cpp:243] Iteration 500, loss = 0.678485
I1006 15:50:47.321027 22909 solver.cpp:259]     Train net output #0: error_blob = 0.678485 (* 1 = 0.678485 loss)
I1006 15:50:47.321033 22909 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:50:52.284626 22909 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:50:52.286056 22909 solver.cpp:415]     Test net output #0: error_blob = 0.66294 (* 1 = 0.66294 loss)
I1006 15:50:52.339627 22909 solver.cpp:243] Iteration 600, loss = 0.677503
I1006 15:50:52.339653 22909 solver.cpp:259]     Train net output #0: error_blob = 0.677503 (* 1 = 0.677503 loss)
I1006 15:50:52.339658 22909 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:50:57.303544 22909 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:50:57.304994 22909 solver.cpp:415]     Test net output #0: error_blob = 0.661699 (* 1 = 0.661699 loss)
I1006 15:50:57.357530 22909 solver.cpp:243] Iteration 700, loss = 0.675568
I1006 15:50:57.357561 22909 solver.cpp:259]     Train net output #0: error_blob = 0.675568 (* 1 = 0.675568 loss)
I1006 15:50:57.357568 22909 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:51:02.339392 22909 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:51:02.340901 22909 solver.cpp:415]     Test net output #0: error_blob = 0.66063 (* 1 = 0.66063 loss)
I1006 15:51:02.391039 22909 solver.cpp:243] Iteration 800, loss = 0.676466
I1006 15:51:02.391065 22909 solver.cpp:259]     Train net output #0: error_blob = 0.676466 (* 1 = 0.676466 loss)
I1006 15:51:02.391072 22909 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:51:07.300746 22909 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:51:07.302253 22909 solver.cpp:415]     Test net output #0: error_blob = 0.657829 (* 1 = 0.657829 loss)
I1006 15:51:07.353617 22909 solver.cpp:243] Iteration 900, loss = 0.673026
I1006 15:51:07.353647 22909 solver.cpp:259]     Train net output #0: error_blob = 0.673026 (* 1 = 0.673026 loss)
I1006 15:51:07.353653 22909 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:51:12.317540 22909 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:51:12.319766 22909 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:51:12.319830 22909 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:51:12.367954 22909 solver.cpp:327] Iteration 1000, loss = 0.669949
I1006 15:51:12.367981 22909 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:51:12.368517 22909 solver.cpp:415]     Test net output #0: error_blob = 0.655698 (* 1 = 0.655698 loss)
I1006 15:51:12.368530 22909 solver.cpp:332] Optimization Done.
I1006 15:51:12.368535 22909 caffe.cpp:215] Optimization Done.
I1006 15:51:12.496470 22929 caffe.cpp:184] Using GPUs 0
I1006 15:51:13.057682 22929 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_sce/model2_part6.prototxt"
I1006 15:51:13.057713 22929 solver.cpp:97] Creating training net from net file: full_batch_sce/model2_part6.prototxt
I1006 15:51:13.057873 22929 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 15:51:13.057907 22929 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part6.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part6.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:51:13.057945 22929 layer_factory.hpp:76] Creating layer data_layer
I1006 15:51:13.084564 22929 net.cpp:110] Creating Layer data_layer
I1006 15:51:13.084584 22929 net.cpp:433] data_layer -> data_blob
I1006 15:51:13.084607 22929 net.cpp:433] data_layer -> label_blob
I1006 15:51:13.085165 22933 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part6.train
I1006 15:51:13.768173 22929 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 15:51:13.778599 22929 net.cpp:155] Setting up data_layer
I1006 15:51:13.778648 22929 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 15:51:13.778653 22929 net.cpp:163] Top shape: 40000 (40000)
I1006 15:51:13.778661 22929 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:51:13.778672 22929 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:51:13.778677 22929 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:51:13.778687 22929 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:51:13.779048 22929 net.cpp:155] Setting up hidden_sum_layer
I1006 15:51:13.779055 22929 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:51:13.779067 22929 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:51:13.779073 22929 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:51:13.779077 22929 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:51:13.779079 22929 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:51:16.984024 22929 net.cpp:155] Setting up hidden_act_layer
I1006 15:51:16.984045 22929 net.cpp:163] Top shape: 40000 10 (400000)
I1006 15:51:16.984050 22929 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:51:16.984060 22929 net.cpp:110] Creating Layer output_sum_layer
I1006 15:51:16.984062 22929 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:51:16.984067 22929 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:51:16.984153 22929 net.cpp:155] Setting up output_sum_layer
I1006 15:51:16.984158 22929 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:51:16.984165 22929 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:51:16.984170 22929 net.cpp:110] Creating Layer output_act_layer
I1006 15:51:16.984172 22929 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:51:16.984176 22929 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:51:16.984228 22929 net.cpp:155] Setting up output_act_layer
I1006 15:51:16.984248 22929 net.cpp:163] Top shape: 40000 1 (40000)
I1006 15:51:16.984251 22929 layer_factory.hpp:76] Creating layer error_layer
I1006 15:51:16.984256 22929 net.cpp:110] Creating Layer error_layer
I1006 15:51:16.984259 22929 net.cpp:477] error_layer <- output_act_blob
I1006 15:51:16.984261 22929 net.cpp:477] error_layer <- label_blob
I1006 15:51:16.984266 22929 net.cpp:433] error_layer -> error_blob
I1006 15:51:16.984288 22929 net.cpp:155] Setting up error_layer
I1006 15:51:16.984292 22929 net.cpp:163] Top shape: (1)
I1006 15:51:16.984294 22929 net.cpp:168]     with loss weight 1
I1006 15:51:16.984310 22929 net.cpp:236] error_layer needs backward computation.
I1006 15:51:16.984313 22929 net.cpp:236] output_act_layer needs backward computation.
I1006 15:51:16.984315 22929 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:51:16.984318 22929 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:51:16.984319 22929 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:51:16.984321 22929 net.cpp:240] data_layer does not need backward computation.
I1006 15:51:16.984323 22929 net.cpp:283] This network produces output error_blob
I1006 15:51:16.984328 22929 net.cpp:297] Network initialization done.
I1006 15:51:16.984329 22929 net.cpp:298] Memory required for data: 13440004
I1006 15:51:16.984452 22929 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_sce/model2_part6.prototxt
I1006 15:51:16.984467 22929 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 15:51:16.984506 22929 net.cpp:50] Initializing net from parameters: 
name: "full_batch_sce/model2_part6.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part6.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 15:51:16.984527 22929 layer_factory.hpp:76] Creating layer data_layer
I1006 15:51:16.986860 22929 net.cpp:110] Creating Layer data_layer
I1006 15:51:16.986877 22929 net.cpp:433] data_layer -> data_blob
I1006 15:51:16.986882 22929 net.cpp:433] data_layer -> label_blob
I1006 15:51:16.987440 22937 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part6.test
I1006 15:51:16.987505 22929 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 15:51:16.989415 22929 net.cpp:155] Setting up data_layer
I1006 15:51:16.989430 22929 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 15:51:16.989436 22929 net.cpp:163] Top shape: 4000 (4000)
I1006 15:51:16.989441 22929 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 15:51:16.989452 22929 net.cpp:110] Creating Layer hidden_sum_layer
I1006 15:51:16.989456 22929 net.cpp:477] hidden_sum_layer <- data_blob
I1006 15:51:16.989464 22929 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 15:51:16.989570 22929 net.cpp:155] Setting up hidden_sum_layer
I1006 15:51:16.989578 22929 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:51:16.989589 22929 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 15:51:16.989600 22929 net.cpp:110] Creating Layer hidden_act_layer
I1006 15:51:16.989615 22929 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 15:51:16.989621 22929 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 15:51:16.989809 22929 net.cpp:155] Setting up hidden_act_layer
I1006 15:51:16.989815 22929 net.cpp:163] Top shape: 4000 10 (40000)
I1006 15:51:16.989820 22929 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 15:51:16.989827 22929 net.cpp:110] Creating Layer output_sum_layer
I1006 15:51:16.989831 22929 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 15:51:16.989836 22929 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 15:51:16.989903 22929 net.cpp:155] Setting up output_sum_layer
I1006 15:51:16.989910 22929 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:51:16.989918 22929 layer_factory.hpp:76] Creating layer output_act_layer
I1006 15:51:16.989925 22929 net.cpp:110] Creating Layer output_act_layer
I1006 15:51:16.989930 22929 net.cpp:477] output_act_layer <- output_sum_blob
I1006 15:51:16.989935 22929 net.cpp:433] output_act_layer -> output_act_blob
I1006 15:51:16.989991 22929 net.cpp:155] Setting up output_act_layer
I1006 15:51:16.989997 22929 net.cpp:163] Top shape: 4000 1 (4000)
I1006 15:51:16.990001 22929 layer_factory.hpp:76] Creating layer error_layer
I1006 15:51:16.990008 22929 net.cpp:110] Creating Layer error_layer
I1006 15:51:16.990012 22929 net.cpp:477] error_layer <- output_act_blob
I1006 15:51:16.990017 22929 net.cpp:477] error_layer <- label_blob
I1006 15:51:16.990021 22929 net.cpp:433] error_layer -> error_blob
I1006 15:51:16.990046 22929 net.cpp:155] Setting up error_layer
I1006 15:51:16.990051 22929 net.cpp:163] Top shape: (1)
I1006 15:51:16.990054 22929 net.cpp:168]     with loss weight 1
I1006 15:51:16.990068 22929 net.cpp:236] error_layer needs backward computation.
I1006 15:51:16.990072 22929 net.cpp:236] output_act_layer needs backward computation.
I1006 15:51:16.990077 22929 net.cpp:236] output_sum_layer needs backward computation.
I1006 15:51:16.990080 22929 net.cpp:236] hidden_act_layer needs backward computation.
I1006 15:51:16.990083 22929 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 15:51:16.990087 22929 net.cpp:240] data_layer does not need backward computation.
I1006 15:51:16.990092 22929 net.cpp:283] This network produces output error_blob
I1006 15:51:16.990098 22929 net.cpp:297] Network initialization done.
I1006 15:51:16.990102 22929 net.cpp:298] Memory required for data: 1344004
I1006 15:51:16.990125 22929 solver.cpp:66] Solver scaffolding done.
I1006 15:51:16.990221 22929 caffe.cpp:212] Starting Optimization
I1006 15:51:16.990228 22929 solver.cpp:294] Solving full_batch_sce/model2_part6.prototxt
I1006 15:51:16.990232 22929 solver.cpp:295] Learning Rate Policy: fixed
I1006 15:51:16.990409 22929 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 15:51:16.990540 22929 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 15:51:17.000036 22929 solver.cpp:415]     Test net output #0: error_blob = 0.70473 (* 1 = 0.70473 loss)
I1006 15:51:17.003798 22929 solver.cpp:243] Iteration 0, loss = 0.698025
I1006 15:51:17.003818 22929 solver.cpp:259]     Train net output #0: error_blob = 0.698025 (* 1 = 0.698025 loss)
I1006 15:51:17.003829 22929 solver.cpp:590] Iteration 0, lr = 0.01
I1006 15:51:22.019215 22929 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 15:51:22.020630 22929 solver.cpp:415]     Test net output #0: error_blob = 0.696865 (* 1 = 0.696865 loss)
I1006 15:51:22.075618 22929 solver.cpp:243] Iteration 100, loss = 0.690955
I1006 15:51:22.075656 22929 solver.cpp:259]     Train net output #0: error_blob = 0.690955 (* 1 = 0.690955 loss)
I1006 15:51:22.075664 22929 solver.cpp:590] Iteration 100, lr = 0.01
I1006 15:51:27.251376 22929 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 15:51:27.252818 22929 solver.cpp:415]     Test net output #0: error_blob = 0.692346 (* 1 = 0.692346 loss)
I1006 15:51:27.307346 22929 solver.cpp:243] Iteration 200, loss = 0.685091
I1006 15:51:27.307391 22929 solver.cpp:259]     Train net output #0: error_blob = 0.685091 (* 1 = 0.685091 loss)
I1006 15:51:27.307425 22929 solver.cpp:590] Iteration 200, lr = 0.01
I1006 15:51:32.551916 22929 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 15:51:32.553329 22929 solver.cpp:415]     Test net output #0: error_blob = 0.689541 (* 1 = 0.689541 loss)
I1006 15:51:32.607640 22929 solver.cpp:243] Iteration 300, loss = 0.682677
I1006 15:51:32.607677 22929 solver.cpp:259]     Train net output #0: error_blob = 0.682677 (* 1 = 0.682677 loss)
I1006 15:51:32.607683 22929 solver.cpp:590] Iteration 300, lr = 0.01
I1006 15:51:37.693621 22929 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 15:51:37.695046 22929 solver.cpp:415]     Test net output #0: error_blob = 0.687519 (* 1 = 0.687519 loss)
I1006 15:51:37.750831 22929 solver.cpp:243] Iteration 400, loss = 0.678105
I1006 15:51:37.750859 22929 solver.cpp:259]     Train net output #0: error_blob = 0.678105 (* 1 = 0.678105 loss)
I1006 15:51:37.750864 22929 solver.cpp:590] Iteration 400, lr = 0.01
I1006 15:51:43.024801 22929 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 15:51:43.026260 22929 solver.cpp:415]     Test net output #0: error_blob = 0.685967 (* 1 = 0.685967 loss)
I1006 15:51:43.081480 22929 solver.cpp:243] Iteration 500, loss = 0.674836
I1006 15:51:43.081517 22929 solver.cpp:259]     Train net output #0: error_blob = 0.674836 (* 1 = 0.674836 loss)
I1006 15:51:43.081523 22929 solver.cpp:590] Iteration 500, lr = 0.01
I1006 15:51:48.380650 22929 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 15:51:48.382102 22929 solver.cpp:415]     Test net output #0: error_blob = 0.684507 (* 1 = 0.684507 loss)
I1006 15:51:48.437646 22929 solver.cpp:243] Iteration 600, loss = 0.674839
I1006 15:51:48.437681 22929 solver.cpp:259]     Train net output #0: error_blob = 0.674839 (* 1 = 0.674839 loss)
I1006 15:51:48.437688 22929 solver.cpp:590] Iteration 600, lr = 0.01
I1006 15:51:53.531883 22929 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 15:51:53.533360 22929 solver.cpp:415]     Test net output #0: error_blob = 0.683279 (* 1 = 0.683279 loss)
I1006 15:51:53.585377 22929 solver.cpp:243] Iteration 700, loss = 0.674214
I1006 15:51:53.585412 22929 solver.cpp:259]     Train net output #0: error_blob = 0.674214 (* 1 = 0.674214 loss)
I1006 15:51:53.585419 22929 solver.cpp:590] Iteration 700, lr = 0.01
I1006 15:51:58.649088 22929 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 15:51:58.650554 22929 solver.cpp:415]     Test net output #0: error_blob = 0.682224 (* 1 = 0.682224 loss)
I1006 15:51:58.701872 22929 solver.cpp:243] Iteration 800, loss = 0.670438
I1006 15:51:58.701910 22929 solver.cpp:259]     Train net output #0: error_blob = 0.670438 (* 1 = 0.670438 loss)
I1006 15:51:58.701915 22929 solver.cpp:590] Iteration 800, lr = 0.01
I1006 15:52:03.745942 22929 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 15:52:03.747385 22929 solver.cpp:415]     Test net output #0: error_blob = 0.681446 (* 1 = 0.681446 loss)
I1006 15:52:03.800752 22929 solver.cpp:243] Iteration 900, loss = 0.670214
I1006 15:52:03.800791 22929 solver.cpp:259]     Train net output #0: error_blob = 0.670214 (* 1 = 0.670214 loss)
I1006 15:52:03.800797 22929 solver.cpp:590] Iteration 900, lr = 0.01
I1006 15:52:08.844298 22929 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 15:52:08.845561 22929 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 15:52:08.894543 22929 solver.cpp:327] Iteration 1000, loss = 0.66656
I1006 15:52:08.894568 22929 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 15:52:08.895009 22929 solver.cpp:415]     Test net output #0: error_blob = 0.680367 (* 1 = 0.680367 loss)
I1006 15:52:08.895018 22929 solver.cpp:332] Optimization Done.
I1006 15:52:08.895022 22929 caffe.cpp:215] Optimization Done.
