I1006 18:13:04.549752 25377 caffe.cpp:184] Using GPUs 0
I1006 18:13:05.121608 25377 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part2.prototxt"
I1006 18:13:05.121639 25377 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part2.prototxt
I1006 18:13:05.121803 25377 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:05.121850 25377 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part2.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part2.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:05.121924 25377 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:05.148272 25377 net.cpp:110] Creating Layer data_layer
I1006 18:13:05.148299 25377 net.cpp:433] data_layer -> data_blob
I1006 18:13:05.148321 25377 net.cpp:433] data_layer -> label_blob
I1006 18:13:05.148924 25381 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part2.train
I1006 18:13:05.836866 25377 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:05.847414 25377 net.cpp:155] Setting up data_layer
I1006 18:13:05.847481 25377 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:05.847484 25377 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:05.847491 25377 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:05.847502 25377 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:05.847506 25377 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:05.847517 25377 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:05.847898 25377 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:05.847905 25377 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:05.847926 25377 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:05.847944 25377 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:05.847946 25377 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:05.847950 25377 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:09.068089 25377 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:09.068121 25377 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:09.068126 25377 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:09.068136 25377 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:09.068140 25377 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:09.068145 25377 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:09.068245 25377 net.cpp:155] Setting up output_sum_layer
I1006 18:13:09.068250 25377 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:09.068267 25377 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:09.068272 25377 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:09.068274 25377 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:09.068277 25377 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:09.068343 25377 net.cpp:155] Setting up output_act_layer
I1006 18:13:09.068358 25377 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:09.068372 25377 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:09.068392 25377 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7f6d393eedaa  (unknown)
    @     0x7f6d393eece4  (unknown)
    @     0x7f6d393ee6e6  (unknown)
    @     0x7f6d393f1687  (unknown)
    @     0x7f6d398813f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7f6d39887105  caffe::Net<>::Init()
    @     0x7f6d398891a5  caffe::Net<>::Net()
    @     0x7f6d3982438a  caffe::Solver<>::InitTrainNet()
    @     0x7f6d398254cc  caffe::Solver<>::Init()
    @     0x7f6d398257d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7f6d38111ec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:09.229187 25387 caffe.cpp:184] Using GPUs 0
I1006 18:13:09.789654 25387 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part5.prototxt"
I1006 18:13:09.789685 25387 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part5.prototxt
I1006 18:13:09.789854 25387 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:09.789901 25387 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part5.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part5.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:09.789979 25387 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:09.816431 25387 net.cpp:110] Creating Layer data_layer
I1006 18:13:09.816463 25387 net.cpp:433] data_layer -> data_blob
I1006 18:13:09.816504 25387 net.cpp:433] data_layer -> label_blob
I1006 18:13:09.817111 25391 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part5.train
I1006 18:13:10.503123 25387 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:10.513566 25387 net.cpp:155] Setting up data_layer
I1006 18:13:10.513609 25387 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:10.513613 25387 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:10.513628 25387 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:10.513641 25387 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:10.513644 25387 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:10.513654 25387 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:10.514044 25387 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:10.514051 25387 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:10.514073 25387 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:10.514091 25387 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:10.514096 25387 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:10.514098 25387 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:13.764696 25387 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:13.764719 25387 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:13.764724 25387 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:13.764731 25387 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:13.764734 25387 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:13.764740 25387 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:13.764847 25387 net.cpp:155] Setting up output_sum_layer
I1006 18:13:13.764853 25387 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:13.764869 25387 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:13.764875 25387 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:13.764876 25387 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:13.764879 25387 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:13.764943 25387 net.cpp:155] Setting up output_act_layer
I1006 18:13:13.764961 25387 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:13.764963 25387 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:13.764981 25387 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7f59bc85fdaa  (unknown)
    @     0x7f59bc85fce4  (unknown)
    @     0x7f59bc85f6e6  (unknown)
    @     0x7f59bc862687  (unknown)
    @     0x7f59bccf23f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7f59bccf8105  caffe::Net<>::Init()
    @     0x7f59bccfa1a5  caffe::Net<>::Net()
    @     0x7f59bcc9538a  caffe::Solver<>::InitTrainNet()
    @     0x7f59bcc964cc  caffe::Solver<>::Init()
    @     0x7f59bcc967d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7f59bb582ec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:13.886366 25395 caffe.cpp:184] Using GPUs 0
I1006 18:13:14.443927 25395 solver.cpp:54] Initializing solver from parameters: 
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_full.prototxt"
I1006 18:13:14.443955 25395 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_full.prototxt
I1006 18:13:14.444134 25395 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_full.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.full"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:14.444175 25395 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:14.470540 25395 net.cpp:110] Creating Layer data_layer
I1006 18:13:14.470577 25395 net.cpp:433] data_layer -> data_blob
I1006 18:13:14.470609 25395 net.cpp:433] data_layer -> label_blob
I1006 18:13:14.471230 25401 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.full
I1006 18:13:15.158093 25395 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:15.168658 25395 net.cpp:155] Setting up data_layer
I1006 18:13:15.168702 25395 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:15.168717 25395 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:15.168723 25395 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:15.168735 25395 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:15.168740 25395 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:15.168750 25395 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:15.169129 25395 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:15.169137 25395 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:15.169158 25395 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:15.169176 25395 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:15.169180 25395 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:15.169184 25395 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:18.403111 25395 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:18.403133 25395 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:18.403138 25395 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:18.403148 25395 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:18.403151 25395 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:18.403156 25395 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:18.403265 25395 net.cpp:155] Setting up output_sum_layer
I1006 18:13:18.403270 25395 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:18.403276 25395 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:18.403298 25395 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:18.403301 25395 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:18.403303 25395 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:18.403370 25395 net.cpp:155] Setting up output_act_layer
I1006 18:13:18.403374 25395 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:18.403376 25395 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:18.403412 25395 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7f2669ab0daa  (unknown)
    @     0x7f2669ab0ce4  (unknown)
    @     0x7f2669ab06e6  (unknown)
    @     0x7f2669ab3687  (unknown)
    @     0x7f2669f433f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7f2669f49105  caffe::Net<>::Init()
    @     0x7f2669f4b1a5  caffe::Net<>::Net()
    @     0x7f2669ee638a  caffe::Solver<>::InitTrainNet()
    @     0x7f2669ee74cc  caffe::Solver<>::Init()
    @     0x7f2669ee77d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7f26687d3ec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:18.519325 25405 caffe.cpp:184] Using GPUs 0
I1006 18:13:19.080073 25405 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part9.prototxt"
I1006 18:13:19.080103 25405 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part9.prototxt
I1006 18:13:19.080267 25405 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:19.080314 25405 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part9.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part9.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:19.080389 25405 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:19.106843 25405 net.cpp:110] Creating Layer data_layer
I1006 18:13:19.106873 25405 net.cpp:433] data_layer -> data_blob
I1006 18:13:19.106895 25405 net.cpp:433] data_layer -> label_blob
I1006 18:13:19.107508 25409 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part9.train
I1006 18:13:19.792850 25405 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:19.803391 25405 net.cpp:155] Setting up data_layer
I1006 18:13:19.803445 25405 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:19.803450 25405 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:19.803457 25405 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:19.803469 25405 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:19.803472 25405 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:19.803483 25405 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:19.803870 25405 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:19.803877 25405 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:19.803900 25405 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:19.803906 25405 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:19.803908 25405 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:19.803912 25405 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:23.044009 25405 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:23.044034 25405 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:23.044039 25405 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:23.044049 25405 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:23.044051 25405 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:23.044056 25405 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:23.044163 25405 net.cpp:155] Setting up output_sum_layer
I1006 18:13:23.044168 25405 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:23.044175 25405 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:23.044179 25405 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:23.044181 25405 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:23.044184 25405 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:23.044266 25405 net.cpp:155] Setting up output_act_layer
I1006 18:13:23.044291 25405 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:23.044293 25405 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:23.044311 25405 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7f627f66adaa  (unknown)
    @     0x7f627f66ace4  (unknown)
    @     0x7f627f66a6e6  (unknown)
    @     0x7f627f66d687  (unknown)
    @     0x7f627fafd3f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7f627fb03105  caffe::Net<>::Init()
    @     0x7f627fb051a5  caffe::Net<>::Net()
    @     0x7f627faa038a  caffe::Solver<>::InitTrainNet()
    @     0x7f627faa14cc  caffe::Solver<>::Init()
    @     0x7f627faa17d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7f627e38dec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:23.163867 25415 caffe.cpp:184] Using GPUs 0
I1006 18:13:23.725821 25415 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part0.prototxt"
I1006 18:13:23.725852 25415 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part0.prototxt
I1006 18:13:23.726016 25415 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:23.726054 25415 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part0.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part0.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:23.726110 25415 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:23.752581 25415 net.cpp:110] Creating Layer data_layer
I1006 18:13:23.752620 25415 net.cpp:433] data_layer -> data_blob
I1006 18:13:23.752647 25415 net.cpp:433] data_layer -> label_blob
I1006 18:13:23.753253 25419 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part0.train
I1006 18:13:24.438750 25415 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:24.449261 25415 net.cpp:155] Setting up data_layer
I1006 18:13:24.449302 25415 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:24.449307 25415 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:24.449314 25415 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:24.449328 25415 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:24.449334 25415 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:24.449352 25415 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:24.449749 25415 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:24.449756 25415 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:24.449779 25415 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:24.449789 25415 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:24.449795 25415 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:24.449800 25415 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:27.671144 25415 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:27.671165 25415 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:27.671170 25415 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:27.671180 25415 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:27.671183 25415 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:27.671188 25415 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:27.671296 25415 net.cpp:155] Setting up output_sum_layer
I1006 18:13:27.671303 25415 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:27.671310 25415 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:27.671315 25415 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:27.671317 25415 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:27.671321 25415 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:27.671388 25415 net.cpp:155] Setting up output_act_layer
I1006 18:13:27.671408 25415 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:27.671411 25415 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:27.671430 25415 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7f9653fd6daa  (unknown)
    @     0x7f9653fd6ce4  (unknown)
    @     0x7f9653fd66e6  (unknown)
    @     0x7f9653fd9687  (unknown)
    @     0x7f96544693f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7f965446f105  caffe::Net<>::Init()
    @     0x7f96544711a5  caffe::Net<>::Net()
    @     0x7f965440c38a  caffe::Solver<>::InitTrainNet()
    @     0x7f965440d4cc  caffe::Solver<>::Init()
    @     0x7f965440d7d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7f9652cf9ec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:27.791357 25425 caffe.cpp:184] Using GPUs 0
I1006 18:13:28.353760 25425 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part1.prototxt"
I1006 18:13:28.353791 25425 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part1.prototxt
I1006 18:13:28.353958 25425 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:28.354007 25425 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part1.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part1.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:28.354071 25425 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:28.380409 25425 net.cpp:110] Creating Layer data_layer
I1006 18:13:28.380429 25425 net.cpp:433] data_layer -> data_blob
I1006 18:13:28.380461 25425 net.cpp:433] data_layer -> label_blob
I1006 18:13:28.381085 25429 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part1.train
I1006 18:13:29.066140 25425 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:29.076633 25425 net.cpp:155] Setting up data_layer
I1006 18:13:29.076690 25425 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:29.076699 25425 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:29.076705 25425 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:29.076716 25425 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:29.076721 25425 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:29.076730 25425 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:29.077131 25425 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:29.077139 25425 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:29.077160 25425 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:29.077167 25425 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:29.077169 25425 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:29.077173 25425 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:32.313086 25425 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:32.313109 25425 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:32.313114 25425 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:32.313123 25425 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:32.313127 25425 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:32.313133 25425 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:32.313227 25425 net.cpp:155] Setting up output_sum_layer
I1006 18:13:32.313233 25425 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:32.313241 25425 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:32.313246 25425 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:32.313252 25425 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:32.313256 25425 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:32.313318 25425 net.cpp:155] Setting up output_act_layer
I1006 18:13:32.313338 25425 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:32.313339 25425 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:32.313359 25425 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7fc6d6859daa  (unknown)
    @     0x7fc6d6859ce4  (unknown)
    @     0x7fc6d68596e6  (unknown)
    @     0x7fc6d685c687  (unknown)
    @     0x7fc6d6cec3f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7fc6d6cf2105  caffe::Net<>::Init()
    @     0x7fc6d6cf41a5  caffe::Net<>::Net()
    @     0x7fc6d6c8f38a  caffe::Solver<>::InitTrainNet()
    @     0x7fc6d6c904cc  caffe::Solver<>::Init()
    @     0x7fc6d6c907d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7fc6d557cec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:32.430894 25433 caffe.cpp:184] Using GPUs 0
I1006 18:13:32.989073 25433 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part8.prototxt"
I1006 18:13:32.989104 25433 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part8.prototxt
I1006 18:13:32.989269 25433 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:32.989310 25433 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part8.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part8.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:32.989363 25433 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:33.015962 25433 net.cpp:110] Creating Layer data_layer
I1006 18:13:33.015990 25433 net.cpp:433] data_layer -> data_blob
I1006 18:13:33.016016 25433 net.cpp:433] data_layer -> label_blob
I1006 18:13:33.016630 25439 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part8.train
I1006 18:13:33.701752 25433 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:33.712213 25433 net.cpp:155] Setting up data_layer
I1006 18:13:33.712242 25433 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:33.712247 25433 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:33.712255 25433 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:33.712270 25433 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:33.712275 25433 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:33.712286 25433 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:33.712692 25433 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:33.712700 25433 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:33.712713 25433 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:33.712723 25433 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:33.712730 25433 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:33.712735 25433 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:36.943347 25433 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:36.943369 25433 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:36.943374 25433 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:36.943384 25433 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:36.943387 25433 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:36.943393 25433 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:36.943500 25433 net.cpp:155] Setting up output_sum_layer
I1006 18:13:36.943507 25433 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:36.943526 25433 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:36.943541 25433 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:36.943543 25433 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:36.943555 25433 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:36.943619 25433 net.cpp:155] Setting up output_act_layer
I1006 18:13:36.943639 25433 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:36.943641 25433 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:36.943660 25433 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7f7c10617daa  (unknown)
    @     0x7f7c10617ce4  (unknown)
    @     0x7f7c106176e6  (unknown)
    @     0x7f7c1061a687  (unknown)
    @     0x7f7c10aaa3f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7f7c10ab0105  caffe::Net<>::Init()
    @     0x7f7c10ab21a5  caffe::Net<>::Net()
    @     0x7f7c10a4d38a  caffe::Solver<>::InitTrainNet()
    @     0x7f7c10a4e4cc  caffe::Solver<>::Init()
    @     0x7f7c10a4e7d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7f7c0f33aec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:37.056273 25445 caffe.cpp:184] Using GPUs 0
I1006 18:13:37.621492 25445 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part4.prototxt"
I1006 18:13:37.621525 25445 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part4.prototxt
I1006 18:13:37.621691 25445 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:37.621736 25445 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part4.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part4.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:37.621779 25445 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:37.648409 25445 net.cpp:110] Creating Layer data_layer
I1006 18:13:37.648438 25445 net.cpp:433] data_layer -> data_blob
I1006 18:13:37.648464 25445 net.cpp:433] data_layer -> label_blob
I1006 18:13:37.649067 25449 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part4.train
I1006 18:13:38.338570 25445 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:38.349023 25445 net.cpp:155] Setting up data_layer
I1006 18:13:38.349066 25445 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:38.349069 25445 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:38.349077 25445 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:38.349092 25445 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:38.349098 25445 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:38.349113 25445 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:38.349510 25445 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:38.349519 25445 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:38.349540 25445 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:38.349550 25445 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:38.349556 25445 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:38.349561 25445 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:41.588132 25445 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:41.588155 25445 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:41.588160 25445 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:41.588170 25445 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:41.588172 25445 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:41.588178 25445 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:41.588294 25445 net.cpp:155] Setting up output_sum_layer
I1006 18:13:41.588300 25445 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:41.588307 25445 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:41.588312 25445 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:41.588315 25445 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:41.588317 25445 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:41.588385 25445 net.cpp:155] Setting up output_act_layer
I1006 18:13:41.588407 25445 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:41.588409 25445 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:41.588428 25445 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7f53e2057daa  (unknown)
    @     0x7f53e2057ce4  (unknown)
    @     0x7f53e20576e6  (unknown)
    @     0x7f53e205a687  (unknown)
    @     0x7f53e24ea3f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7f53e24f0105  caffe::Net<>::Init()
    @     0x7f53e24f21a5  caffe::Net<>::Net()
    @     0x7f53e248d38a  caffe::Solver<>::InitTrainNet()
    @     0x7f53e248e4cc  caffe::Solver<>::Init()
    @     0x7f53e248e7d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7f53e0d7aec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:41.702136 25453 caffe.cpp:184] Using GPUs 0
I1006 18:13:42.265161 25453 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part7.prototxt"
I1006 18:13:42.265192 25453 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part7.prototxt
I1006 18:13:42.265399 25453 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:42.265441 25453 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part7.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part7.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:42.265486 25453 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:42.292143 25453 net.cpp:110] Creating Layer data_layer
I1006 18:13:42.292165 25453 net.cpp:433] data_layer -> data_blob
I1006 18:13:42.292191 25453 net.cpp:433] data_layer -> label_blob
I1006 18:13:42.292791 25457 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part7.train
I1006 18:13:42.979755 25453 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:42.990267 25453 net.cpp:155] Setting up data_layer
I1006 18:13:42.990304 25453 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:42.990310 25453 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:42.990317 25453 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:42.990331 25453 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:42.990341 25453 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:42.990355 25453 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:42.990751 25453 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:42.990761 25453 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:42.990777 25453 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:42.990788 25453 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:42.990792 25453 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:42.990798 25453 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:46.237494 25453 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:46.237517 25453 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:46.237524 25453 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:46.237546 25453 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:46.237550 25453 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:46.237558 25453 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:46.238430 25453 net.cpp:155] Setting up output_sum_layer
I1006 18:13:46.238437 25453 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:46.238447 25453 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:46.238466 25453 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:46.238469 25453 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:46.238474 25453 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:46.238549 25453 net.cpp:155] Setting up output_act_layer
I1006 18:13:46.238565 25453 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:46.238570 25453 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:46.238605 25453 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7f2a2cd78daa  (unknown)
    @     0x7f2a2cd78ce4  (unknown)
    @     0x7f2a2cd786e6  (unknown)
    @     0x7f2a2cd7b687  (unknown)
    @     0x7f2a2d20b3f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7f2a2d211105  caffe::Net<>::Init()
    @     0x7f2a2d2131a5  caffe::Net<>::Net()
    @     0x7f2a2d1ae38a  caffe::Solver<>::InitTrainNet()
    @     0x7f2a2d1af4cc  caffe::Solver<>::Init()
    @     0x7f2a2d1af7d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7f2a2ba9bec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:46.359539 25463 caffe.cpp:184] Using GPUs 0
I1006 18:13:46.924830 25463 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part3.prototxt"
I1006 18:13:46.924862 25463 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part3.prototxt
I1006 18:13:46.925009 25463 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:46.925043 25463 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part3.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part3.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:46.925091 25463 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:46.950273 25463 net.cpp:110] Creating Layer data_layer
I1006 18:13:46.950300 25463 net.cpp:433] data_layer -> data_blob
I1006 18:13:46.950321 25463 net.cpp:433] data_layer -> label_blob
I1006 18:13:46.950886 25467 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part3.train
I1006 18:13:47.637955 25463 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:47.648401 25463 net.cpp:155] Setting up data_layer
I1006 18:13:47.648440 25463 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:47.648445 25463 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:47.648461 25463 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:47.648474 25463 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:47.648478 25463 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:47.648495 25463 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:47.648876 25463 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:47.648885 25463 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:47.648905 25463 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:47.648923 25463 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:47.648926 25463 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:47.648928 25463 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:50.889430 25463 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:50.889453 25463 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:50.889458 25463 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:50.889468 25463 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:50.889472 25463 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:50.889477 25463 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:50.889575 25463 net.cpp:155] Setting up output_sum_layer
I1006 18:13:50.889580 25463 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:50.889602 25463 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:50.889607 25463 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:50.889618 25463 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:50.889621 25463 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:50.889693 25463 net.cpp:155] Setting up output_act_layer
I1006 18:13:50.889714 25463 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:50.889727 25463 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:50.889765 25463 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7f6dd69b4daa  (unknown)
    @     0x7f6dd69b4ce4  (unknown)
    @     0x7f6dd69b46e6  (unknown)
    @     0x7f6dd69b7687  (unknown)
    @     0x7f6dd6e473f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7f6dd6e4d105  caffe::Net<>::Init()
    @     0x7f6dd6e4f1a5  caffe::Net<>::Net()
    @     0x7f6dd6dea38a  caffe::Solver<>::InitTrainNet()
    @     0x7f6dd6deb4cc  caffe::Solver<>::Init()
    @     0x7f6dd6deb7d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7f6dd56d7ec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
I1006 18:13:51.005540 25473 caffe.cpp:184] Using GPUs 0
I1006 18:13:51.570525 25473 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part6.prototxt"
I1006 18:13:51.570556 25473 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part6.prototxt
I1006 18:13:51.570724 25473 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:13:51.570770 25473 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part6.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part6.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "output_act_layer"
  type: "Sigmoid"
  bottom: "output_sum_blob"
  top: "output_act_blob"
}
layer {
  name: "error_layer"
  type: "CrossEntropyLoss"
  bottom: "output_act_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:13:51.570837 25473 layer_factory.hpp:76] Creating layer data_layer
I1006 18:13:51.597110 25473 net.cpp:110] Creating Layer data_layer
I1006 18:13:51.597144 25473 net.cpp:433] data_layer -> data_blob
I1006 18:13:51.597177 25473 net.cpp:433] data_layer -> label_blob
I1006 18:13:51.597796 25477 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part6.train
I1006 18:13:52.280911 25473 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:13:52.291373 25473 net.cpp:155] Setting up data_layer
I1006 18:13:52.291438 25473 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:13:52.291443 25473 net.cpp:163] Top shape: 40000 (40000)
I1006 18:13:52.291450 25473 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:13:52.291466 25473 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:13:52.291471 25473 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:13:52.291481 25473 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:13:52.291858 25473 net.cpp:155] Setting up hidden_sum_layer
I1006 18:13:52.291865 25473 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:52.291887 25473 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:13:52.291904 25473 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:13:52.291906 25473 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:13:52.291909 25473 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:13:55.528108 25473 net.cpp:155] Setting up hidden_act_layer
I1006 18:13:55.528131 25473 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:13:55.528136 25473 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:13:55.528146 25473 net.cpp:110] Creating Layer output_sum_layer
I1006 18:13:55.528148 25473 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:13:55.528154 25473 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:13:55.528261 25473 net.cpp:155] Setting up output_sum_layer
I1006 18:13:55.528267 25473 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:55.528275 25473 layer_factory.hpp:76] Creating layer output_act_layer
I1006 18:13:55.528280 25473 net.cpp:110] Creating Layer output_act_layer
I1006 18:13:55.528281 25473 net.cpp:477] output_act_layer <- output_sum_blob
I1006 18:13:55.528285 25473 net.cpp:433] output_act_layer -> output_act_blob
I1006 18:13:55.528357 25473 net.cpp:155] Setting up output_act_layer
I1006 18:13:55.528375 25473 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:13:55.528378 25473 layer_factory.hpp:76] Creating layer error_layer
F1006 18:13:55.528406 25473 layer_factory.hpp:80] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: CrossEntropyLoss (known types: AbsVal, Accuracy, ArgMax, BNLL, Concat, ContrastiveLoss, Convolution, Data, Deconvolution, Dropout, DummyData, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, LRN, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Pooling, Power, Python, ReLU, Reduction, Reshape, SPP, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7fbb0ec97daa  (unknown)
    @     0x7fbb0ec97ce4  (unknown)
    @     0x7fbb0ec976e6  (unknown)
    @     0x7fbb0ec9a687  (unknown)
    @     0x7fbb0f12a3f4  caffe::LayerRegistry<>::CreateLayer()
    @     0x7fbb0f130105  caffe::Net<>::Init()
    @     0x7fbb0f1321a5  caffe::Net<>::Net()
    @     0x7fbb0f0cd38a  caffe::Solver<>::InitTrainNet()
    @     0x7fbb0f0ce4cc  caffe::Solver<>::Init()
    @     0x7fbb0f0ce7d9  caffe::Solver<>::Solver()
    @           0x412945  caffe::GetSolver<>()
    @           0x40b626  train()
    @           0x409471  main
    @     0x7fbb0d9baec5  (unknown)
    @           0x409c0b  (unknown)
    @              (nil)  (unknown)
