I1006 18:20:02.122652 26002 caffe.cpp:184] Using GPUs 0
I1006 18:20:02.699656 26002 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part2.prototxt"
I1006 18:20:02.699687 26002 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part2.prototxt
I1006 18:20:02.699859 26002 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:20:02.699894 26002 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part2.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part2.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:20:02.699933 26002 layer_factory.hpp:76] Creating layer data_layer
I1006 18:20:02.726485 26002 net.cpp:110] Creating Layer data_layer
I1006 18:20:02.726516 26002 net.cpp:433] data_layer -> data_blob
I1006 18:20:02.726548 26002 net.cpp:433] data_layer -> label_blob
I1006 18:20:02.727121 26006 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part2.train
I1006 18:20:03.413830 26002 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:20:03.424341 26002 net.cpp:155] Setting up data_layer
I1006 18:20:03.424384 26002 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:20:03.424389 26002 net.cpp:163] Top shape: 40000 (40000)
I1006 18:20:03.424406 26002 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:20:03.424417 26002 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:20:03.424422 26002 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:20:03.424430 26002 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:20:03.424828 26002 net.cpp:155] Setting up hidden_sum_layer
I1006 18:20:03.424834 26002 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:20:03.424856 26002 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:20:03.424873 26002 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:20:03.424876 26002 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:20:03.424880 26002 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:20:06.663790 26002 net.cpp:155] Setting up hidden_act_layer
I1006 18:20:06.663826 26002 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:20:06.663832 26002 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:20:06.663844 26002 net.cpp:110] Creating Layer output_sum_layer
I1006 18:20:06.663848 26002 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:20:06.663866 26002 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:20:06.664005 26002 net.cpp:155] Setting up output_sum_layer
I1006 18:20:06.664011 26002 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:20:06.664027 26002 layer_factory.hpp:76] Creating layer error_layer
I1006 18:20:06.664043 26002 net.cpp:110] Creating Layer error_layer
I1006 18:20:06.664047 26002 net.cpp:477] error_layer <- output_sum_blob
I1006 18:20:06.664048 26002 net.cpp:477] error_layer <- label_blob
I1006 18:20:06.664062 26002 net.cpp:433] error_layer -> error_blob
I1006 18:20:06.664088 26002 net.cpp:155] Setting up error_layer
I1006 18:20:06.664100 26002 net.cpp:163] Top shape: (1)
I1006 18:20:06.664139 26002 net.cpp:168]     with loss weight 1
I1006 18:20:06.664168 26002 net.cpp:236] error_layer needs backward computation.
I1006 18:20:06.664170 26002 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:20:06.664172 26002 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:20:06.664175 26002 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:20:06.664176 26002 net.cpp:240] data_layer does not need backward computation.
I1006 18:20:06.664178 26002 net.cpp:283] This network produces output error_blob
I1006 18:20:06.664182 26002 net.cpp:297] Network initialization done.
I1006 18:20:06.664185 26002 net.cpp:298] Memory required for data: 13280004
I1006 18:20:06.664341 26002 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part2.prototxt
I1006 18:20:06.664363 26002 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:20:06.664413 26002 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part2.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part2.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:20:06.664453 26002 layer_factory.hpp:76] Creating layer data_layer
I1006 18:20:06.666733 26002 net.cpp:110] Creating Layer data_layer
I1006 18:20:06.666750 26002 net.cpp:433] data_layer -> data_blob
I1006 18:20:06.666765 26002 net.cpp:433] data_layer -> label_blob
I1006 18:20:06.667321 26012 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part2.test
I1006 18:20:06.667389 26002 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:20:06.669301 26002 net.cpp:155] Setting up data_layer
I1006 18:20:06.669313 26002 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:20:06.669317 26002 net.cpp:163] Top shape: 4000 (4000)
I1006 18:20:06.669320 26002 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:20:06.669327 26002 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:20:06.669330 26002 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:20:06.669334 26002 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:20:06.669441 26002 net.cpp:155] Setting up hidden_sum_layer
I1006 18:20:06.669447 26002 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:20:06.669456 26002 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:20:06.669459 26002 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:20:06.669461 26002 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:20:06.669464 26002 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:20:06.669538 26002 net.cpp:155] Setting up hidden_act_layer
I1006 18:20:06.669543 26002 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:20:06.669544 26002 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:20:06.669548 26002 net.cpp:110] Creating Layer output_sum_layer
I1006 18:20:06.669550 26002 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:20:06.669553 26002 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:20:06.669613 26002 net.cpp:155] Setting up output_sum_layer
I1006 18:20:06.669617 26002 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:20:06.669633 26002 layer_factory.hpp:76] Creating layer error_layer
I1006 18:20:06.669638 26002 net.cpp:110] Creating Layer error_layer
I1006 18:20:06.669641 26002 net.cpp:477] error_layer <- output_sum_blob
I1006 18:20:06.669643 26002 net.cpp:477] error_layer <- label_blob
I1006 18:20:06.669646 26002 net.cpp:433] error_layer -> error_blob
I1006 18:20:06.669669 26002 net.cpp:155] Setting up error_layer
I1006 18:20:06.669673 26002 net.cpp:163] Top shape: (1)
I1006 18:20:06.669675 26002 net.cpp:168]     with loss weight 1
I1006 18:20:06.669682 26002 net.cpp:236] error_layer needs backward computation.
I1006 18:20:06.669683 26002 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:20:06.669685 26002 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:20:06.669687 26002 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:20:06.669690 26002 net.cpp:240] data_layer does not need backward computation.
I1006 18:20:06.669692 26002 net.cpp:283] This network produces output error_blob
I1006 18:20:06.669695 26002 net.cpp:297] Network initialization done.
I1006 18:20:06.669697 26002 net.cpp:298] Memory required for data: 1328004
I1006 18:20:06.669713 26002 solver.cpp:66] Solver scaffolding done.
I1006 18:20:06.669806 26002 caffe.cpp:212] Starting Optimization
I1006 18:20:06.669811 26002 solver.cpp:294] Solving full_batch_ce/model2_part2.prototxt
I1006 18:20:06.669813 26002 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:20:06.669970 26002 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:20:06.670043 26002 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:20:06.675372 26002 solver.cpp:415]     Test net output #0: error_blob = 0.687025 (* 1 = 0.687025 loss)
I1006 18:20:06.679138 26002 solver.cpp:243] Iteration 0, loss = 0.692633
I1006 18:20:06.679162 26002 solver.cpp:259]     Train net output #0: error_blob = 0.692633 (* 1 = 0.692633 loss)
I1006 18:20:06.679172 26002 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:20:11.500583 26002 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:20:11.501991 26002 solver.cpp:415]     Test net output #0: error_blob = 0.627496 (* 1 = 0.627496 loss)
I1006 18:20:11.556138 26002 solver.cpp:243] Iteration 100, loss = 0.635834
I1006 18:20:11.556174 26002 solver.cpp:259]     Train net output #0: error_blob = 0.635834 (* 1 = 0.635834 loss)
I1006 18:20:11.556181 26002 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:20:16.475796 26002 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:20:16.477231 26002 solver.cpp:415]     Test net output #0: error_blob = 0.610674 (* 1 = 0.610674 loss)
I1006 18:20:16.527806 26002 solver.cpp:243] Iteration 200, loss = 0.619827
I1006 18:20:16.527840 26002 solver.cpp:259]     Train net output #0: error_blob = 0.619827 (* 1 = 0.619827 loss)
I1006 18:20:16.527847 26002 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:20:21.574790 26002 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:20:21.576207 26002 solver.cpp:415]     Test net output #0: error_blob = 0.597733 (* 1 = 0.597733 loss)
I1006 18:20:21.630581 26002 solver.cpp:243] Iteration 300, loss = 0.605125
I1006 18:20:21.630622 26002 solver.cpp:259]     Train net output #0: error_blob = 0.605125 (* 1 = 0.605125 loss)
I1006 18:20:21.630628 26002 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:20:26.570595 26002 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:20:26.571995 26002 solver.cpp:415]     Test net output #0: error_blob = 0.592147 (* 1 = 0.592147 loss)
I1006 18:20:26.622972 26002 solver.cpp:243] Iteration 400, loss = 0.604911
I1006 18:20:26.623003 26002 solver.cpp:259]     Train net output #0: error_blob = 0.604911 (* 1 = 0.604911 loss)
I1006 18:20:26.623008 26002 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:20:31.557320 26002 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:20:31.558775 26002 solver.cpp:415]     Test net output #0: error_blob = 0.589799 (* 1 = 0.589799 loss)
I1006 18:20:31.609627 26002 solver.cpp:243] Iteration 500, loss = 0.596207
I1006 18:20:31.609663 26002 solver.cpp:259]     Train net output #0: error_blob = 0.596207 (* 1 = 0.596207 loss)
I1006 18:20:31.609694 26002 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:20:36.559937 26002 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:20:36.561380 26002 solver.cpp:415]     Test net output #0: error_blob = 0.587883 (* 1 = 0.587883 loss)
I1006 18:20:36.612663 26002 solver.cpp:243] Iteration 600, loss = 0.591091
I1006 18:20:36.612699 26002 solver.cpp:259]     Train net output #0: error_blob = 0.591091 (* 1 = 0.591091 loss)
I1006 18:20:36.612717 26002 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:20:41.547721 26002 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:20:41.549139 26002 solver.cpp:415]     Test net output #0: error_blob = 0.589888 (* 1 = 0.589888 loss)
I1006 18:20:41.599920 26002 solver.cpp:243] Iteration 700, loss = 0.59123
I1006 18:20:41.599951 26002 solver.cpp:259]     Train net output #0: error_blob = 0.59123 (* 1 = 0.59123 loss)
I1006 18:20:41.599956 26002 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:20:46.534620 26002 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:20:46.536062 26002 solver.cpp:415]     Test net output #0: error_blob = 0.584473 (* 1 = 0.584473 loss)
I1006 18:20:46.586727 26002 solver.cpp:243] Iteration 800, loss = 0.589971
I1006 18:20:46.586763 26002 solver.cpp:259]     Train net output #0: error_blob = 0.589971 (* 1 = 0.589971 loss)
I1006 18:20:46.586772 26002 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:20:51.507868 26002 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:20:51.509277 26002 solver.cpp:415]     Test net output #0: error_blob = 0.581995 (* 1 = 0.581995 loss)
I1006 18:20:51.561291 26002 solver.cpp:243] Iteration 900, loss = 0.586822
I1006 18:20:51.561323 26002 solver.cpp:259]     Train net output #0: error_blob = 0.586822 (* 1 = 0.586822 loss)
I1006 18:20:51.561329 26002 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:20:56.467933 26002 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:20:56.470183 26002 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:20:56.517137 26002 solver.cpp:327] Iteration 1000, loss = 0.57908
I1006 18:20:56.517161 26002 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:20:56.517591 26002 solver.cpp:415]     Test net output #0: error_blob = 0.582344 (* 1 = 0.582344 loss)
I1006 18:20:56.517601 26002 solver.cpp:332] Optimization Done.
I1006 18:20:56.517606 26002 caffe.cpp:215] Optimization Done.
I1006 18:20:56.664314 26031 caffe.cpp:184] Using GPUs 0
I1006 18:20:57.226699 26031 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part5.prototxt"
I1006 18:20:57.226730 26031 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part5.prototxt
I1006 18:20:57.226897 26031 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:20:57.226943 26031 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part5.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part5.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:20:57.226979 26031 layer_factory.hpp:76] Creating layer data_layer
I1006 18:20:57.253180 26031 net.cpp:110] Creating Layer data_layer
I1006 18:20:57.253211 26031 net.cpp:433] data_layer -> data_blob
I1006 18:20:57.253233 26031 net.cpp:433] data_layer -> label_blob
I1006 18:20:57.253834 26035 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part5.train
I1006 18:20:57.939898 26031 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:20:57.950453 26031 net.cpp:155] Setting up data_layer
I1006 18:20:57.950495 26031 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:20:57.950500 26031 net.cpp:163] Top shape: 40000 (40000)
I1006 18:20:57.950505 26031 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:20:57.950516 26031 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:20:57.950520 26031 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:20:57.950531 26031 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:20:57.950901 26031 net.cpp:155] Setting up hidden_sum_layer
I1006 18:20:57.950908 26031 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:20:57.950929 26031 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:20:57.950937 26031 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:20:57.950939 26031 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:20:57.950942 26031 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:21:01.198212 26031 net.cpp:155] Setting up hidden_act_layer
I1006 18:21:01.198231 26031 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:21:01.198236 26031 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:21:01.198246 26031 net.cpp:110] Creating Layer output_sum_layer
I1006 18:21:01.198248 26031 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:21:01.198253 26031 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:21:01.198339 26031 net.cpp:155] Setting up output_sum_layer
I1006 18:21:01.198345 26031 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:21:01.198351 26031 layer_factory.hpp:76] Creating layer error_layer
I1006 18:21:01.198359 26031 net.cpp:110] Creating Layer error_layer
I1006 18:21:01.198360 26031 net.cpp:477] error_layer <- output_sum_blob
I1006 18:21:01.198362 26031 net.cpp:477] error_layer <- label_blob
I1006 18:21:01.198365 26031 net.cpp:433] error_layer -> error_blob
I1006 18:21:01.198390 26031 net.cpp:155] Setting up error_layer
I1006 18:21:01.198395 26031 net.cpp:163] Top shape: (1)
I1006 18:21:01.198410 26031 net.cpp:168]     with loss weight 1
I1006 18:21:01.198426 26031 net.cpp:236] error_layer needs backward computation.
I1006 18:21:01.198428 26031 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:21:01.198431 26031 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:21:01.198432 26031 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:21:01.198433 26031 net.cpp:240] data_layer does not need backward computation.
I1006 18:21:01.198436 26031 net.cpp:283] This network produces output error_blob
I1006 18:21:01.198439 26031 net.cpp:297] Network initialization done.
I1006 18:21:01.198441 26031 net.cpp:298] Memory required for data: 13280004
I1006 18:21:01.198549 26031 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part5.prototxt
I1006 18:21:01.198576 26031 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:21:01.198614 26031 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part5.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part5.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:21:01.198632 26031 layer_factory.hpp:76] Creating layer data_layer
I1006 18:21:01.200888 26031 net.cpp:110] Creating Layer data_layer
I1006 18:21:01.200904 26031 net.cpp:433] data_layer -> data_blob
I1006 18:21:01.200909 26031 net.cpp:433] data_layer -> label_blob
I1006 18:21:01.201472 26037 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part5.test
I1006 18:21:01.201537 26031 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:21:01.203496 26031 net.cpp:155] Setting up data_layer
I1006 18:21:01.203512 26031 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:21:01.203516 26031 net.cpp:163] Top shape: 4000 (4000)
I1006 18:21:01.203518 26031 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:21:01.203537 26031 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:21:01.203539 26031 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:21:01.203544 26031 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:21:01.203680 26031 net.cpp:155] Setting up hidden_sum_layer
I1006 18:21:01.203685 26031 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:21:01.203691 26031 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:21:01.203706 26031 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:21:01.203708 26031 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:21:01.203711 26031 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:21:01.203807 26031 net.cpp:155] Setting up hidden_act_layer
I1006 18:21:01.203812 26031 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:21:01.203815 26031 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:21:01.203819 26031 net.cpp:110] Creating Layer output_sum_layer
I1006 18:21:01.203831 26031 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:21:01.203835 26031 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:21:01.203901 26031 net.cpp:155] Setting up output_sum_layer
I1006 18:21:01.203905 26031 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:21:01.203933 26031 layer_factory.hpp:76] Creating layer error_layer
I1006 18:21:01.203938 26031 net.cpp:110] Creating Layer error_layer
I1006 18:21:01.203939 26031 net.cpp:477] error_layer <- output_sum_blob
I1006 18:21:01.203943 26031 net.cpp:477] error_layer <- label_blob
I1006 18:21:01.203944 26031 net.cpp:433] error_layer -> error_blob
I1006 18:21:01.203968 26031 net.cpp:155] Setting up error_layer
I1006 18:21:01.203971 26031 net.cpp:163] Top shape: (1)
I1006 18:21:01.203982 26031 net.cpp:168]     with loss weight 1
I1006 18:21:01.204000 26031 net.cpp:236] error_layer needs backward computation.
I1006 18:21:01.204002 26031 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:21:01.204005 26031 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:21:01.204006 26031 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:21:01.204008 26031 net.cpp:240] data_layer does not need backward computation.
I1006 18:21:01.204010 26031 net.cpp:283] This network produces output error_blob
I1006 18:21:01.204015 26031 net.cpp:297] Network initialization done.
I1006 18:21:01.204017 26031 net.cpp:298] Memory required for data: 1328004
I1006 18:21:01.204036 26031 solver.cpp:66] Solver scaffolding done.
I1006 18:21:01.204152 26031 caffe.cpp:212] Starting Optimization
I1006 18:21:01.204159 26031 solver.cpp:294] Solving full_batch_ce/model2_part5.prototxt
I1006 18:21:01.204159 26031 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:21:01.204336 26031 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:21:01.204406 26031 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:21:01.209530 26031 solver.cpp:415]     Test net output #0: error_blob = 0.674788 (* 1 = 0.674788 loss)
I1006 18:21:01.213157 26031 solver.cpp:243] Iteration 0, loss = 0.679382
I1006 18:21:01.213186 26031 solver.cpp:259]     Train net output #0: error_blob = 0.679382 (* 1 = 0.679382 loss)
I1006 18:21:01.213193 26031 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:21:06.002022 26031 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:21:06.003473 26031 solver.cpp:415]     Test net output #0: error_blob = 0.620841 (* 1 = 0.620841 loss)
I1006 18:21:06.057055 26031 solver.cpp:243] Iteration 100, loss = 0.629796
I1006 18:21:06.057081 26031 solver.cpp:259]     Train net output #0: error_blob = 0.629796 (* 1 = 0.629796 loss)
I1006 18:21:06.057086 26031 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:21:10.898114 26031 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:21:10.899583 26031 solver.cpp:415]     Test net output #0: error_blob = 0.606211 (* 1 = 0.606211 loss)
I1006 18:21:10.950122 26031 solver.cpp:243] Iteration 200, loss = 0.619525
I1006 18:21:10.950151 26031 solver.cpp:259]     Train net output #0: error_blob = 0.619525 (* 1 = 0.619525 loss)
I1006 18:21:10.950156 26031 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:21:15.838876 26031 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:21:15.840296 26031 solver.cpp:415]     Test net output #0: error_blob = 0.595219 (* 1 = 0.595219 loss)
I1006 18:21:15.890732 26031 solver.cpp:243] Iteration 300, loss = 0.606908
I1006 18:21:15.890763 26031 solver.cpp:259]     Train net output #0: error_blob = 0.606908 (* 1 = 0.606908 loss)
I1006 18:21:15.890770 26031 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:21:20.784011 26031 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:21:20.785418 26031 solver.cpp:415]     Test net output #0: error_blob = 0.588644 (* 1 = 0.588644 loss)
I1006 18:21:20.838572 26031 solver.cpp:243] Iteration 400, loss = 0.60469
I1006 18:21:20.838599 26031 solver.cpp:259]     Train net output #0: error_blob = 0.60469 (* 1 = 0.60469 loss)
I1006 18:21:20.838603 26031 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:21:25.727385 26031 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:21:25.728790 26031 solver.cpp:415]     Test net output #0: error_blob = 0.582829 (* 1 = 0.582829 loss)
I1006 18:21:25.781136 26031 solver.cpp:243] Iteration 500, loss = 0.596242
I1006 18:21:25.781167 26031 solver.cpp:259]     Train net output #0: error_blob = 0.596242 (* 1 = 0.596242 loss)
I1006 18:21:25.781201 26031 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:21:30.667057 26031 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:21:30.668483 26031 solver.cpp:415]     Test net output #0: error_blob = 0.578017 (* 1 = 0.578017 loss)
I1006 18:21:30.717141 26031 solver.cpp:243] Iteration 600, loss = 0.600718
I1006 18:21:30.717170 26031 solver.cpp:259]     Train net output #0: error_blob = 0.600718 (* 1 = 0.600718 loss)
I1006 18:21:30.717175 26031 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:21:35.614190 26031 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:21:35.615671 26031 solver.cpp:415]     Test net output #0: error_blob = 0.575106 (* 1 = 0.575106 loss)
I1006 18:21:35.666359 26031 solver.cpp:243] Iteration 700, loss = 0.59003
I1006 18:21:35.666384 26031 solver.cpp:259]     Train net output #0: error_blob = 0.59003 (* 1 = 0.59003 loss)
I1006 18:21:35.666389 26031 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:21:40.565505 26031 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:21:40.566911 26031 solver.cpp:415]     Test net output #0: error_blob = 0.572404 (* 1 = 0.572404 loss)
I1006 18:21:40.619235 26031 solver.cpp:243] Iteration 800, loss = 0.590292
I1006 18:21:40.619264 26031 solver.cpp:259]     Train net output #0: error_blob = 0.590292 (* 1 = 0.590292 loss)
I1006 18:21:40.619271 26031 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:21:45.517216 26031 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:21:45.518597 26031 solver.cpp:415]     Test net output #0: error_blob = 0.570521 (* 1 = 0.570521 loss)
I1006 18:21:45.568001 26031 solver.cpp:243] Iteration 900, loss = 0.585764
I1006 18:21:45.568040 26031 solver.cpp:259]     Train net output #0: error_blob = 0.585764 (* 1 = 0.585764 loss)
I1006 18:21:45.568047 26031 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:21:50.437912 26031 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:21:50.439178 26031 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:21:50.486886 26031 solver.cpp:327] Iteration 1000, loss = 0.581605
I1006 18:21:50.486912 26031 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:21:50.487397 26031 solver.cpp:415]     Test net output #0: error_blob = 0.566811 (* 1 = 0.566811 loss)
I1006 18:21:50.487409 26031 solver.cpp:332] Optimization Done.
I1006 18:21:50.487414 26031 caffe.cpp:215] Optimization Done.
I1006 18:21:50.626366 26060 caffe.cpp:184] Using GPUs 0
I1006 18:21:51.183115 26060 solver.cpp:54] Initializing solver from parameters: 
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_full.prototxt"
I1006 18:21:51.183145 26060 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_full.prototxt
I1006 18:21:51.183305 26060 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_full.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.full"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:21:51.183353 26060 layer_factory.hpp:76] Creating layer data_layer
I1006 18:21:51.210224 26060 net.cpp:110] Creating Layer data_layer
I1006 18:21:51.210245 26060 net.cpp:433] data_layer -> data_blob
I1006 18:21:51.210271 26060 net.cpp:433] data_layer -> label_blob
I1006 18:21:51.210841 26064 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.full
I1006 18:21:51.897905 26060 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:21:51.908474 26060 net.cpp:155] Setting up data_layer
I1006 18:21:51.908535 26060 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:21:51.908542 26060 net.cpp:163] Top shape: 40000 (40000)
I1006 18:21:51.908551 26060 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:21:51.908566 26060 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:21:51.908572 26060 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:21:51.908586 26060 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:21:51.908973 26060 net.cpp:155] Setting up hidden_sum_layer
I1006 18:21:51.908982 26060 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:21:51.908998 26060 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:21:51.909010 26060 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:21:51.909015 26060 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:21:51.909020 26060 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:21:55.158093 26060 net.cpp:155] Setting up hidden_act_layer
I1006 18:21:55.158119 26060 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:21:55.158126 26060 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:21:55.158150 26060 net.cpp:110] Creating Layer output_sum_layer
I1006 18:21:55.158156 26060 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:21:55.158165 26060 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:21:55.158293 26060 net.cpp:155] Setting up output_sum_layer
I1006 18:21:55.158300 26060 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:21:55.158320 26060 layer_factory.hpp:76] Creating layer error_layer
I1006 18:21:55.158331 26060 net.cpp:110] Creating Layer error_layer
I1006 18:21:55.158335 26060 net.cpp:477] error_layer <- output_sum_blob
I1006 18:21:55.158339 26060 net.cpp:477] error_layer <- label_blob
I1006 18:21:55.158345 26060 net.cpp:433] error_layer -> error_blob
I1006 18:21:55.158385 26060 net.cpp:155] Setting up error_layer
I1006 18:21:55.158390 26060 net.cpp:163] Top shape: (1)
I1006 18:21:55.158393 26060 net.cpp:168]     with loss weight 1
I1006 18:21:55.158424 26060 net.cpp:236] error_layer needs backward computation.
I1006 18:21:55.158429 26060 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:21:55.158445 26060 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:21:55.158450 26060 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:21:55.158463 26060 net.cpp:240] data_layer does not need backward computation.
I1006 18:21:55.158465 26060 net.cpp:283] This network produces output error_blob
I1006 18:21:55.158483 26060 net.cpp:297] Network initialization done.
I1006 18:21:55.158485 26060 net.cpp:298] Memory required for data: 13280004
I1006 18:21:55.158511 26060 solver.cpp:66] Solver scaffolding done.
I1006 18:21:55.158627 26060 caffe.cpp:212] Starting Optimization
I1006 18:21:55.158633 26060 solver.cpp:294] Solving full_batch_ce/model2_full.prototxt
I1006 18:21:55.158637 26060 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:21:55.162530 26060 solver.cpp:243] Iteration 0, loss = 0.72737
I1006 18:21:55.162546 26060 solver.cpp:259]     Train net output #0: error_blob = 0.72737 (* 1 = 0.72737 loss)
I1006 18:21:55.162565 26060 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:21:55.169883 26060 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:22:00.110630 26060 solver.cpp:243] Iteration 100, loss = 0.636343
I1006 18:22:00.110664 26060 solver.cpp:259]     Train net output #0: error_blob = 0.636343 (* 1 = 0.636343 loss)
I1006 18:22:00.110671 26060 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:22:05.234061 26060 solver.cpp:243] Iteration 200, loss = 0.618493
I1006 18:22:05.234100 26060 solver.cpp:259]     Train net output #0: error_blob = 0.618493 (* 1 = 0.618493 loss)
I1006 18:22:05.234107 26060 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:22:10.289629 26060 solver.cpp:243] Iteration 300, loss = 0.608292
I1006 18:22:10.289665 26060 solver.cpp:259]     Train net output #0: error_blob = 0.608292 (* 1 = 0.608292 loss)
I1006 18:22:10.289672 26060 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:22:15.400758 26060 solver.cpp:243] Iteration 400, loss = 0.59907
I1006 18:22:15.400802 26060 solver.cpp:259]     Train net output #0: error_blob = 0.59907 (* 1 = 0.59907 loss)
I1006 18:22:15.400810 26060 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:22:20.494532 26060 solver.cpp:243] Iteration 500, loss = 0.595206
I1006 18:22:20.494568 26060 solver.cpp:259]     Train net output #0: error_blob = 0.595206 (* 1 = 0.595206 loss)
I1006 18:22:20.494576 26060 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:22:25.581918 26060 solver.cpp:243] Iteration 600, loss = 0.594185
I1006 18:22:25.581990 26060 solver.cpp:259]     Train net output #0: error_blob = 0.594185 (* 1 = 0.594185 loss)
I1006 18:22:25.581998 26060 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:22:30.635854 26060 solver.cpp:243] Iteration 700, loss = 0.590664
I1006 18:22:30.635893 26060 solver.cpp:259]     Train net output #0: error_blob = 0.590664 (* 1 = 0.590664 loss)
I1006 18:22:30.635898 26060 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:22:35.698500 26060 solver.cpp:243] Iteration 800, loss = 0.589316
I1006 18:22:35.698539 26060 solver.cpp:259]     Train net output #0: error_blob = 0.589316 (* 1 = 0.589316 loss)
I1006 18:22:35.698544 26060 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:22:40.740330 26060 solver.cpp:243] Iteration 900, loss = 0.583994
I1006 18:22:40.740375 26060 solver.cpp:259]     Train net output #0: error_blob = 0.583994 (* 1 = 0.583994 loss)
I1006 18:22:40.740393 26060 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:22:45.792098 26060 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:22:45.793333 26060 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:22:45.841291 26060 solver.cpp:327] Iteration 1000, loss = 0.582703
I1006 18:22:45.841325 26060 solver.cpp:332] Optimization Done.
I1006 18:22:45.841328 26060 caffe.cpp:215] Optimization Done.
I1006 18:22:45.969895 26088 caffe.cpp:184] Using GPUs 0
I1006 18:22:46.531852 26088 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part9.prototxt"
I1006 18:22:46.531882 26088 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part9.prototxt
I1006 18:22:46.532089 26088 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:22:46.532135 26088 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part9.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part9.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:22:46.532212 26088 layer_factory.hpp:76] Creating layer data_layer
I1006 18:22:46.558188 26088 net.cpp:110] Creating Layer data_layer
I1006 18:22:46.558208 26088 net.cpp:433] data_layer -> data_blob
I1006 18:22:46.558238 26088 net.cpp:433] data_layer -> label_blob
I1006 18:22:46.558841 26092 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part9.train
I1006 18:22:47.246522 26088 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:22:47.257063 26088 net.cpp:155] Setting up data_layer
I1006 18:22:47.257096 26088 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:22:47.257100 26088 net.cpp:163] Top shape: 40000 (40000)
I1006 18:22:47.257117 26088 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:22:47.257128 26088 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:22:47.257132 26088 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:22:47.257140 26088 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:22:47.257540 26088 net.cpp:155] Setting up hidden_sum_layer
I1006 18:22:47.257549 26088 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:22:47.257570 26088 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:22:47.257576 26088 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:22:47.257578 26088 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:22:47.257581 26088 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:22:50.489333 26088 net.cpp:155] Setting up hidden_act_layer
I1006 18:22:50.489356 26088 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:22:50.489362 26088 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:22:50.489372 26088 net.cpp:110] Creating Layer output_sum_layer
I1006 18:22:50.489374 26088 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:22:50.489380 26088 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:22:50.489491 26088 net.cpp:155] Setting up output_sum_layer
I1006 18:22:50.489497 26088 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:22:50.489513 26088 layer_factory.hpp:76] Creating layer error_layer
I1006 18:22:50.489529 26088 net.cpp:110] Creating Layer error_layer
I1006 18:22:50.489531 26088 net.cpp:477] error_layer <- output_sum_blob
I1006 18:22:50.489533 26088 net.cpp:477] error_layer <- label_blob
I1006 18:22:50.489547 26088 net.cpp:433] error_layer -> error_blob
I1006 18:22:50.489572 26088 net.cpp:155] Setting up error_layer
I1006 18:22:50.489584 26088 net.cpp:163] Top shape: (1)
I1006 18:22:50.489596 26088 net.cpp:168]     with loss weight 1
I1006 18:22:50.489622 26088 net.cpp:236] error_layer needs backward computation.
I1006 18:22:50.489624 26088 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:22:50.489626 26088 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:22:50.489629 26088 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:22:50.489630 26088 net.cpp:240] data_layer does not need backward computation.
I1006 18:22:50.489631 26088 net.cpp:283] This network produces output error_blob
I1006 18:22:50.489636 26088 net.cpp:297] Network initialization done.
I1006 18:22:50.489637 26088 net.cpp:298] Memory required for data: 13280004
I1006 18:22:50.489760 26088 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part9.prototxt
I1006 18:22:50.489790 26088 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:22:50.489847 26088 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part9.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part9.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:22:50.489887 26088 layer_factory.hpp:76] Creating layer data_layer
I1006 18:22:50.492142 26088 net.cpp:110] Creating Layer data_layer
I1006 18:22:50.492147 26088 net.cpp:433] data_layer -> data_blob
I1006 18:22:50.492151 26088 net.cpp:433] data_layer -> label_blob
I1006 18:22:50.492736 26096 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part9.test
I1006 18:22:50.492817 26088 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:22:50.494685 26088 net.cpp:155] Setting up data_layer
I1006 18:22:50.494707 26088 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:22:50.494710 26088 net.cpp:163] Top shape: 4000 (4000)
I1006 18:22:50.494714 26088 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:22:50.494730 26088 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:22:50.494734 26088 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:22:50.494737 26088 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:22:50.494843 26088 net.cpp:155] Setting up hidden_sum_layer
I1006 18:22:50.494846 26088 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:22:50.494853 26088 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:22:50.494868 26088 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:22:50.494870 26088 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:22:50.494874 26088 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:22:50.494966 26088 net.cpp:155] Setting up hidden_act_layer
I1006 18:22:50.494971 26088 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:22:50.494972 26088 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:22:50.494976 26088 net.cpp:110] Creating Layer output_sum_layer
I1006 18:22:50.494988 26088 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:22:50.494992 26088 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:22:50.495059 26088 net.cpp:155] Setting up output_sum_layer
I1006 18:22:50.495062 26088 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:22:50.495088 26088 layer_factory.hpp:76] Creating layer error_layer
I1006 18:22:50.495093 26088 net.cpp:110] Creating Layer error_layer
I1006 18:22:50.495095 26088 net.cpp:477] error_layer <- output_sum_blob
I1006 18:22:50.495097 26088 net.cpp:477] error_layer <- label_blob
I1006 18:22:50.495100 26088 net.cpp:433] error_layer -> error_blob
I1006 18:22:50.495122 26088 net.cpp:155] Setting up error_layer
I1006 18:22:50.495136 26088 net.cpp:163] Top shape: (1)
I1006 18:22:50.495137 26088 net.cpp:168]     with loss weight 1
I1006 18:22:50.495143 26088 net.cpp:236] error_layer needs backward computation.
I1006 18:22:50.495157 26088 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:22:50.495158 26088 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:22:50.495159 26088 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:22:50.495162 26088 net.cpp:240] data_layer does not need backward computation.
I1006 18:22:50.495163 26088 net.cpp:283] This network produces output error_blob
I1006 18:22:50.495167 26088 net.cpp:297] Network initialization done.
I1006 18:22:50.495169 26088 net.cpp:298] Memory required for data: 1328004
I1006 18:22:50.495187 26088 solver.cpp:66] Solver scaffolding done.
I1006 18:22:50.495283 26088 caffe.cpp:212] Starting Optimization
I1006 18:22:50.495290 26088 solver.cpp:294] Solving full_batch_ce/model2_part9.prototxt
I1006 18:22:50.495290 26088 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:22:50.495456 26088 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:22:50.495530 26088 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:22:50.500726 26088 solver.cpp:415]     Test net output #0: error_blob = 0.74822 (* 1 = 0.74822 loss)
I1006 18:22:50.504315 26088 solver.cpp:243] Iteration 0, loss = 0.736754
I1006 18:22:50.504336 26088 solver.cpp:259]     Train net output #0: error_blob = 0.736754 (* 1 = 0.736754 loss)
I1006 18:22:50.504344 26088 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:22:55.288610 26088 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:22:55.289986 26088 solver.cpp:415]     Test net output #0: error_blob = 0.648093 (* 1 = 0.648093 loss)
I1006 18:22:55.343621 26088 solver.cpp:243] Iteration 100, loss = 0.644072
I1006 18:22:55.343650 26088 solver.cpp:259]     Train net output #0: error_blob = 0.644072 (* 1 = 0.644072 loss)
I1006 18:22:55.343657 26088 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:23:00.267963 26088 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:23:00.269387 26088 solver.cpp:415]     Test net output #0: error_blob = 0.632112 (* 1 = 0.632112 loss)
I1006 18:23:00.323858 26088 solver.cpp:243] Iteration 200, loss = 0.623565
I1006 18:23:00.323885 26088 solver.cpp:259]     Train net output #0: error_blob = 0.623565 (* 1 = 0.623565 loss)
I1006 18:23:00.323890 26088 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:23:05.221956 26088 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:23:05.223382 26088 solver.cpp:415]     Test net output #0: error_blob = 0.625535 (* 1 = 0.625535 loss)
I1006 18:23:05.278000 26088 solver.cpp:243] Iteration 300, loss = 0.613136
I1006 18:23:05.278034 26088 solver.cpp:259]     Train net output #0: error_blob = 0.613136 (* 1 = 0.613136 loss)
I1006 18:23:05.278041 26088 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:23:10.206730 26088 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:23:10.208117 26088 solver.cpp:415]     Test net output #0: error_blob = 0.625481 (* 1 = 0.625481 loss)
I1006 18:23:10.262744 26088 solver.cpp:243] Iteration 400, loss = 0.597904
I1006 18:23:10.262779 26088 solver.cpp:259]     Train net output #0: error_blob = 0.597904 (* 1 = 0.597904 loss)
I1006 18:23:10.262786 26088 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:23:15.198966 26088 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:23:15.200389 26088 solver.cpp:415]     Test net output #0: error_blob = 0.634258 (* 1 = 0.634258 loss)
I1006 18:23:15.251914 26088 solver.cpp:243] Iteration 500, loss = 0.591248
I1006 18:23:15.251945 26088 solver.cpp:259]     Train net output #0: error_blob = 0.591248 (* 1 = 0.591248 loss)
I1006 18:23:15.251981 26088 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:23:20.168169 26088 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:23:20.169562 26088 solver.cpp:415]     Test net output #0: error_blob = 0.642734 (* 1 = 0.642734 loss)
I1006 18:23:20.220041 26088 solver.cpp:243] Iteration 600, loss = 0.591148
I1006 18:23:20.220072 26088 solver.cpp:259]     Train net output #0: error_blob = 0.591148 (* 1 = 0.591148 loss)
I1006 18:23:20.220077 26088 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:23:25.117669 26088 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:23:25.119051 26088 solver.cpp:415]     Test net output #0: error_blob = 0.649404 (* 1 = 0.649404 loss)
I1006 18:23:25.173621 26088 solver.cpp:243] Iteration 700, loss = 0.581275
I1006 18:23:25.173650 26088 solver.cpp:259]     Train net output #0: error_blob = 0.581275 (* 1 = 0.581275 loss)
I1006 18:23:25.173657 26088 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:23:30.069305 26088 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:23:30.070721 26088 solver.cpp:415]     Test net output #0: error_blob = 0.655482 (* 1 = 0.655482 loss)
I1006 18:23:30.120622 26088 solver.cpp:243] Iteration 800, loss = 0.582005
I1006 18:23:30.120652 26088 solver.cpp:259]     Train net output #0: error_blob = 0.582005 (* 1 = 0.582005 loss)
I1006 18:23:30.120661 26088 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:23:35.126266 26088 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:23:35.127660 26088 solver.cpp:415]     Test net output #0: error_blob = 0.663363 (* 1 = 0.663363 loss)
I1006 18:23:35.179512 26088 solver.cpp:243] Iteration 900, loss = 0.584843
I1006 18:23:35.179543 26088 solver.cpp:259]     Train net output #0: error_blob = 0.584843 (* 1 = 0.584843 loss)
I1006 18:23:35.179550 26088 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:23:40.049993 26088 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:23:40.051265 26088 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:23:40.099447 26088 solver.cpp:327] Iteration 1000, loss = 0.569285
I1006 18:23:40.099475 26088 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:23:40.099901 26088 solver.cpp:415]     Test net output #0: error_blob = 0.663584 (* 1 = 0.663584 loss)
I1006 18:23:40.099910 26088 solver.cpp:332] Optimization Done.
I1006 18:23:40.099915 26088 caffe.cpp:215] Optimization Done.
I1006 18:23:40.235518 26117 caffe.cpp:184] Using GPUs 0
I1006 18:23:40.798943 26117 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part0.prototxt"
I1006 18:23:40.798974 26117 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part0.prototxt
I1006 18:23:40.799141 26117 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:23:40.799186 26117 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part0.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part0.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:23:40.799222 26117 layer_factory.hpp:76] Creating layer data_layer
I1006 18:23:40.825495 26117 net.cpp:110] Creating Layer data_layer
I1006 18:23:40.825525 26117 net.cpp:433] data_layer -> data_blob
I1006 18:23:40.825557 26117 net.cpp:433] data_layer -> label_blob
I1006 18:23:40.826169 26123 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part0.train
I1006 18:23:41.511724 26117 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:23:41.522220 26117 net.cpp:155] Setting up data_layer
I1006 18:23:41.522284 26117 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:23:41.522289 26117 net.cpp:163] Top shape: 40000 (40000)
I1006 18:23:41.522296 26117 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:23:41.522310 26117 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:23:41.522313 26117 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:23:41.522323 26117 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:23:41.522724 26117 net.cpp:155] Setting up hidden_sum_layer
I1006 18:23:41.522732 26117 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:23:41.522754 26117 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:23:41.522771 26117 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:23:41.522776 26117 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:23:41.522779 26117 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:23:44.762212 26117 net.cpp:155] Setting up hidden_act_layer
I1006 18:23:44.762243 26117 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:23:44.762248 26117 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:23:44.762259 26117 net.cpp:110] Creating Layer output_sum_layer
I1006 18:23:44.762261 26117 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:23:44.762276 26117 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:23:44.762389 26117 net.cpp:155] Setting up output_sum_layer
I1006 18:23:44.762394 26117 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:23:44.762410 26117 layer_factory.hpp:76] Creating layer error_layer
I1006 18:23:44.762428 26117 net.cpp:110] Creating Layer error_layer
I1006 18:23:44.762430 26117 net.cpp:477] error_layer <- output_sum_blob
I1006 18:23:44.762434 26117 net.cpp:477] error_layer <- label_blob
I1006 18:23:44.762437 26117 net.cpp:433] error_layer -> error_blob
I1006 18:23:44.762464 26117 net.cpp:155] Setting up error_layer
I1006 18:23:44.762466 26117 net.cpp:163] Top shape: (1)
I1006 18:23:44.762501 26117 net.cpp:168]     with loss weight 1
I1006 18:23:44.762518 26117 net.cpp:236] error_layer needs backward computation.
I1006 18:23:44.762521 26117 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:23:44.762522 26117 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:23:44.762524 26117 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:23:44.762526 26117 net.cpp:240] data_layer does not need backward computation.
I1006 18:23:44.762528 26117 net.cpp:283] This network produces output error_blob
I1006 18:23:44.762533 26117 net.cpp:297] Network initialization done.
I1006 18:23:44.762536 26117 net.cpp:298] Memory required for data: 13280004
I1006 18:23:44.762660 26117 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part0.prototxt
I1006 18:23:44.762672 26117 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:23:44.762712 26117 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part0.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part0.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:23:44.762750 26117 layer_factory.hpp:76] Creating layer data_layer
I1006 18:23:44.765141 26117 net.cpp:110] Creating Layer data_layer
I1006 18:23:44.765157 26117 net.cpp:433] data_layer -> data_blob
I1006 18:23:44.765162 26117 net.cpp:433] data_layer -> label_blob
I1006 18:23:44.765733 26125 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part0.test
I1006 18:23:44.765812 26117 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:23:44.767727 26117 net.cpp:155] Setting up data_layer
I1006 18:23:44.767740 26117 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:23:44.767743 26117 net.cpp:163] Top shape: 4000 (4000)
I1006 18:23:44.767747 26117 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:23:44.767755 26117 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:23:44.767757 26117 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:23:44.767762 26117 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:23:44.767885 26117 net.cpp:155] Setting up hidden_sum_layer
I1006 18:23:44.767890 26117 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:23:44.767897 26117 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:23:44.767902 26117 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:23:44.767904 26117 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:23:44.767907 26117 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:23:44.767978 26117 net.cpp:155] Setting up hidden_act_layer
I1006 18:23:44.767983 26117 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:23:44.767985 26117 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:23:44.767989 26117 net.cpp:110] Creating Layer output_sum_layer
I1006 18:23:44.767992 26117 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:23:44.767994 26117 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:23:44.768051 26117 net.cpp:155] Setting up output_sum_layer
I1006 18:23:44.768056 26117 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:23:44.768074 26117 layer_factory.hpp:76] Creating layer error_layer
I1006 18:23:44.768079 26117 net.cpp:110] Creating Layer error_layer
I1006 18:23:44.768080 26117 net.cpp:477] error_layer <- output_sum_blob
I1006 18:23:44.768084 26117 net.cpp:477] error_layer <- label_blob
I1006 18:23:44.768086 26117 net.cpp:433] error_layer -> error_blob
I1006 18:23:44.768107 26117 net.cpp:155] Setting up error_layer
I1006 18:23:44.768111 26117 net.cpp:163] Top shape: (1)
I1006 18:23:44.768113 26117 net.cpp:168]     with loss weight 1
I1006 18:23:44.768121 26117 net.cpp:236] error_layer needs backward computation.
I1006 18:23:44.768122 26117 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:23:44.768124 26117 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:23:44.768126 26117 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:23:44.768129 26117 net.cpp:240] data_layer does not need backward computation.
I1006 18:23:44.768131 26117 net.cpp:283] This network produces output error_blob
I1006 18:23:44.768134 26117 net.cpp:297] Network initialization done.
I1006 18:23:44.768136 26117 net.cpp:298] Memory required for data: 1328004
I1006 18:23:44.768154 26117 solver.cpp:66] Solver scaffolding done.
I1006 18:23:44.768240 26117 caffe.cpp:212] Starting Optimization
I1006 18:23:44.768246 26117 solver.cpp:294] Solving full_batch_ce/model2_part0.prototxt
I1006 18:23:44.768249 26117 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:23:44.768390 26117 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:23:44.768440 26117 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:23:44.779660 26117 solver.cpp:415]     Test net output #0: error_blob = 0.70431 (* 1 = 0.70431 loss)
I1006 18:23:44.783285 26117 solver.cpp:243] Iteration 0, loss = 0.68952
I1006 18:23:44.783305 26117 solver.cpp:259]     Train net output #0: error_blob = 0.68952 (* 1 = 0.68952 loss)
I1006 18:23:44.783313 26117 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:23:49.690932 26117 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:23:49.692306 26117 solver.cpp:415]     Test net output #0: error_blob = 0.636126 (* 1 = 0.636126 loss)
I1006 18:23:49.747652 26117 solver.cpp:243] Iteration 100, loss = 0.617523
I1006 18:23:49.747678 26117 solver.cpp:259]     Train net output #0: error_blob = 0.617523 (* 1 = 0.617523 loss)
I1006 18:23:49.747684 26117 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:23:54.728070 26117 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:23:54.729537 26117 solver.cpp:415]     Test net output #0: error_blob = 0.627838 (* 1 = 0.627838 loss)
I1006 18:23:54.780534 26117 solver.cpp:243] Iteration 200, loss = 0.606866
I1006 18:23:54.780560 26117 solver.cpp:259]     Train net output #0: error_blob = 0.606866 (* 1 = 0.606866 loss)
I1006 18:23:54.780565 26117 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:23:59.813010 26117 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:23:59.814388 26117 solver.cpp:415]     Test net output #0: error_blob = 0.61301 (* 1 = 0.61301 loss)
I1006 18:23:59.871157 26117 solver.cpp:243] Iteration 300, loss = 0.594495
I1006 18:23:59.871183 26117 solver.cpp:259]     Train net output #0: error_blob = 0.594495 (* 1 = 0.594495 loss)
I1006 18:23:59.871188 26117 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:24:04.860544 26117 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:24:04.861982 26117 solver.cpp:415]     Test net output #0: error_blob = 0.622824 (* 1 = 0.622824 loss)
I1006 18:24:04.912824 26117 solver.cpp:243] Iteration 400, loss = 0.593169
I1006 18:24:04.912853 26117 solver.cpp:259]     Train net output #0: error_blob = 0.593169 (* 1 = 0.593169 loss)
I1006 18:24:04.912856 26117 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:24:09.861966 26117 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:24:09.863399 26117 solver.cpp:415]     Test net output #0: error_blob = 0.618894 (* 1 = 0.618894 loss)
I1006 18:24:09.920716 26117 solver.cpp:243] Iteration 500, loss = 0.593624
I1006 18:24:09.920742 26117 solver.cpp:259]     Train net output #0: error_blob = 0.593624 (* 1 = 0.593624 loss)
I1006 18:24:09.920768 26117 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:24:14.879858 26117 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:24:14.881278 26117 solver.cpp:415]     Test net output #0: error_blob = 0.604662 (* 1 = 0.604662 loss)
I1006 18:24:14.934566 26117 solver.cpp:243] Iteration 600, loss = 0.584423
I1006 18:24:14.934604 26117 solver.cpp:259]     Train net output #0: error_blob = 0.584423 (* 1 = 0.584423 loss)
I1006 18:24:14.934609 26117 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:24:20.003239 26117 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:24:20.004647 26117 solver.cpp:415]     Test net output #0: error_blob = 0.609117 (* 1 = 0.609117 loss)
I1006 18:24:20.054121 26117 solver.cpp:243] Iteration 700, loss = 0.584452
I1006 18:24:20.054148 26117 solver.cpp:259]     Train net output #0: error_blob = 0.584452 (* 1 = 0.584452 loss)
I1006 18:24:20.054154 26117 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:24:25.089015 26117 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:24:25.090421 26117 solver.cpp:415]     Test net output #0: error_blob = 0.607326 (* 1 = 0.607326 loss)
I1006 18:24:25.143132 26117 solver.cpp:243] Iteration 800, loss = 0.582148
I1006 18:24:25.143162 26117 solver.cpp:259]     Train net output #0: error_blob = 0.582148 (* 1 = 0.582148 loss)
I1006 18:24:25.143168 26117 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:24:30.205114 26117 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:24:30.206596 26117 solver.cpp:415]     Test net output #0: error_blob = 0.594592 (* 1 = 0.594592 loss)
I1006 18:24:30.259836 26117 solver.cpp:243] Iteration 900, loss = 0.577729
I1006 18:24:30.259863 26117 solver.cpp:259]     Train net output #0: error_blob = 0.577729 (* 1 = 0.577729 loss)
I1006 18:24:30.259868 26117 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:24:35.317311 26117 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:24:35.318584 26117 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:24:35.369087 26117 solver.cpp:327] Iteration 1000, loss = 0.572095
I1006 18:24:35.369123 26117 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:24:35.369603 26117 solver.cpp:415]     Test net output #0: error_blob = 0.601787 (* 1 = 0.601787 loss)
I1006 18:24:35.369613 26117 solver.cpp:332] Optimization Done.
I1006 18:24:35.369626 26117 caffe.cpp:215] Optimization Done.
I1006 18:24:35.509263 26146 caffe.cpp:184] Using GPUs 0
I1006 18:24:36.070634 26146 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part1.prototxt"
I1006 18:24:36.070665 26146 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part1.prototxt
I1006 18:24:36.070829 26146 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:24:36.070874 26146 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part1.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part1.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:24:36.070952 26146 layer_factory.hpp:76] Creating layer data_layer
I1006 18:24:36.097379 26146 net.cpp:110] Creating Layer data_layer
I1006 18:24:36.097415 26146 net.cpp:433] data_layer -> data_blob
I1006 18:24:36.097450 26146 net.cpp:433] data_layer -> label_blob
I1006 18:24:36.098054 26152 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part1.train
I1006 18:24:36.782937 26146 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:24:36.793377 26146 net.cpp:155] Setting up data_layer
I1006 18:24:36.793421 26146 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:24:36.793424 26146 net.cpp:163] Top shape: 40000 (40000)
I1006 18:24:36.793442 26146 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:24:36.793453 26146 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:24:36.793457 26146 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:24:36.793467 26146 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:24:36.793846 26146 net.cpp:155] Setting up hidden_sum_layer
I1006 18:24:36.793854 26146 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:24:36.793875 26146 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:24:36.793892 26146 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:24:36.793895 26146 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:24:36.793898 26146 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:24:40.023052 26146 net.cpp:155] Setting up hidden_act_layer
I1006 18:24:40.023075 26146 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:24:40.023080 26146 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:24:40.023090 26146 net.cpp:110] Creating Layer output_sum_layer
I1006 18:24:40.023093 26146 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:24:40.023098 26146 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:24:40.023228 26146 net.cpp:155] Setting up output_sum_layer
I1006 18:24:40.023234 26146 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:24:40.023241 26146 layer_factory.hpp:76] Creating layer error_layer
I1006 18:24:40.023247 26146 net.cpp:110] Creating Layer error_layer
I1006 18:24:40.023249 26146 net.cpp:477] error_layer <- output_sum_blob
I1006 18:24:40.023252 26146 net.cpp:477] error_layer <- label_blob
I1006 18:24:40.023255 26146 net.cpp:433] error_layer -> error_blob
I1006 18:24:40.023289 26146 net.cpp:155] Setting up error_layer
I1006 18:24:40.023293 26146 net.cpp:163] Top shape: (1)
I1006 18:24:40.023310 26146 net.cpp:168]     with loss weight 1
I1006 18:24:40.023337 26146 net.cpp:236] error_layer needs backward computation.
I1006 18:24:40.023340 26146 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:24:40.023341 26146 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:24:40.023354 26146 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:24:40.023355 26146 net.cpp:240] data_layer does not need backward computation.
I1006 18:24:40.023356 26146 net.cpp:283] This network produces output error_blob
I1006 18:24:40.023361 26146 net.cpp:297] Network initialization done.
I1006 18:24:40.023371 26146 net.cpp:298] Memory required for data: 13280004
I1006 18:24:40.023511 26146 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part1.prototxt
I1006 18:24:40.023522 26146 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:24:40.023561 26146 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part1.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part1.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:24:40.023598 26146 layer_factory.hpp:76] Creating layer data_layer
I1006 18:24:40.025828 26146 net.cpp:110] Creating Layer data_layer
I1006 18:24:40.025833 26146 net.cpp:433] data_layer -> data_blob
I1006 18:24:40.025856 26146 net.cpp:433] data_layer -> label_blob
I1006 18:24:40.026448 26156 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part1.test
I1006 18:24:40.026520 26146 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:24:40.028383 26146 net.cpp:155] Setting up data_layer
I1006 18:24:40.028403 26146 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:24:40.028406 26146 net.cpp:163] Top shape: 4000 (4000)
I1006 18:24:40.028409 26146 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:24:40.028425 26146 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:24:40.028429 26146 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:24:40.028432 26146 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:24:40.028625 26146 net.cpp:155] Setting up hidden_sum_layer
I1006 18:24:40.028632 26146 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:24:40.028640 26146 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:24:40.028645 26146 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:24:40.028647 26146 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:24:40.028650 26146 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:24:40.028728 26146 net.cpp:155] Setting up hidden_act_layer
I1006 18:24:40.028733 26146 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:24:40.028735 26146 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:24:40.028739 26146 net.cpp:110] Creating Layer output_sum_layer
I1006 18:24:40.028743 26146 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:24:40.028745 26146 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:24:40.028807 26146 net.cpp:155] Setting up output_sum_layer
I1006 18:24:40.028812 26146 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:24:40.028831 26146 layer_factory.hpp:76] Creating layer error_layer
I1006 18:24:40.028836 26146 net.cpp:110] Creating Layer error_layer
I1006 18:24:40.028838 26146 net.cpp:477] error_layer <- output_sum_blob
I1006 18:24:40.028841 26146 net.cpp:477] error_layer <- label_blob
I1006 18:24:40.028844 26146 net.cpp:433] error_layer -> error_blob
I1006 18:24:40.028867 26146 net.cpp:155] Setting up error_layer
I1006 18:24:40.028872 26146 net.cpp:163] Top shape: (1)
I1006 18:24:40.028874 26146 net.cpp:168]     with loss weight 1
I1006 18:24:40.028882 26146 net.cpp:236] error_layer needs backward computation.
I1006 18:24:40.028884 26146 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:24:40.028887 26146 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:24:40.028888 26146 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:24:40.028890 26146 net.cpp:240] data_layer does not need backward computation.
I1006 18:24:40.028892 26146 net.cpp:283] This network produces output error_blob
I1006 18:24:40.028897 26146 net.cpp:297] Network initialization done.
I1006 18:24:40.028898 26146 net.cpp:298] Memory required for data: 1328004
I1006 18:24:40.028918 26146 solver.cpp:66] Solver scaffolding done.
I1006 18:24:40.029016 26146 caffe.cpp:212] Starting Optimization
I1006 18:24:40.029023 26146 solver.cpp:294] Solving full_batch_ce/model2_part1.prototxt
I1006 18:24:40.029026 26146 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:24:40.029184 26146 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:24:40.029258 26146 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:24:40.034828 26146 solver.cpp:415]     Test net output #0: error_blob = 0.786877 (* 1 = 0.786877 loss)
I1006 18:24:40.038781 26146 solver.cpp:243] Iteration 0, loss = 0.780404
I1006 18:24:40.038802 26146 solver.cpp:259]     Train net output #0: error_blob = 0.780404 (* 1 = 0.780404 loss)
I1006 18:24:40.038820 26146 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:24:44.786159 26146 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:24:44.787559 26146 solver.cpp:415]     Test net output #0: error_blob = 0.638121 (* 1 = 0.638121 loss)
I1006 18:24:44.836701 26146 solver.cpp:243] Iteration 100, loss = 0.642911
I1006 18:24:44.836735 26146 solver.cpp:259]     Train net output #0: error_blob = 0.642911 (* 1 = 0.642911 loss)
I1006 18:24:44.836742 26146 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:24:49.651859 26146 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:24:49.653257 26146 solver.cpp:415]     Test net output #0: error_blob = 0.623593 (* 1 = 0.623593 loss)
I1006 18:24:49.706182 26146 solver.cpp:243] Iteration 200, loss = 0.627505
I1006 18:24:49.706212 26146 solver.cpp:259]     Train net output #0: error_blob = 0.627505 (* 1 = 0.627505 loss)
I1006 18:24:49.706217 26146 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:24:54.536202 26146 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:24:54.537611 26146 solver.cpp:415]     Test net output #0: error_blob = 0.621197 (* 1 = 0.621197 loss)
I1006 18:24:54.589262 26146 solver.cpp:243] Iteration 300, loss = 0.615205
I1006 18:24:54.589289 26146 solver.cpp:259]     Train net output #0: error_blob = 0.615205 (* 1 = 0.615205 loss)
I1006 18:24:54.589294 26146 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:24:59.487527 26146 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:24:59.488945 26146 solver.cpp:415]     Test net output #0: error_blob = 0.614175 (* 1 = 0.614175 loss)
I1006 18:24:59.539728 26146 solver.cpp:243] Iteration 400, loss = 0.606645
I1006 18:24:59.539762 26146 solver.cpp:259]     Train net output #0: error_blob = 0.606645 (* 1 = 0.606645 loss)
I1006 18:24:59.539769 26146 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:25:04.357870 26146 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:25:04.359263 26146 solver.cpp:415]     Test net output #0: error_blob = 0.611953 (* 1 = 0.611953 loss)
I1006 18:25:04.408386 26146 solver.cpp:243] Iteration 500, loss = 0.602669
I1006 18:25:04.408416 26146 solver.cpp:259]     Train net output #0: error_blob = 0.602669 (* 1 = 0.602669 loss)
I1006 18:25:04.408443 26146 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:25:09.208281 26146 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:25:09.209789 26146 solver.cpp:415]     Test net output #0: error_blob = 0.605477 (* 1 = 0.605477 loss)
I1006 18:25:09.262037 26146 solver.cpp:243] Iteration 600, loss = 0.594923
I1006 18:25:09.262061 26146 solver.cpp:259]     Train net output #0: error_blob = 0.594923 (* 1 = 0.594923 loss)
I1006 18:25:09.262066 26146 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:25:14.051940 26146 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:25:14.053356 26146 solver.cpp:415]     Test net output #0: error_blob = 0.603317 (* 1 = 0.603317 loss)
I1006 18:25:14.104100 26146 solver.cpp:243] Iteration 700, loss = 0.587326
I1006 18:25:14.104132 26146 solver.cpp:259]     Train net output #0: error_blob = 0.587326 (* 1 = 0.587326 loss)
I1006 18:25:14.104140 26146 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:25:18.962918 26146 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:25:18.964310 26146 solver.cpp:415]     Test net output #0: error_blob = 0.61058 (* 1 = 0.61058 loss)
I1006 18:25:19.014317 26146 solver.cpp:243] Iteration 800, loss = 0.591699
I1006 18:25:19.014346 26146 solver.cpp:259]     Train net output #0: error_blob = 0.591699 (* 1 = 0.591699 loss)
I1006 18:25:19.014353 26146 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:25:23.826984 26146 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:25:23.828375 26146 solver.cpp:415]     Test net output #0: error_blob = 0.610861 (* 1 = 0.610861 loss)
I1006 18:25:23.882638 26146 solver.cpp:243] Iteration 900, loss = 0.590346
I1006 18:25:23.882663 26146 solver.cpp:259]     Train net output #0: error_blob = 0.590346 (* 1 = 0.590346 loss)
I1006 18:25:23.882668 26146 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:25:28.810083 26146 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:25:28.811328 26146 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:25:28.857939 26146 solver.cpp:327] Iteration 1000, loss = 0.585182
I1006 18:25:28.857974 26146 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:25:28.858413 26146 solver.cpp:415]     Test net output #0: error_blob = 0.605662 (* 1 = 0.605662 loss)
I1006 18:25:28.858422 26146 solver.cpp:332] Optimization Done.
I1006 18:25:28.858423 26146 caffe.cpp:215] Optimization Done.
I1006 18:25:28.999963 26174 caffe.cpp:184] Using GPUs 0
I1006 18:25:29.562021 26174 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part8.prototxt"
I1006 18:25:29.562050 26174 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part8.prototxt
I1006 18:25:29.562211 26174 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:25:29.562255 26174 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part8.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part8.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:25:29.562330 26174 layer_factory.hpp:76] Creating layer data_layer
I1006 18:25:29.588872 26174 net.cpp:110] Creating Layer data_layer
I1006 18:25:29.588903 26174 net.cpp:433] data_layer -> data_blob
I1006 18:25:29.588935 26174 net.cpp:433] data_layer -> label_blob
I1006 18:25:29.589534 26178 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part8.train
I1006 18:25:30.278307 26174 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:25:30.288650 26174 net.cpp:155] Setting up data_layer
I1006 18:25:30.288691 26174 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:25:30.288697 26174 net.cpp:163] Top shape: 40000 (40000)
I1006 18:25:30.288712 26174 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:25:30.288723 26174 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:25:30.288728 26174 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:25:30.288738 26174 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:25:30.289114 26174 net.cpp:155] Setting up hidden_sum_layer
I1006 18:25:30.289121 26174 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:25:30.289144 26174 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:25:30.289161 26174 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:25:30.289163 26174 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:25:30.289166 26174 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:25:33.511710 26174 net.cpp:155] Setting up hidden_act_layer
I1006 18:25:33.511744 26174 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:25:33.511750 26174 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:25:33.511759 26174 net.cpp:110] Creating Layer output_sum_layer
I1006 18:25:33.511762 26174 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:25:33.511768 26174 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:25:33.511865 26174 net.cpp:155] Setting up output_sum_layer
I1006 18:25:33.511872 26174 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:25:33.511888 26174 layer_factory.hpp:76] Creating layer error_layer
I1006 18:25:33.511896 26174 net.cpp:110] Creating Layer error_layer
I1006 18:25:33.511898 26174 net.cpp:477] error_layer <- output_sum_blob
I1006 18:25:33.511900 26174 net.cpp:477] error_layer <- label_blob
I1006 18:25:33.511904 26174 net.cpp:433] error_layer -> error_blob
I1006 18:25:33.511929 26174 net.cpp:155] Setting up error_layer
I1006 18:25:33.511934 26174 net.cpp:163] Top shape: (1)
I1006 18:25:33.511948 26174 net.cpp:168]     with loss weight 1
I1006 18:25:33.511965 26174 net.cpp:236] error_layer needs backward computation.
I1006 18:25:33.511968 26174 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:25:33.511970 26174 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:25:33.511973 26174 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:25:33.511976 26174 net.cpp:240] data_layer does not need backward computation.
I1006 18:25:33.511977 26174 net.cpp:283] This network produces output error_blob
I1006 18:25:33.511981 26174 net.cpp:297] Network initialization done.
I1006 18:25:33.511983 26174 net.cpp:298] Memory required for data: 13280004
I1006 18:25:33.512109 26174 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part8.prototxt
I1006 18:25:33.512140 26174 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:25:33.512189 26174 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part8.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part8.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:25:33.512226 26174 layer_factory.hpp:76] Creating layer data_layer
I1006 18:25:33.514744 26174 net.cpp:110] Creating Layer data_layer
I1006 18:25:33.514755 26174 net.cpp:433] data_layer -> data_blob
I1006 18:25:33.514763 26174 net.cpp:433] data_layer -> label_blob
I1006 18:25:33.515326 26184 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part8.test
I1006 18:25:33.515393 26174 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:25:33.517427 26174 net.cpp:155] Setting up data_layer
I1006 18:25:33.517442 26174 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:25:33.517447 26174 net.cpp:163] Top shape: 4000 (4000)
I1006 18:25:33.517453 26174 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:25:33.517463 26174 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:25:33.517468 26174 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:25:33.517482 26174 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:25:33.517580 26174 net.cpp:155] Setting up hidden_sum_layer
I1006 18:25:33.517590 26174 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:25:33.517601 26174 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:25:33.517611 26174 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:25:33.517616 26174 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:25:33.517622 26174 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:25:33.517698 26174 net.cpp:155] Setting up hidden_act_layer
I1006 18:25:33.517704 26174 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:25:33.517709 26174 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:25:33.517716 26174 net.cpp:110] Creating Layer output_sum_layer
I1006 18:25:33.517720 26174 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:25:33.517725 26174 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:25:33.517791 26174 net.cpp:155] Setting up output_sum_layer
I1006 18:25:33.517798 26174 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:25:33.517818 26174 layer_factory.hpp:76] Creating layer error_layer
I1006 18:25:33.517828 26174 net.cpp:110] Creating Layer error_layer
I1006 18:25:33.517834 26174 net.cpp:477] error_layer <- output_sum_blob
I1006 18:25:33.517839 26174 net.cpp:477] error_layer <- label_blob
I1006 18:25:33.517845 26174 net.cpp:433] error_layer -> error_blob
I1006 18:25:33.517874 26174 net.cpp:155] Setting up error_layer
I1006 18:25:33.517879 26174 net.cpp:163] Top shape: (1)
I1006 18:25:33.517882 26174 net.cpp:168]     with loss weight 1
I1006 18:25:33.517894 26174 net.cpp:236] error_layer needs backward computation.
I1006 18:25:33.517899 26174 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:25:33.517904 26174 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:25:33.517906 26174 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:25:33.517910 26174 net.cpp:240] data_layer does not need backward computation.
I1006 18:25:33.517913 26174 net.cpp:283] This network produces output error_blob
I1006 18:25:33.517922 26174 net.cpp:297] Network initialization done.
I1006 18:25:33.517926 26174 net.cpp:298] Memory required for data: 1328004
I1006 18:25:33.517949 26174 solver.cpp:66] Solver scaffolding done.
I1006 18:25:33.518041 26174 caffe.cpp:212] Starting Optimization
I1006 18:25:33.518049 26174 solver.cpp:294] Solving full_batch_ce/model2_part8.prototxt
I1006 18:25:33.518054 26174 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:25:33.518242 26174 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:25:33.518379 26174 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:25:33.528141 26174 solver.cpp:415]     Test net output #0: error_blob = 0.783019 (* 1 = 0.783019 loss)
I1006 18:25:33.531831 26174 solver.cpp:243] Iteration 0, loss = 0.786971
I1006 18:25:33.531851 26174 solver.cpp:259]     Train net output #0: error_blob = 0.786971 (* 1 = 0.786971 loss)
I1006 18:25:33.531862 26174 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:25:38.388507 26174 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:25:38.389878 26174 solver.cpp:415]     Test net output #0: error_blob = 0.653394 (* 1 = 0.653394 loss)
I1006 18:25:38.443408 26174 solver.cpp:243] Iteration 100, loss = 0.623471
I1006 18:25:38.443454 26174 solver.cpp:259]     Train net output #0: error_blob = 0.623471 (* 1 = 0.623471 loss)
I1006 18:25:38.443461 26174 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:25:43.315393 26174 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:25:43.316797 26174 solver.cpp:415]     Test net output #0: error_blob = 0.648208 (* 1 = 0.648208 loss)
I1006 18:25:43.368880 26174 solver.cpp:243] Iteration 200, loss = 0.61221
I1006 18:25:43.368909 26174 solver.cpp:259]     Train net output #0: error_blob = 0.61221 (* 1 = 0.61221 loss)
I1006 18:25:43.368916 26174 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:25:48.300925 26174 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:25:48.302367 26174 solver.cpp:415]     Test net output #0: error_blob = 0.64229 (* 1 = 0.64229 loss)
I1006 18:25:48.352614 26174 solver.cpp:243] Iteration 300, loss = 0.600421
I1006 18:25:48.352649 26174 solver.cpp:259]     Train net output #0: error_blob = 0.600421 (* 1 = 0.600421 loss)
I1006 18:25:48.352656 26174 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:25:53.171648 26174 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:25:53.173053 26174 solver.cpp:415]     Test net output #0: error_blob = 0.637709 (* 1 = 0.637709 loss)
I1006 18:25:53.224584 26174 solver.cpp:243] Iteration 400, loss = 0.596271
I1006 18:25:53.224616 26174 solver.cpp:259]     Train net output #0: error_blob = 0.596271 (* 1 = 0.596271 loss)
I1006 18:25:53.224620 26174 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:25:58.173549 26174 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:25:58.174959 26174 solver.cpp:415]     Test net output #0: error_blob = 0.631093 (* 1 = 0.631093 loss)
I1006 18:25:58.228744 26174 solver.cpp:243] Iteration 500, loss = 0.591634
I1006 18:25:58.228780 26174 solver.cpp:259]     Train net output #0: error_blob = 0.591634 (* 1 = 0.591634 loss)
I1006 18:25:58.228813 26174 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:26:03.138283 26174 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:26:03.139694 26174 solver.cpp:415]     Test net output #0: error_blob = 0.625511 (* 1 = 0.625511 loss)
I1006 18:26:03.193209 26174 solver.cpp:243] Iteration 600, loss = 0.588466
I1006 18:26:03.193239 26174 solver.cpp:259]     Train net output #0: error_blob = 0.588466 (* 1 = 0.588466 loss)
I1006 18:26:03.193246 26174 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:26:08.011672 26174 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:26:08.013113 26174 solver.cpp:415]     Test net output #0: error_blob = 0.620253 (* 1 = 0.620253 loss)
I1006 18:26:08.063791 26174 solver.cpp:243] Iteration 700, loss = 0.590428
I1006 18:26:08.063835 26174 solver.cpp:259]     Train net output #0: error_blob = 0.590428 (* 1 = 0.590428 loss)
I1006 18:26:08.063843 26174 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:26:13.014627 26174 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:26:13.016067 26174 solver.cpp:415]     Test net output #0: error_blob = 0.613921 (* 1 = 0.613921 loss)
I1006 18:26:13.068737 26174 solver.cpp:243] Iteration 800, loss = 0.582097
I1006 18:26:13.068780 26174 solver.cpp:259]     Train net output #0: error_blob = 0.582097 (* 1 = 0.582097 loss)
I1006 18:26:13.068789 26174 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:26:17.943399 26174 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:26:17.944839 26174 solver.cpp:415]     Test net output #0: error_blob = 0.612816 (* 1 = 0.612816 loss)
I1006 18:26:17.996031 26174 solver.cpp:243] Iteration 900, loss = 0.580574
I1006 18:26:17.996069 26174 solver.cpp:259]     Train net output #0: error_blob = 0.580574 (* 1 = 0.580574 loss)
I1006 18:26:17.996074 26174 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:26:22.815773 26174 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:26:22.817035 26174 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:26:22.863742 26174 solver.cpp:327] Iteration 1000, loss = 0.577978
I1006 18:26:22.863780 26174 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:26:22.864269 26174 solver.cpp:415]     Test net output #0: error_blob = 0.610719 (* 1 = 0.610719 loss)
I1006 18:26:22.864282 26174 solver.cpp:332] Optimization Done.
I1006 18:26:22.864289 26174 caffe.cpp:215] Optimization Done.
I1006 18:26:22.995573 26205 caffe.cpp:184] Using GPUs 0
I1006 18:26:23.555711 26205 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part4.prototxt"
I1006 18:26:23.555743 26205 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part4.prototxt
I1006 18:26:23.555907 26205 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:26:23.555953 26205 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part4.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part4.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:26:23.556020 26205 layer_factory.hpp:76] Creating layer data_layer
I1006 18:26:23.582340 26205 net.cpp:110] Creating Layer data_layer
I1006 18:26:23.582360 26205 net.cpp:433] data_layer -> data_blob
I1006 18:26:23.582381 26205 net.cpp:433] data_layer -> label_blob
I1006 18:26:23.583009 26209 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part4.train
I1006 18:26:24.267452 26205 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:26:24.277994 26205 net.cpp:155] Setting up data_layer
I1006 18:26:24.278034 26205 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:26:24.278038 26205 net.cpp:163] Top shape: 40000 (40000)
I1006 18:26:24.278054 26205 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:26:24.278067 26205 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:26:24.278070 26205 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:26:24.278080 26205 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:26:24.278481 26205 net.cpp:155] Setting up hidden_sum_layer
I1006 18:26:24.278488 26205 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:26:24.278511 26205 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:26:24.278527 26205 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:26:24.278530 26205 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:26:24.278533 26205 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:26:27.500921 26205 net.cpp:155] Setting up hidden_act_layer
I1006 18:26:27.500943 26205 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:26:27.500948 26205 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:26:27.500957 26205 net.cpp:110] Creating Layer output_sum_layer
I1006 18:26:27.500960 26205 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:26:27.500967 26205 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:26:27.501075 26205 net.cpp:155] Setting up output_sum_layer
I1006 18:26:27.501080 26205 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:26:27.501085 26205 layer_factory.hpp:76] Creating layer error_layer
I1006 18:26:27.501092 26205 net.cpp:110] Creating Layer error_layer
I1006 18:26:27.501094 26205 net.cpp:477] error_layer <- output_sum_blob
I1006 18:26:27.501097 26205 net.cpp:477] error_layer <- label_blob
I1006 18:26:27.501101 26205 net.cpp:433] error_layer -> error_blob
I1006 18:26:27.501135 26205 net.cpp:155] Setting up error_layer
I1006 18:26:27.501140 26205 net.cpp:163] Top shape: (1)
I1006 18:26:27.501165 26205 net.cpp:168]     with loss weight 1
I1006 18:26:27.501191 26205 net.cpp:236] error_layer needs backward computation.
I1006 18:26:27.501204 26205 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:26:27.501205 26205 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:26:27.501207 26205 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:26:27.501209 26205 net.cpp:240] data_layer does not need backward computation.
I1006 18:26:27.501221 26205 net.cpp:283] This network produces output error_blob
I1006 18:26:27.501225 26205 net.cpp:297] Network initialization done.
I1006 18:26:27.501227 26205 net.cpp:298] Memory required for data: 13280004
I1006 18:26:27.501351 26205 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part4.prototxt
I1006 18:26:27.501380 26205 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:26:27.501423 26205 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part4.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part4.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:26:27.501441 26205 layer_factory.hpp:76] Creating layer data_layer
I1006 18:26:27.503676 26205 net.cpp:110] Creating Layer data_layer
I1006 18:26:27.503682 26205 net.cpp:433] data_layer -> data_blob
I1006 18:26:27.503703 26205 net.cpp:433] data_layer -> label_blob
I1006 18:26:27.504294 26211 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part4.test
I1006 18:26:27.504359 26205 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:26:27.506278 26205 net.cpp:155] Setting up data_layer
I1006 18:26:27.506290 26205 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:26:27.506294 26205 net.cpp:163] Top shape: 4000 (4000)
I1006 18:26:27.506297 26205 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:26:27.506315 26205 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:26:27.506319 26205 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:26:27.506322 26205 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:26:27.506469 26205 net.cpp:155] Setting up hidden_sum_layer
I1006 18:26:27.506474 26205 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:26:27.506480 26205 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:26:27.506495 26205 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:26:27.506497 26205 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:26:27.506500 26205 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:26:27.506611 26205 net.cpp:155] Setting up hidden_act_layer
I1006 18:26:27.506615 26205 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:26:27.506618 26205 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:26:27.506621 26205 net.cpp:110] Creating Layer output_sum_layer
I1006 18:26:27.506623 26205 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:26:27.506638 26205 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:26:27.506703 26205 net.cpp:155] Setting up output_sum_layer
I1006 18:26:27.506708 26205 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:26:27.506736 26205 layer_factory.hpp:76] Creating layer error_layer
I1006 18:26:27.506742 26205 net.cpp:110] Creating Layer error_layer
I1006 18:26:27.506742 26205 net.cpp:477] error_layer <- output_sum_blob
I1006 18:26:27.506746 26205 net.cpp:477] error_layer <- label_blob
I1006 18:26:27.506748 26205 net.cpp:433] error_layer -> error_blob
I1006 18:26:27.506770 26205 net.cpp:155] Setting up error_layer
I1006 18:26:27.506774 26205 net.cpp:163] Top shape: (1)
I1006 18:26:27.506785 26205 net.cpp:168]     with loss weight 1
I1006 18:26:27.506793 26205 net.cpp:236] error_layer needs backward computation.
I1006 18:26:27.506805 26205 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:26:27.506808 26205 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:26:27.506809 26205 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:26:27.506811 26205 net.cpp:240] data_layer does not need backward computation.
I1006 18:26:27.506814 26205 net.cpp:283] This network produces output error_blob
I1006 18:26:27.506817 26205 net.cpp:297] Network initialization done.
I1006 18:26:27.506819 26205 net.cpp:298] Memory required for data: 1328004
I1006 18:26:27.506837 26205 solver.cpp:66] Solver scaffolding done.
I1006 18:26:27.506933 26205 caffe.cpp:212] Starting Optimization
I1006 18:26:27.506939 26205 solver.cpp:294] Solving full_batch_ce/model2_part4.prototxt
I1006 18:26:27.506940 26205 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:26:27.507100 26205 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:26:27.507167 26205 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:26:27.512130 26205 solver.cpp:415]     Test net output #0: error_blob = 0.697872 (* 1 = 0.697872 loss)
I1006 18:26:27.515748 26205 solver.cpp:243] Iteration 0, loss = 0.713891
I1006 18:26:27.515766 26205 solver.cpp:259]     Train net output #0: error_blob = 0.713891 (* 1 = 0.713891 loss)
I1006 18:26:27.515776 26205 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:26:32.349891 26205 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:26:32.351274 26205 solver.cpp:415]     Test net output #0: error_blob = 0.620768 (* 1 = 0.620768 loss)
I1006 18:26:32.406204 26205 solver.cpp:243] Iteration 100, loss = 0.63171
I1006 18:26:32.406235 26205 solver.cpp:259]     Train net output #0: error_blob = 0.63171 (* 1 = 0.63171 loss)
I1006 18:26:32.406242 26205 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:26:37.387315 26205 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:26:37.388689 26205 solver.cpp:415]     Test net output #0: error_blob = 0.606693 (* 1 = 0.606693 loss)
I1006 18:26:37.441728 26205 solver.cpp:243] Iteration 200, loss = 0.615867
I1006 18:26:37.441759 26205 solver.cpp:259]     Train net output #0: error_blob = 0.615867 (* 1 = 0.615867 loss)
I1006 18:26:37.441766 26205 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:26:42.397357 26205 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:26:42.398815 26205 solver.cpp:415]     Test net output #0: error_blob = 0.597747 (* 1 = 0.597747 loss)
I1006 18:26:42.450238 26205 solver.cpp:243] Iteration 300, loss = 0.604972
I1006 18:26:42.450265 26205 solver.cpp:259]     Train net output #0: error_blob = 0.604972 (* 1 = 0.604972 loss)
I1006 18:26:42.450270 26205 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:26:47.353907 26205 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:26:47.355356 26205 solver.cpp:415]     Test net output #0: error_blob = 0.592284 (* 1 = 0.592284 loss)
I1006 18:26:47.407801 26205 solver.cpp:243] Iteration 400, loss = 0.601019
I1006 18:26:47.407840 26205 solver.cpp:259]     Train net output #0: error_blob = 0.601019 (* 1 = 0.601019 loss)
I1006 18:26:47.407845 26205 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:26:52.326786 26205 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:26:52.328172 26205 solver.cpp:415]     Test net output #0: error_blob = 0.588598 (* 1 = 0.588598 loss)
I1006 18:26:52.379833 26205 solver.cpp:243] Iteration 500, loss = 0.599948
I1006 18:26:52.379861 26205 solver.cpp:259]     Train net output #0: error_blob = 0.599948 (* 1 = 0.599948 loss)
I1006 18:26:52.379887 26205 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:26:57.330106 26205 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:26:57.331598 26205 solver.cpp:415]     Test net output #0: error_blob = 0.586061 (* 1 = 0.586061 loss)
I1006 18:26:57.382936 26205 solver.cpp:243] Iteration 600, loss = 0.596344
I1006 18:26:57.382963 26205 solver.cpp:259]     Train net output #0: error_blob = 0.596344 (* 1 = 0.596344 loss)
I1006 18:26:57.382968 26205 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:27:02.317081 26205 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:27:02.318547 26205 solver.cpp:415]     Test net output #0: error_blob = 0.584119 (* 1 = 0.584119 loss)
I1006 18:27:02.371929 26205 solver.cpp:243] Iteration 700, loss = 0.594537
I1006 18:27:02.371968 26205 solver.cpp:259]     Train net output #0: error_blob = 0.594537 (* 1 = 0.594537 loss)
I1006 18:27:02.371973 26205 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:27:07.381881 26205 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:27:07.383317 26205 solver.cpp:415]     Test net output #0: error_blob = 0.582193 (* 1 = 0.582193 loss)
I1006 18:27:07.436600 26205 solver.cpp:243] Iteration 800, loss = 0.589435
I1006 18:27:07.436645 26205 solver.cpp:259]     Train net output #0: error_blob = 0.589435 (* 1 = 0.589435 loss)
I1006 18:27:07.436651 26205 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:27:12.395020 26205 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:27:12.396458 26205 solver.cpp:415]     Test net output #0: error_blob = 0.580377 (* 1 = 0.580377 loss)
I1006 18:27:12.446672 26205 solver.cpp:243] Iteration 900, loss = 0.587717
I1006 18:27:12.446702 26205 solver.cpp:259]     Train net output #0: error_blob = 0.587717 (* 1 = 0.587717 loss)
I1006 18:27:12.446707 26205 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:27:17.331449 26205 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:27:17.332708 26205 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:27:17.382601 26205 solver.cpp:327] Iteration 1000, loss = 0.58282
I1006 18:27:17.382643 26205 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:27:17.383087 26205 solver.cpp:415]     Test net output #0: error_blob = 0.578399 (* 1 = 0.578399 loss)
I1006 18:27:17.383095 26205 solver.cpp:332] Optimization Done.
I1006 18:27:17.383098 26205 caffe.cpp:215] Optimization Done.
I1006 18:27:17.509539 26232 caffe.cpp:184] Using GPUs 0
I1006 18:27:18.069767 26232 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part7.prototxt"
I1006 18:27:18.069797 26232 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part7.prototxt
I1006 18:27:18.069959 26232 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:27:18.070003 26232 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part7.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part7.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:27:18.070039 26232 layer_factory.hpp:76] Creating layer data_layer
I1006 18:27:18.096657 26232 net.cpp:110] Creating Layer data_layer
I1006 18:27:18.096688 26232 net.cpp:433] data_layer -> data_blob
I1006 18:27:18.096721 26232 net.cpp:433] data_layer -> label_blob
I1006 18:27:18.097319 26238 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part7.train
I1006 18:27:18.778399 26232 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:27:18.788836 26232 net.cpp:155] Setting up data_layer
I1006 18:27:18.788878 26232 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:27:18.788882 26232 net.cpp:163] Top shape: 40000 (40000)
I1006 18:27:18.788898 26232 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:27:18.788911 26232 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:27:18.788915 26232 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:27:18.788925 26232 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:27:18.789314 26232 net.cpp:155] Setting up hidden_sum_layer
I1006 18:27:18.789320 26232 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:27:18.789342 26232 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:27:18.789361 26232 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:27:18.789362 26232 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:27:18.789366 26232 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:27:22.015270 26232 net.cpp:155] Setting up hidden_act_layer
I1006 18:27:22.015293 26232 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:27:22.015297 26232 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:27:22.015308 26232 net.cpp:110] Creating Layer output_sum_layer
I1006 18:27:22.015312 26232 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:27:22.015317 26232 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:27:22.015420 26232 net.cpp:155] Setting up output_sum_layer
I1006 18:27:22.015435 26232 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:27:22.015452 26232 layer_factory.hpp:76] Creating layer error_layer
I1006 18:27:22.015460 26232 net.cpp:110] Creating Layer error_layer
I1006 18:27:22.015461 26232 net.cpp:477] error_layer <- output_sum_blob
I1006 18:27:22.015465 26232 net.cpp:477] error_layer <- label_blob
I1006 18:27:22.015468 26232 net.cpp:433] error_layer -> error_blob
I1006 18:27:22.015496 26232 net.cpp:155] Setting up error_layer
I1006 18:27:22.015509 26232 net.cpp:163] Top shape: (1)
I1006 18:27:22.015534 26232 net.cpp:168]     with loss weight 1
I1006 18:27:22.015560 26232 net.cpp:236] error_layer needs backward computation.
I1006 18:27:22.015563 26232 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:27:22.015565 26232 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:27:22.015578 26232 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:27:22.015579 26232 net.cpp:240] data_layer does not need backward computation.
I1006 18:27:22.015581 26232 net.cpp:283] This network produces output error_blob
I1006 18:27:22.015586 26232 net.cpp:297] Network initialization done.
I1006 18:27:22.015588 26232 net.cpp:298] Memory required for data: 13280004
I1006 18:27:22.015708 26232 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part7.prototxt
I1006 18:27:22.015720 26232 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:27:22.015750 26232 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part7.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part7.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:27:22.015770 26232 layer_factory.hpp:76] Creating layer data_layer
I1006 18:27:22.018335 26232 net.cpp:110] Creating Layer data_layer
I1006 18:27:22.018342 26232 net.cpp:433] data_layer -> data_blob
I1006 18:27:22.018347 26232 net.cpp:433] data_layer -> label_blob
I1006 18:27:22.018952 26240 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part7.test
I1006 18:27:22.019029 26232 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:27:22.021016 26232 net.cpp:155] Setting up data_layer
I1006 18:27:22.021031 26232 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:27:22.021035 26232 net.cpp:163] Top shape: 4000 (4000)
I1006 18:27:22.021039 26232 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:27:22.021049 26232 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:27:22.021051 26232 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:27:22.021056 26232 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:27:22.021208 26232 net.cpp:155] Setting up hidden_sum_layer
I1006 18:27:22.021214 26232 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:27:22.021220 26232 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:27:22.021226 26232 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:27:22.021229 26232 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:27:22.021232 26232 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:27:22.021311 26232 net.cpp:155] Setting up hidden_act_layer
I1006 18:27:22.021317 26232 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:27:22.021318 26232 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:27:22.021323 26232 net.cpp:110] Creating Layer output_sum_layer
I1006 18:27:22.021325 26232 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:27:22.021328 26232 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:27:22.021394 26232 net.cpp:155] Setting up output_sum_layer
I1006 18:27:22.021400 26232 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:27:22.021417 26232 layer_factory.hpp:76] Creating layer error_layer
I1006 18:27:22.021423 26232 net.cpp:110] Creating Layer error_layer
I1006 18:27:22.021425 26232 net.cpp:477] error_layer <- output_sum_blob
I1006 18:27:22.021428 26232 net.cpp:477] error_layer <- label_blob
I1006 18:27:22.021431 26232 net.cpp:433] error_layer -> error_blob
I1006 18:27:22.021456 26232 net.cpp:155] Setting up error_layer
I1006 18:27:22.021461 26232 net.cpp:163] Top shape: (1)
I1006 18:27:22.021463 26232 net.cpp:168]     with loss weight 1
I1006 18:27:22.021472 26232 net.cpp:236] error_layer needs backward computation.
I1006 18:27:22.021474 26232 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:27:22.021477 26232 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:27:22.021479 26232 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:27:22.021481 26232 net.cpp:240] data_layer does not need backward computation.
I1006 18:27:22.021483 26232 net.cpp:283] This network produces output error_blob
I1006 18:27:22.021488 26232 net.cpp:297] Network initialization done.
I1006 18:27:22.021491 26232 net.cpp:298] Memory required for data: 1328004
I1006 18:27:22.021509 26232 solver.cpp:66] Solver scaffolding done.
I1006 18:27:22.021612 26232 caffe.cpp:212] Starting Optimization
I1006 18:27:22.021620 26232 solver.cpp:294] Solving full_batch_ce/model2_part7.prototxt
I1006 18:27:22.021621 26232 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:27:22.021772 26232 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:27:22.021826 26232 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:27:22.032295 26232 solver.cpp:415]     Test net output #0: error_blob = 0.695073 (* 1 = 0.695073 loss)
I1006 18:27:22.035976 26232 solver.cpp:243] Iteration 0, loss = 0.698792
I1006 18:27:22.036000 26232 solver.cpp:259]     Train net output #0: error_blob = 0.698792 (* 1 = 0.698792 loss)
I1006 18:27:22.036010 26232 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:27:26.880175 26232 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:27:26.881603 26232 solver.cpp:415]     Test net output #0: error_blob = 0.640767 (* 1 = 0.640767 loss)
I1006 18:27:26.933744 26232 solver.cpp:243] Iteration 100, loss = 0.634116
I1006 18:27:26.933780 26232 solver.cpp:259]     Train net output #0: error_blob = 0.634116 (* 1 = 0.634116 loss)
I1006 18:27:26.933789 26232 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:27:31.855917 26232 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:27:31.857364 26232 solver.cpp:415]     Test net output #0: error_blob = 0.630772 (* 1 = 0.630772 loss)
I1006 18:27:31.910404 26232 solver.cpp:243] Iteration 200, loss = 0.618703
I1006 18:27:31.910434 26232 solver.cpp:259]     Train net output #0: error_blob = 0.618703 (* 1 = 0.618703 loss)
I1006 18:27:31.910439 26232 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:27:36.758131 26232 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:27:36.759552 26232 solver.cpp:415]     Test net output #0: error_blob = 0.627713 (* 1 = 0.627713 loss)
I1006 18:27:36.808080 26232 solver.cpp:243] Iteration 300, loss = 0.604219
I1006 18:27:36.808111 26232 solver.cpp:259]     Train net output #0: error_blob = 0.604219 (* 1 = 0.604219 loss)
I1006 18:27:36.808117 26232 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:27:41.724568 26232 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:27:41.725976 26232 solver.cpp:415]     Test net output #0: error_blob = 0.625125 (* 1 = 0.625125 loss)
I1006 18:27:41.777994 26232 solver.cpp:243] Iteration 400, loss = 0.601248
I1006 18:27:41.778025 26232 solver.cpp:259]     Train net output #0: error_blob = 0.601248 (* 1 = 0.601248 loss)
I1006 18:27:41.778031 26232 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:27:46.645458 26232 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:27:46.646855 26232 solver.cpp:415]     Test net output #0: error_blob = 0.622013 (* 1 = 0.622013 loss)
I1006 18:27:46.695494 26232 solver.cpp:243] Iteration 500, loss = 0.593335
I1006 18:27:46.695526 26232 solver.cpp:259]     Train net output #0: error_blob = 0.593335 (* 1 = 0.593335 loss)
I1006 18:27:46.695557 26232 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:27:51.570638 26232 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:27:51.572100 26232 solver.cpp:415]     Test net output #0: error_blob = 0.619266 (* 1 = 0.619266 loss)
I1006 18:27:51.624459 26232 solver.cpp:243] Iteration 600, loss = 0.594102
I1006 18:27:51.624501 26232 solver.cpp:259]     Train net output #0: error_blob = 0.594102 (* 1 = 0.594102 loss)
I1006 18:27:51.624507 26232 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:27:56.495172 26232 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:27:56.496644 26232 solver.cpp:415]     Test net output #0: error_blob = 0.617451 (* 1 = 0.617451 loss)
I1006 18:27:56.548909 26232 solver.cpp:243] Iteration 700, loss = 0.586548
I1006 18:27:56.548939 26232 solver.cpp:259]     Train net output #0: error_blob = 0.586548 (* 1 = 0.586548 loss)
I1006 18:27:56.548944 26232 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:28:01.400157 26232 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:28:01.401577 26232 solver.cpp:415]     Test net output #0: error_blob = 0.614868 (* 1 = 0.614868 loss)
I1006 18:28:01.453474 26232 solver.cpp:243] Iteration 800, loss = 0.589798
I1006 18:28:01.453505 26232 solver.cpp:259]     Train net output #0: error_blob = 0.589798 (* 1 = 0.589798 loss)
I1006 18:28:01.453511 26232 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:28:06.305197 26232 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:28:06.306602 26232 solver.cpp:415]     Test net output #0: error_blob = 0.61258 (* 1 = 0.61258 loss)
I1006 18:28:06.355747 26232 solver.cpp:243] Iteration 900, loss = 0.58235
I1006 18:28:06.355779 26232 solver.cpp:259]     Train net output #0: error_blob = 0.58235 (* 1 = 0.58235 loss)
I1006 18:28:06.355785 26232 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:28:11.221370 26232 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:28:11.222654 26232 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:28:11.270668 26232 solver.cpp:327] Iteration 1000, loss = 0.585825
I1006 18:28:11.270705 26232 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:28:11.271204 26232 solver.cpp:415]     Test net output #0: error_blob = 0.609413 (* 1 = 0.609413 loss)
I1006 18:28:11.271219 26232 solver.cpp:332] Optimization Done.
I1006 18:28:11.271232 26232 caffe.cpp:215] Optimization Done.
I1006 18:28:11.403120 26261 caffe.cpp:184] Using GPUs 0
I1006 18:28:11.961504 26261 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part3.prototxt"
I1006 18:28:11.961534 26261 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part3.prototxt
I1006 18:28:11.961699 26261 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:28:11.961743 26261 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part3.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part3.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:28:11.961808 26261 layer_factory.hpp:76] Creating layer data_layer
I1006 18:28:11.988103 26261 net.cpp:110] Creating Layer data_layer
I1006 18:28:11.988133 26261 net.cpp:433] data_layer -> data_blob
I1006 18:28:11.988167 26261 net.cpp:433] data_layer -> label_blob
I1006 18:28:11.988780 26266 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part3.train
I1006 18:28:12.672054 26261 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:28:12.682514 26261 net.cpp:155] Setting up data_layer
I1006 18:28:12.682576 26261 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:28:12.682581 26261 net.cpp:163] Top shape: 40000 (40000)
I1006 18:28:12.682587 26261 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:28:12.682600 26261 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:28:12.682603 26261 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:28:12.682613 26261 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:28:12.682988 26261 net.cpp:155] Setting up hidden_sum_layer
I1006 18:28:12.682996 26261 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:28:12.683017 26261 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:28:12.683034 26261 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:28:12.683037 26261 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:28:12.683040 26261 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:28:15.902389 26261 net.cpp:155] Setting up hidden_act_layer
I1006 18:28:15.902411 26261 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:28:15.902416 26261 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:28:15.902426 26261 net.cpp:110] Creating Layer output_sum_layer
I1006 18:28:15.902428 26261 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:28:15.902433 26261 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:28:15.902539 26261 net.cpp:155] Setting up output_sum_layer
I1006 18:28:15.902544 26261 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:28:15.902561 26261 layer_factory.hpp:76] Creating layer error_layer
I1006 18:28:15.902577 26261 net.cpp:110] Creating Layer error_layer
I1006 18:28:15.902580 26261 net.cpp:477] error_layer <- output_sum_blob
I1006 18:28:15.902581 26261 net.cpp:477] error_layer <- label_blob
I1006 18:28:15.902585 26261 net.cpp:433] error_layer -> error_blob
I1006 18:28:15.902619 26261 net.cpp:155] Setting up error_layer
I1006 18:28:15.902632 26261 net.cpp:163] Top shape: (1)
I1006 18:28:15.902658 26261 net.cpp:168]     with loss weight 1
I1006 18:28:15.902674 26261 net.cpp:236] error_layer needs backward computation.
I1006 18:28:15.902676 26261 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:28:15.902678 26261 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:28:15.902680 26261 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:28:15.902683 26261 net.cpp:240] data_layer does not need backward computation.
I1006 18:28:15.902684 26261 net.cpp:283] This network produces output error_blob
I1006 18:28:15.902689 26261 net.cpp:297] Network initialization done.
I1006 18:28:15.902690 26261 net.cpp:298] Memory required for data: 13280004
I1006 18:28:15.902811 26261 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part3.prototxt
I1006 18:28:15.902832 26261 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:28:15.902860 26261 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part3.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part3.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:28:15.902897 26261 layer_factory.hpp:76] Creating layer data_layer
I1006 18:28:15.905181 26261 net.cpp:110] Creating Layer data_layer
I1006 18:28:15.905196 26261 net.cpp:433] data_layer -> data_blob
I1006 18:28:15.905211 26261 net.cpp:433] data_layer -> label_blob
I1006 18:28:15.905781 26270 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part3.test
I1006 18:28:15.905874 26261 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:28:15.907729 26261 net.cpp:155] Setting up data_layer
I1006 18:28:15.907760 26261 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:28:15.907763 26261 net.cpp:163] Top shape: 4000 (4000)
I1006 18:28:15.907766 26261 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:28:15.907773 26261 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:28:15.907776 26261 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:28:15.907779 26261 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:28:15.907923 26261 net.cpp:155] Setting up hidden_sum_layer
I1006 18:28:15.907928 26261 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:28:15.907944 26261 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:28:15.907949 26261 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:28:15.907951 26261 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:28:15.907954 26261 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:28:15.908032 26261 net.cpp:155] Setting up hidden_act_layer
I1006 18:28:15.908036 26261 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:28:15.908049 26261 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:28:15.908053 26261 net.cpp:110] Creating Layer output_sum_layer
I1006 18:28:15.908054 26261 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:28:15.908058 26261 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:28:15.908121 26261 net.cpp:155] Setting up output_sum_layer
I1006 18:28:15.908126 26261 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:28:15.908151 26261 layer_factory.hpp:76] Creating layer error_layer
I1006 18:28:15.908156 26261 net.cpp:110] Creating Layer error_layer
I1006 18:28:15.908159 26261 net.cpp:477] error_layer <- output_sum_blob
I1006 18:28:15.908160 26261 net.cpp:477] error_layer <- label_blob
I1006 18:28:15.908164 26261 net.cpp:433] error_layer -> error_blob
I1006 18:28:15.908185 26261 net.cpp:155] Setting up error_layer
I1006 18:28:15.908190 26261 net.cpp:163] Top shape: (1)
I1006 18:28:15.908190 26261 net.cpp:168]     with loss weight 1
I1006 18:28:15.908207 26261 net.cpp:236] error_layer needs backward computation.
I1006 18:28:15.908220 26261 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:28:15.908221 26261 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:28:15.908223 26261 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:28:15.908226 26261 net.cpp:240] data_layer does not need backward computation.
I1006 18:28:15.908227 26261 net.cpp:283] This network produces output error_blob
I1006 18:28:15.908231 26261 net.cpp:297] Network initialization done.
I1006 18:28:15.908233 26261 net.cpp:298] Memory required for data: 1328004
I1006 18:28:15.908252 26261 solver.cpp:66] Solver scaffolding done.
I1006 18:28:15.908346 26261 caffe.cpp:212] Starting Optimization
I1006 18:28:15.908352 26261 solver.cpp:294] Solving full_batch_ce/model2_part3.prototxt
I1006 18:28:15.908365 26261 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:28:15.908530 26261 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:28:15.908614 26261 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:28:15.914131 26261 solver.cpp:415]     Test net output #0: error_blob = 0.708475 (* 1 = 0.708475 loss)
I1006 18:28:15.917992 26261 solver.cpp:243] Iteration 0, loss = 0.726418
I1006 18:28:15.918015 26261 solver.cpp:259]     Train net output #0: error_blob = 0.726418 (* 1 = 0.726418 loss)
I1006 18:28:15.918023 26261 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:28:20.741281 26261 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:28:20.742676 26261 solver.cpp:415]     Test net output #0: error_blob = 0.598831 (* 1 = 0.598831 loss)
I1006 18:28:20.795377 26261 solver.cpp:243] Iteration 100, loss = 0.631252
I1006 18:28:20.795424 26261 solver.cpp:259]     Train net output #0: error_blob = 0.631252 (* 1 = 0.631252 loss)
I1006 18:28:20.795433 26261 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:28:25.702607 26261 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:28:25.703989 26261 solver.cpp:415]     Test net output #0: error_blob = 0.575841 (* 1 = 0.575841 loss)
I1006 18:28:25.755815 26261 solver.cpp:243] Iteration 200, loss = 0.622084
I1006 18:28:25.755847 26261 solver.cpp:259]     Train net output #0: error_blob = 0.622084 (* 1 = 0.622084 loss)
I1006 18:28:25.755853 26261 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:28:30.642096 26261 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:28:30.643488 26261 solver.cpp:415]     Test net output #0: error_blob = 0.565754 (* 1 = 0.565754 loss)
I1006 18:28:30.696782 26261 solver.cpp:243] Iteration 300, loss = 0.612638
I1006 18:28:30.696812 26261 solver.cpp:259]     Train net output #0: error_blob = 0.612638 (* 1 = 0.612638 loss)
I1006 18:28:30.696818 26261 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:28:35.568399 26261 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:28:35.569742 26261 solver.cpp:415]     Test net output #0: error_blob = 0.560845 (* 1 = 0.560845 loss)
I1006 18:28:35.622030 26261 solver.cpp:243] Iteration 400, loss = 0.601412
I1006 18:28:35.622069 26261 solver.cpp:259]     Train net output #0: error_blob = 0.601412 (* 1 = 0.601412 loss)
I1006 18:28:35.622076 26261 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:28:40.469460 26261 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:28:40.470865 26261 solver.cpp:415]     Test net output #0: error_blob = 0.55628 (* 1 = 0.55628 loss)
I1006 18:28:40.523198 26261 solver.cpp:243] Iteration 500, loss = 0.603013
I1006 18:28:40.523227 26261 solver.cpp:259]     Train net output #0: error_blob = 0.603013 (* 1 = 0.603013 loss)
I1006 18:28:40.523260 26261 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:28:45.398659 26261 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:28:45.400089 26261 solver.cpp:415]     Test net output #0: error_blob = 0.555478 (* 1 = 0.555478 loss)
I1006 18:28:45.452558 26261 solver.cpp:243] Iteration 600, loss = 0.601177
I1006 18:28:45.452600 26261 solver.cpp:259]     Train net output #0: error_blob = 0.601177 (* 1 = 0.601177 loss)
I1006 18:28:45.452607 26261 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:28:50.335052 26261 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:28:50.336527 26261 solver.cpp:415]     Test net output #0: error_blob = 0.555541 (* 1 = 0.555541 loss)
I1006 18:28:50.387701 26261 solver.cpp:243] Iteration 700, loss = 0.598582
I1006 18:28:50.387738 26261 solver.cpp:259]     Train net output #0: error_blob = 0.598582 (* 1 = 0.598582 loss)
I1006 18:28:50.387744 26261 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:28:55.244086 26261 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:28:55.245534 26261 solver.cpp:415]     Test net output #0: error_blob = 0.556645 (* 1 = 0.556645 loss)
I1006 18:28:55.300220 26261 solver.cpp:243] Iteration 800, loss = 0.596108
I1006 18:28:55.300264 26261 solver.cpp:259]     Train net output #0: error_blob = 0.596108 (* 1 = 0.596108 loss)
I1006 18:28:55.300271 26261 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:29:00.163732 26261 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:29:00.165156 26261 solver.cpp:415]     Test net output #0: error_blob = 0.553863 (* 1 = 0.553863 loss)
I1006 18:29:00.218849 26261 solver.cpp:243] Iteration 900, loss = 0.596328
I1006 18:29:00.218888 26261 solver.cpp:259]     Train net output #0: error_blob = 0.596328 (* 1 = 0.596328 loss)
I1006 18:29:00.218894 26261 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:29:05.109897 26261 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:29:05.112133 26261 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:29:05.158422 26261 solver.cpp:327] Iteration 1000, loss = 0.588838
I1006 18:29:05.158458 26261 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:29:05.158882 26261 solver.cpp:415]     Test net output #0: error_blob = 0.552497 (* 1 = 0.552497 loss)
I1006 18:29:05.158890 26261 solver.cpp:332] Optimization Done.
I1006 18:29:05.158892 26261 caffe.cpp:215] Optimization Done.
I1006 18:29:05.308131 26291 caffe.cpp:184] Using GPUs 0
I1006 18:29:05.873739 26291 solver.cpp:54] Initializing solver from parameters: 
test_iter: 1
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 1000
lr_policy: "fixed"
momentum: 0.9
solver_mode: GPU
device_id: 0
net: "full_batch_ce/model2_part6.prototxt"
I1006 18:29:05.873770 26291 solver.cpp:97] Creating training net from net file: full_batch_ce/model2_part6.prototxt
I1006 18:29:05.873934 26291 net.cpp:339] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_layer
I1006 18:29:05.873977 26291 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part6.prototxt"
state {
  phase: TRAIN
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TRAIN
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part6.train"
    batch_size: 40000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:29:05.874016 26291 layer_factory.hpp:76] Creating layer data_layer
I1006 18:29:05.900300 26291 net.cpp:110] Creating Layer data_layer
I1006 18:29:05.900317 26291 net.cpp:433] data_layer -> data_blob
I1006 18:29:05.900339 26291 net.cpp:433] data_layer -> label_blob
I1006 18:29:05.900954 26295 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part6.train
I1006 18:29:06.583163 26291 data_layer.cpp:45] output data size: 40000,61,1,1
I1006 18:29:06.593765 26291 net.cpp:155] Setting up data_layer
I1006 18:29:06.593819 26291 net.cpp:163] Top shape: 40000 61 1 1 (2440000)
I1006 18:29:06.593824 26291 net.cpp:163] Top shape: 40000 (40000)
I1006 18:29:06.593830 26291 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:29:06.593842 26291 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:29:06.593848 26291 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:29:06.593858 26291 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:29:06.594234 26291 net.cpp:155] Setting up hidden_sum_layer
I1006 18:29:06.594241 26291 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:29:06.594264 26291 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:29:06.594270 26291 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:29:06.594272 26291 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:29:06.594275 26291 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:29:09.809285 26291 net.cpp:155] Setting up hidden_act_layer
I1006 18:29:09.809316 26291 net.cpp:163] Top shape: 40000 10 (400000)
I1006 18:29:09.809324 26291 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:29:09.809344 26291 net.cpp:110] Creating Layer output_sum_layer
I1006 18:29:09.809346 26291 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:29:09.809351 26291 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:29:09.809454 26291 net.cpp:155] Setting up output_sum_layer
I1006 18:29:09.809460 26291 net.cpp:163] Top shape: 40000 1 (40000)
I1006 18:29:09.809478 26291 layer_factory.hpp:76] Creating layer error_layer
I1006 18:29:09.809484 26291 net.cpp:110] Creating Layer error_layer
I1006 18:29:09.809487 26291 net.cpp:477] error_layer <- output_sum_blob
I1006 18:29:09.809489 26291 net.cpp:477] error_layer <- label_blob
I1006 18:29:09.809494 26291 net.cpp:433] error_layer -> error_blob
I1006 18:29:09.809519 26291 net.cpp:155] Setting up error_layer
I1006 18:29:09.809521 26291 net.cpp:163] Top shape: (1)
I1006 18:29:09.809540 26291 net.cpp:168]     with loss weight 1
I1006 18:29:09.809557 26291 net.cpp:236] error_layer needs backward computation.
I1006 18:29:09.809559 26291 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:29:09.809561 26291 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:29:09.809563 26291 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:29:09.809566 26291 net.cpp:240] data_layer does not need backward computation.
I1006 18:29:09.809567 26291 net.cpp:283] This network produces output error_blob
I1006 18:29:09.809571 26291 net.cpp:297] Network initialization done.
I1006 18:29:09.809573 26291 net.cpp:298] Memory required for data: 13280004
I1006 18:29:09.809701 26291 solver.cpp:187] Creating test net (#0) specified by net file: full_batch_ce/model2_part6.prototxt
I1006 18:29:09.809723 26291 net.cpp:339] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_layer
I1006 18:29:09.809752 26291 net.cpp:50] Initializing net from parameters: 
name: "full_batch_ce/model2_part6.prototxt"
state {
  phase: TEST
}
layer {
  name: "data_layer"
  type: "Data"
  top: "data_blob"
  top: "label_blob"
  include {
    phase: TEST
  }
  data_param {
    source: "lmdb/SCOREDATA.vina.balanced.part6.test"
    batch_size: 4000
    backend: LMDB
    prefetch: 8
  }
}
layer {
  name: "hidden_sum_layer"
  type: "InnerProduct"
  bottom: "data_blob"
  top: "hidden_sum_blob"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "hidden_act_layer"
  type: "Sigmoid"
  bottom: "hidden_sum_blob"
  top: "hidden_act_blob"
}
layer {
  name: "output_sum_layer"
  type: "InnerProduct"
  bottom: "hidden_act_blob"
  top: "output_sum_blob"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "error_layer"
  type: "SigmoidCrossEntropyLoss"
  bottom: "output_sum_blob"
  bottom: "label_blob"
  top: "error_blob"
}
I1006 18:29:09.809790 26291 layer_factory.hpp:76] Creating layer data_layer
I1006 18:29:09.812194 26291 net.cpp:110] Creating Layer data_layer
I1006 18:29:09.812213 26291 net.cpp:433] data_layer -> data_blob
I1006 18:29:09.812219 26291 net.cpp:433] data_layer -> label_blob
I1006 18:29:09.812821 26301 db_lmdb.cpp:23] Opened lmdb lmdb/SCOREDATA.vina.balanced.part6.test
I1006 18:29:09.812902 26291 data_layer.cpp:45] output data size: 4000,61,1,1
I1006 18:29:09.814836 26291 net.cpp:155] Setting up data_layer
I1006 18:29:09.814849 26291 net.cpp:163] Top shape: 4000 61 1 1 (244000)
I1006 18:29:09.814852 26291 net.cpp:163] Top shape: 4000 (4000)
I1006 18:29:09.814857 26291 layer_factory.hpp:76] Creating layer hidden_sum_layer
I1006 18:29:09.814867 26291 net.cpp:110] Creating Layer hidden_sum_layer
I1006 18:29:09.814870 26291 net.cpp:477] hidden_sum_layer <- data_blob
I1006 18:29:09.814877 26291 net.cpp:433] hidden_sum_layer -> hidden_sum_blob
I1006 18:29:09.815027 26291 net.cpp:155] Setting up hidden_sum_layer
I1006 18:29:09.815034 26291 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:29:09.815043 26291 layer_factory.hpp:76] Creating layer hidden_act_layer
I1006 18:29:09.815050 26291 net.cpp:110] Creating Layer hidden_act_layer
I1006 18:29:09.815053 26291 net.cpp:477] hidden_act_layer <- hidden_sum_blob
I1006 18:29:09.815059 26291 net.cpp:433] hidden_act_layer -> hidden_act_blob
I1006 18:29:09.815135 26291 net.cpp:155] Setting up hidden_act_layer
I1006 18:29:09.815141 26291 net.cpp:163] Top shape: 4000 10 (40000)
I1006 18:29:09.815143 26291 layer_factory.hpp:76] Creating layer output_sum_layer
I1006 18:29:09.815150 26291 net.cpp:110] Creating Layer output_sum_layer
I1006 18:29:09.815153 26291 net.cpp:477] output_sum_layer <- hidden_act_blob
I1006 18:29:09.815158 26291 net.cpp:433] output_sum_layer -> output_sum_blob
I1006 18:29:09.815227 26291 net.cpp:155] Setting up output_sum_layer
I1006 18:29:09.815234 26291 net.cpp:163] Top shape: 4000 1 (4000)
I1006 18:29:09.815258 26291 layer_factory.hpp:76] Creating layer error_layer
I1006 18:29:09.815268 26291 net.cpp:110] Creating Layer error_layer
I1006 18:29:09.815273 26291 net.cpp:477] error_layer <- output_sum_blob
I1006 18:29:09.815276 26291 net.cpp:477] error_layer <- label_blob
I1006 18:29:09.815281 26291 net.cpp:433] error_layer -> error_blob
I1006 18:29:09.815310 26291 net.cpp:155] Setting up error_layer
I1006 18:29:09.815315 26291 net.cpp:163] Top shape: (1)
I1006 18:29:09.815317 26291 net.cpp:168]     with loss weight 1
I1006 18:29:09.815328 26291 net.cpp:236] error_layer needs backward computation.
I1006 18:29:09.815332 26291 net.cpp:236] output_sum_layer needs backward computation.
I1006 18:29:09.815337 26291 net.cpp:236] hidden_act_layer needs backward computation.
I1006 18:29:09.815341 26291 net.cpp:236] hidden_sum_layer needs backward computation.
I1006 18:29:09.815346 26291 net.cpp:240] data_layer does not need backward computation.
I1006 18:29:09.815348 26291 net.cpp:283] This network produces output error_blob
I1006 18:29:09.815356 26291 net.cpp:297] Network initialization done.
I1006 18:29:09.815358 26291 net.cpp:298] Memory required for data: 1328004
I1006 18:29:09.815382 26291 solver.cpp:66] Solver scaffolding done.
I1006 18:29:09.815479 26291 caffe.cpp:212] Starting Optimization
I1006 18:29:09.815485 26291 solver.cpp:294] Solving full_batch_ce/model2_part6.prototxt
I1006 18:29:09.815489 26291 solver.cpp:295] Learning Rate Policy: fixed
I1006 18:29:09.815696 26291 solver.cpp:347] Iteration 0, Testing net (#0)
I1006 18:29:09.815837 26291 blocking_queue.cpp:50] Data layer prefetch queue empty
I1006 18:29:09.825562 26291 solver.cpp:415]     Test net output #0: error_blob = 0.787802 (* 1 = 0.787802 loss)
I1006 18:29:09.829208 26291 solver.cpp:243] Iteration 0, loss = 0.769032
I1006 18:29:09.829231 26291 solver.cpp:259]     Train net output #0: error_blob = 0.769032 (* 1 = 0.769032 loss)
I1006 18:29:09.829238 26291 solver.cpp:590] Iteration 0, lr = 0.01
I1006 18:29:14.643477 26291 solver.cpp:347] Iteration 100, Testing net (#0)
I1006 18:29:14.644876 26291 solver.cpp:415]     Test net output #0: error_blob = 0.644757 (* 1 = 0.644757 loss)
I1006 18:29:14.696120 26291 solver.cpp:243] Iteration 100, loss = 0.627716
I1006 18:29:14.696167 26291 solver.cpp:259]     Train net output #0: error_blob = 0.627716 (* 1 = 0.627716 loss)
I1006 18:29:14.696176 26291 solver.cpp:590] Iteration 100, lr = 0.01
I1006 18:29:19.571254 26291 solver.cpp:347] Iteration 200, Testing net (#0)
I1006 18:29:19.572644 26291 solver.cpp:415]     Test net output #0: error_blob = 0.632283 (* 1 = 0.632283 loss)
I1006 18:29:19.624866 26291 solver.cpp:243] Iteration 200, loss = 0.611883
I1006 18:29:19.624894 26291 solver.cpp:259]     Train net output #0: error_blob = 0.611883 (* 1 = 0.611883 loss)
I1006 18:29:19.624902 26291 solver.cpp:590] Iteration 200, lr = 0.01
I1006 18:29:24.679451 26291 solver.cpp:347] Iteration 300, Testing net (#0)
I1006 18:29:24.680963 26291 solver.cpp:415]     Test net output #0: error_blob = 0.623232 (* 1 = 0.623232 loss)
I1006 18:29:24.730329 26291 solver.cpp:243] Iteration 300, loss = 0.606348
I1006 18:29:24.730360 26291 solver.cpp:259]     Train net output #0: error_blob = 0.606348 (* 1 = 0.606348 loss)
I1006 18:29:24.730365 26291 solver.cpp:590] Iteration 300, lr = 0.01
I1006 18:29:29.620817 26291 solver.cpp:347] Iteration 400, Testing net (#0)
I1006 18:29:29.622253 26291 solver.cpp:415]     Test net output #0: error_blob = 0.616814 (* 1 = 0.616814 loss)
I1006 18:29:29.671954 26291 solver.cpp:243] Iteration 400, loss = 0.593746
I1006 18:29:29.671993 26291 solver.cpp:259]     Train net output #0: error_blob = 0.593746 (* 1 = 0.593746 loss)
I1006 18:29:29.672013 26291 solver.cpp:590] Iteration 400, lr = 0.01
I1006 18:29:34.607825 26291 solver.cpp:347] Iteration 500, Testing net (#0)
I1006 18:29:34.609284 26291 solver.cpp:415]     Test net output #0: error_blob = 0.61218 (* 1 = 0.61218 loss)
I1006 18:29:34.659718 26291 solver.cpp:243] Iteration 500, loss = 0.590726
I1006 18:29:34.659755 26291 solver.cpp:259]     Train net output #0: error_blob = 0.590726 (* 1 = 0.590726 loss)
I1006 18:29:34.659802 26291 solver.cpp:590] Iteration 500, lr = 0.01
I1006 18:29:39.541954 26291 solver.cpp:347] Iteration 600, Testing net (#0)
I1006 18:29:39.543454 26291 solver.cpp:415]     Test net output #0: error_blob = 0.608639 (* 1 = 0.608639 loss)
I1006 18:29:39.593561 26291 solver.cpp:243] Iteration 600, loss = 0.589783
I1006 18:29:39.593595 26291 solver.cpp:259]     Train net output #0: error_blob = 0.589783 (* 1 = 0.589783 loss)
I1006 18:29:39.593603 26291 solver.cpp:590] Iteration 600, lr = 0.01
I1006 18:29:44.423681 26291 solver.cpp:347] Iteration 700, Testing net (#0)
I1006 18:29:44.425086 26291 solver.cpp:415]     Test net output #0: error_blob = 0.606126 (* 1 = 0.606126 loss)
I1006 18:29:44.476912 26291 solver.cpp:243] Iteration 700, loss = 0.589041
I1006 18:29:44.476945 26291 solver.cpp:259]     Train net output #0: error_blob = 0.589041 (* 1 = 0.589041 loss)
I1006 18:29:44.476953 26291 solver.cpp:590] Iteration 700, lr = 0.01
I1006 18:29:49.337527 26291 solver.cpp:347] Iteration 800, Testing net (#0)
I1006 18:29:49.338966 26291 solver.cpp:415]     Test net output #0: error_blob = 0.604201 (* 1 = 0.604201 loss)
I1006 18:29:49.391626 26291 solver.cpp:243] Iteration 800, loss = 0.581736
I1006 18:29:49.391659 26291 solver.cpp:259]     Train net output #0: error_blob = 0.581736 (* 1 = 0.581736 loss)
I1006 18:29:49.391664 26291 solver.cpp:590] Iteration 800, lr = 0.01
I1006 18:29:54.314795 26291 solver.cpp:347] Iteration 900, Testing net (#0)
I1006 18:29:54.316196 26291 solver.cpp:415]     Test net output #0: error_blob = 0.602764 (* 1 = 0.602764 loss)
I1006 18:29:54.366096 26291 solver.cpp:243] Iteration 900, loss = 0.58942
I1006 18:29:54.366127 26291 solver.cpp:259]     Train net output #0: error_blob = 0.58942 (* 1 = 0.58942 loss)
I1006 18:29:54.366132 26291 solver.cpp:590] Iteration 900, lr = 0.01
I1006 18:29:59.242903 26291 solver.cpp:468] Snapshotting to binary proto file _iter_1000.caffemodel
I1006 18:29:59.244194 26291 solver.cpp:753] Snapshotting solver state to binary proto file _iter_1000.solverstate
I1006 18:29:59.291924 26291 solver.cpp:327] Iteration 1000, loss = 0.576062
I1006 18:29:59.291950 26291 solver.cpp:347] Iteration 1000, Testing net (#0)
I1006 18:29:59.292405 26291 solver.cpp:415]     Test net output #0: error_blob = 0.601062 (* 1 = 0.601062 loss)
I1006 18:29:59.292413 26291 solver.cpp:332] Optimization Done.
I1006 18:29:59.292418 26291 caffe.cpp:215] Optimization Done.
